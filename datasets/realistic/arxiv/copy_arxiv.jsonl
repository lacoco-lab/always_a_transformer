{"input": "\\section{\\label{sec: intro} Introduction} % This paragraph answers why we do this. The ability to perform fast, high-fidelity, quantum nondemolition (QND) measurements is essential for quantum error correction and, more generally, for any quantum circuit that requires mid-circuit measurement. The standard method to qubit measurement in superconducting circuits is dispersive readout~\\cite{blais2021, blais2004}. In this approach, a superconducting qubit, e.g., transmon~\\cite{koch2007} or fluxonium~\\cite{manucharyan2009}, weakly coupled to a far-detuned resonator, induces a state-dependent frequency shift to the resonator. A qubit measurement is performed by exciting the resonator with a readout tone, such that the field in the resonator entangles with the qubit, resulting in a projection of the qubit state as the field is detected~\\cite{krantz2019}. In principle, this process is QND, and the signal-to-noise ratio within a given time can be improved by increasing the amplitude of the resonator field. Recent experiments have achieved over $99\\%$ assignment fidelity on transmons with readout times equal to or less than \\qty{100}{\\ns}~\\cite{walter2017, sunada2022, swiadek2024, sunada2024, hazra2025}. Despite this progress, readout errors continue to be a major bottleneck in achieving fault-tolerant quantum computation~\\cite{acharya2023, acharya2024a}. % This paragraph lists what has been done. An important limitation to dispersive readout in a transmon is that strong drives can excite the transmon outside its computational two-level subspace~\\cite{jeffrey2014, sank2016, walter2017, Minev2019, Verney2019, khezri2023, hazra2025} in a process that has been referred to as measurement-induced state transition (MIST)~\\cite{sank2016} and transmon ionization~\\cite{shillito2022, Verney2019, Mathieu_thesis}."}
{"input": "occurrence have been developed~\\cite{sank2016, shillito2022, cohen2023, Xiao2023, khezri2023, dumas2024}. These tools have also been applied to the fluxonium~\\cite{nesterov2024b, singh2024, bista2025} and to develop methods for mitigating ionization~\\cite{bengtsson2024, kurilovich2025, chapple2025, chapple2024}. This phenomenon bears some resemblance to multiphoton ionization in atoms and molecules, where a strong laser or microwave field promotes the electron from a bound state into the continuum~\\cite{Mainfray1991, agostini1968, mainfray1980, martin1976, deng1984, deng1985}. In both cases, the system can be driven into a highly excited state with delocalized wavefunctions and energies above the confining potential. % This paragraph emphasizes what we do differently. A comprehensive understanding of these multiphoton processes is a key step in developing strategies to avoid unwanted transitions in dispersive readout. While experimental results are consistent with theoretical predictions for the critical photon number of transmon ionization, other features---such as the final state reached and the occurrence of Landau-Zener dynamics---remain unverified. It is challenging to observe these phenomena since the control and measurement of typical transmons are often limited to the 4 lowest states, excluding the highly excited states. In this work, we study these unexplored features of transmon ionization by directly measuring its excited-state dynamics using high-$E_J/E_C$ transmons that enable high-fidelity control and readout of 10 energy eigenstates~\\cite{wang2025, champion2024}. In the regime of negative transmon-resonator detuning, we demonstrate that the transmon ionization is indeed a pairwise transition between a qubit state and a highly excited state."}
{"input": "\\section{\\label{sec: concepts} Ionization of high-$E_J/E_C$ transmons} The mechanism of transmon ionization can be understood as a multiphoton resonance in a driven transmon. The drive induces an ac-Stark shift to each transmon eigenstate, resulting in a resonance when the energy difference between two shifted transmon states equals an integer number of the drive photon energy~\\cite{sank2016, shillito2022, cohen2023, Xiao2023, khezri2023, dumas2024}. As a result, a driven transmon can transition from its computational subspace to a highly excited state at a specific drive amplitude. In principle, these resonances can occur between many pairs of transmon states and for a variety of transmon parameters. In practice, however, typical transmons that have relatively shallow potential are often excited to a state close to the top of the potential, as shown in \\cref{fig: 1_concepts}(a), and such a highly excited state is harder to address experimentally due to charge noise. Transmon ionization to highly excited states has thus far only been observed indirectly as a leakage out of the qubit subspace. In contrast, the high-$E_J/E_C$ transmons in our experiments have deeper potentials and confine more energy levels; see the right-hand side of \\cref{fig: 1_concepts}(a). As a result, at least 10 transmon eigenstates are insensitive to charge noise and can be controlled and measured~\\cite{wang2025, champion2024}. This enables us to directly probe excited-state dynamics of transmon ionization."}
{"input": "and $\\ket{7}$, as will be the case in \\cref{sec: experiments}. \\begin{figure}[tp] \\includegraphics{fig1.pdf} \\caption{ \\label{fig: 1_concepts} Transmon ionization concepts. (a) The potentials and eigenstates of transmons. Here, we show examples of two transmons with $\\omega_{01}/2\\pi=\\qty{5}{\\GHz}$ and $E_J/E_C=100$ (left) or $E_J/E_C=270$ (right). The target state of ionization for a typical transmon is often close to (or even above) the top of its potential. High-$E_J/E_C$ transmons have a deeper potential and confine more energy levels, which makes the highly excited states accessible during the transmon ionization. The level diagram depicts a multiphoton resonance. In this example, the energy levels of states $\\ket{1}$ and $\\ket{7}$ are ac-Stark shifted by the drive to reach the resonance condition $\\tilde{\\omega}_7-\\tilde{\\omega}_1=n\\omega_d$ at a certain drive power, with $\\omega_d$ the drive frequency and $n$ the number of absorbed photons. Typically, $n > 1$. (b) Circuit diagrams. When the transmon is in one of its eigenstates, a readout pulse with frequency $\\omega_d$ and amplitude $\\varepsilon(t)$ creates a coherent state in the resonator. This coherent state can be effectively modeled as a classical drive applied directly to the transmon, which can induce transitions between transmon states. This driven transmon model is used for the numerical simulations in this work; see \\cref{eq:driven_harmonic_transmon_hamiltonian}."}
{"input": "\\hat{a}^\\dagger) \\\\ & - i \\varepsilon(t) \\cos(\\omega_dt)(\\hat{a} - \\hat{a}^\\dagger). \\label{eq:transverse_coupling_hamiltonian} \\end{split} \\end{equation} In this expression, $E_{Jm}$, $E_C$, $\\hat{n}_t$, $\\hat{\\varphi}_t$ and $n_g$ are respectively the Josephson energies, the charging energy, the charge operator, the phase operator, and the offset charge of the transmon. Moreover, $\\omega_r$ and $\\hat{a}$ are the bare frequency and annihilation operator of the resonator, while $\\varepsilon(t)$ and $\\omega_d$ are the amplitude and frequency of the capacitive drive on the resonator, respectively. The resonator has a linewidth $\\kappa$. In the dispersive regime, it inherits a transmon-state-dependent frequency shift $\\chi_j$ as well as Kerr $K_{r, \\ket{j}}$ and higher-order nonlinearities for any nonzero coupling strength $g$~\\cite{blais2021}. \\Cref{eq:transverse_coupling_hamiltonian} includes higher harmonics of the Josephson potential that provide a more accurate description of the transmon spectrum~\\cite{willsch2024, wang2025}. Two high-$E_J/E_C$ transmons are used in this work, $Q_A$ with $E_{J1}/E_C=275$ and $Q_B$ with $E_{J1}/E_C=235$. For both transmon-resonator pairs, the qubit frequency is lower than the resonator frequency; see \\cref{sec: device_params} for the full set of parameters. Because of this choice of qubit-resonator detuning and the weak transmon-resonator coupling strength $g/2\\pi \\sim \\qty{30}{\\MHz}$, the dispersive shifts are small, and ionization occurs at large photon numbers in these devices~\\cite{dumas2024}. As a result, the dimension of the full transmon-resonator Hilbert space required to model and simulate the experiment is prohibitively large. However, previous works have shown that the coherent state $\\alpha(t)$ in the resonator generated by the readout tone approximately results in an effective classical drive acting on the transmon~\\cite{cohen2023, lledo2023, khezri2023, dumas2024}; see \\cref{fig: 1_concepts}(b)."}
{"input": "case, the effective semiclassical Hamiltonian for the transmon is \\begin{equation} \\begin{split} \\hat{H}_{\\rm sc}(t) &= 4 E_C (\\hat{n}_t - n_g)^2 - \\sum_{m = 1}^M E_{Jm} \\cos(m \\hat{\\varphi}_t) \\\\ &- 2g \\sqrt{\\bar n_r(t)} \\sin[\\omega_d t - \\phi(t)] (\\hat{n}_t - n_g), \\label{eq:driven_harmonic_transmon_hamiltonian} \\end{split} \\end{equation} where the resonator field is written as $\\alpha(t) = \\sqrt{\\bar n_r(t)} e^{i \\phi(t)}$ with $\\bar n_r(t) = |\\alpha(t)|^2$ being the average photon number. Transmon ionization occurs at specific values of $\\bar n_r$, corresponding to a set of critical photon numbers for each transmon eigenstate $\\ket{j}$. This model neglects quantum fluctuations in the resonator and, in particular, measurement-induced dephasing. Nevertheless, as discussed below, it captures experimental observations after the results are averaged over gate charge. % \\bencomment{Get into why later. Another issue here is that I still need to connect this to the Kerr model used for calibration; it might be confusing.} \\abc{I would not talk about this here. The reader is referred to an appendix for that, which seems good enough for the main text."}
{"input": "ac-Stark shift $ (\\chi_{j+1} -\\chi_j) \\bar n_r$ using a \\qty{40}{\\ns} spectroscopy pulse on the transmon. By changing the timing of the spectroscopy pulse, the time-dependent $\\bar n_r(t)$ can be measured; see \\cref{fig: 2_experiments}(b). Because of the relatively small linewidth $\\kappa$, the resonator does not reach a steady state during the stimulation pulse and requires a long ring-down time. In this experiment, we use this spectroscopy sequence to calibrate the conversion between stimulation amplitude and maximum mean photon number $\\bar n_{r, \\rm{max}}$ at low photon number, up to 400 photons, and extrapolate to higher photon numbers accounting for the induced Kerr nonlinearity. Details of the conversion and the effect of nonlinearity are discussed in \\cref{sec: photon_extrapolation}. In \\cref{fig: 2_experiments}(c), we show the resulting transmon populations at the end of the sequence for different maximum mean photon numbers $\\bar n_{r, \\rm{max}}$ when the transmon is initially prepared in $\\ket{1}$. We observe a series of distinct drops in the population of $\\ket{1}$ at specific photon numbers. First, at $\\bar n_{r, \\rm{max}} \\sim 170$, qubit $Q_A$ becomes resonant with a neighboring transmon, leading to an exchange of excitations between the two transmons; see \\cref{sec: QAQBswap} for more details. More interestingly, a signature of ionization is visible at $\\bar n_{r, \\rm{max}} \\sim 880$ where a population drop of $\\ket{1}$ coincides with increased populations in several highly excited states. We attribute the fact that multiple excited states are populated to energy relaxation that occurs after ionization."}
{"input": "frequencies: Including additional Josephson harmonics in the transmon Hamiltonian plays a key role in accurately predicting the frequency of highly excited states and, thus, the occurrence of transmon ionization. %% This paragraph tells what the Floquet branch is and how to calculate it. The ionization process can also be understood via the more intuitive and more computationally efficient Floquet branch analysis~\\cite{dumas2024}. Because the average photon number $\\bar n_r(t)$ and phase $\\phi(t)$ in \\cref{eq:driven_harmonic_transmon_hamiltonian} vary slowly on the timescale of the drive period $T_d=2\\pi/\\omega_d$, the transmon Hamiltonian is approximately periodic on short timescales. As a result, ionization is determined by resonances in the Floquet spectrum associated with the instantaneously periodic Hamiltonian. To obtain this Floquet spectrum, we choose linearly spaced effective transmon drive amplitudes $2g\\sqrt{\\bar n_r}$ in steps of $2\\pi \\times \\qty{100}{\\kHz}$. For each constant drive amplitude, we calculate the Floquet modes and quasienergies by solving the eigenvalue problem of the propagator $U(T_d, 0)$ for \\cref{eq:driven_harmonic_transmon_hamiltonian}. At $\\bar n_{r} = 0$, the result coincides with the bare transmon eigenstates and eigenenergies, from where we sort other Floquet modes and quasienergies at higher amplitudes into a ``Floquet branch'' for each bare transmon state \\cite{dumas2024}. %% This paragraph explains the result of Floquet analysis. We show the normalized quasienergies $\\epsilon_j/\\omega_d$ of the Floquet branches for $\\omega_d=\\omega_{r, \\ket{0}}$ in \\cref{fig: 3_comparisons}(c) and $\\omega_d=\\omega_{r, \\ket{1}}$ in \\cref{fig: 3_comparisons}(f), respectively, highlighting in color the most relevant Floquet branches for the $\\ket{0} \\leftrightarrow \\ket{6}$ and $\\ket{1} \\leftrightarrow \\ket{7}$ resonances."}
{"input": "shows avoided crossings with other branches as $\\bar n_r$ increases. These avoided crossings indicate the multiphoton transitions responsible for ionization (the quasienergies are defined only modulo $\\omega_d$). In general, there are multiple avoided crossings associated with various pairs of Floquet branches. However, we find that the positions of the largest avoided crossings, as well as the branches they involve, are consistent with those observed in the experiments and the dynamical simulations. Once ionization has occurred at these dominant avoided crossings, any number of other avoided crossings involving any of the populated branches can become relevant. This can occur during both the ramp-up and the ring-down phases of the readout pulse sequence."}
{"input": "\\section{\\label{sec: LandauZener} Landau-Zener transitions} \\begin{figure*}[!t] \\includegraphics{fig4.pdf} \\caption{ \\label{fig: 4_LandauZener} Landau-Zener transitions. (a) The measured photon numbers (red dots) of the steady-state sequence (top) and Landau-Zener sequence (bottom) for different steady-state times $t_s$. The data agree well with the numerical prediction (red lines), which uses parameters extracted from independent measurements. The transmon is prepared in $\\ket{0}$ at the beginning of the sequence. The insets show the envelopes of the shaped stimulation pulses, each of which includes three segments: ramp-up, steady state, and ramp-down. In the Landau-Zener sequence, the amplitude during $t_s$ is intentionally increased above the amplitude used for the steady-state sequence to drive the resonator from $\\bar n_{r, i}$ to $\\bar n_{r, f}$. The pulse is followed by a \\qty{6}{\\us} ring-down and an end-sequence measurement. We show three sequences with different Landau-Zener speeds identified by different symbols in the bottom panel. (b) The measured population of $\\ket{0}$ from the end-sequence measurement for different steady-state durations $t_s$ and photon numbers $\\bar n_{r, s}$. We observe a critical photon number for transmon ionization at around 1500 photons (see inset). Above 3000 photons, more resonances appear, while our pulse-shaping method fails to stabilize the photon number due to the higher-order nonlinearities of the resonator. (c) The measured populations of Landau-Zener experiments with different Landau-Zener speeds. The slope $d\\bar n_r(t)/dt$ is controlled by changing the duration $t_s$ with fixed $\\bar n_{r, i}=1300$, $\\bar n_{r, f}=1700$ (circles), or by changing the difference $\\bar n_{r, f}-\\bar n_{r, i}$ with fixed $t_s=\\qty{10}{\\us}$."}
{"input": "all parameters used in the numerical simulations are extracted from independent measurements. Details of the pulse calibration can be found in \\cref{sec: pulses}. In the steady-state sequence, the transmon is initially prepared in $\\ket{0}$, followed by a stimulation pulse with three segments; see the top panel of \\cref{fig: 4_LandauZener}(a). The first segment is a \\qty{40}{\\ns} ramp-up to rapidly bring the resonator from the vacuum state to the desired photon number $\\bar n_{r,s}$. The second segment holds the resonator in its steady state with variable duration $t_s$. The third segment is a \\qty{40}{\\ns} ramp-down to actively empty the resonator. The frequency of the stimulation pulse is detuned from $\\omega_{r, \\ket{0}}$ by the expected Kerr shift $\\bar n_{r,s} K_{r, \\ket{0}}$ to compensate for the Kerr effect in the steady state. The short ramping time ensures strong diabaticity during ramping, making the transmon ionization most likely to happen during the steady-state segment. We end the sequence with a \\qty{6}{\\us} free ring-down time to fully empty the resonator, followed by a measurement. The results of the steady-state experiment for different durations $t_s$ and photon numbers $\\bar n_{r,s}$ are shown in \\cref{fig: 4_LandauZener}(b). At low photon numbers, the system remains below the critical photon number, and the transmon stays in its initial state $\\ket{0}$. At $\\bar n_{r,s} \\approx 1500$, however, the transmon ionizes, with a longer $t_s$ resulting in a lower ground-state population."}
{"input": "is further ionized to higher excited states; see \\cref{sec: full_fig4b}. Above the critical photon number, there exists a range of photon numbers where the transition is suppressed again. This is because the system diabatically crosses the resonance at $\\bar n_{r,s} \\approx 1500$ during the ramping segments while remaining far from other avoided crossings responsible for ionization during the steady-state segment. A similar phenomenon was observed in Ref.~\\cite{sank2016}. At $\\bar n_{r,s} > 3000$, there is again a reduction in the ground-state population. This occurs because the higher-order nonlinearities of the resonator become too strong for our pulse to stabilize the photon number for a long time while more avoided crossings appear. As a result, the resonator photon number sweeps through strong resonances, and most of the population eventually transfers from $\\ket{0}$ to higher excited states. Having identified the critical photon number for ionization, we next perform Landau-Zener experiments. The Landau-Zener sequence differs from the steady-state sequence in two ways; see the bottom panel of \\cref{fig: 4_LandauZener}(a). First, after rapidly filling $\\bar n_{r, i}$ photons into the resonator, the pulse amplitude of the steady-state segment is adjusted so that the photon number $\\bar n_r(t)$ increases from $\\bar n_{r, i}$ to $\\bar n_{r, f}$ during the time $t_s$. As a result, the slope $d \\bar n_r(t)/dt$ near the avoided crossing can be controlled by changing either $t_s$ or $\\bar n_{r, f}-\\bar n_{r, i}$."}
{"input": "see \\cref{sec: LZ_pulse}. The Landau-Zener speed near the avoided crossing is thus determined by the slope $d \\bar n_r(t)/dt$ close to the critical photon number $\\bar n_r \\approx 1500$, with a flat (steep) slope corresponding to an adiabatic (diabatic) process. To reach a wide range of Landau-Zener speeds, we use two different parameterizations of the slope. In the diabatic region, we fix $\\bar n_{r,i}=1300$ and $\\bar n_{r,f}=1700$, while sweeping the duration $t_s$ from \\qty{40}{ns} to \\qty{13}{\\us}. In the adiabatic regime, we fix $t_s=\\qty{10}{\\us}$ and sweep the difference $\\bar n_{r, f}-\\bar n_{r, i}$ while keeping the mean photon number constant, $(\\bar n_{r, f}+\\bar n_{r, i}) / 2 = 1500$. The measured final populations of state $\\ket{0}$, state $\\ket{6}$, and the combined populations of states $\\ket{8}$ or higher are shown in \\cref{fig: 4_LandauZener}(c) for both the diabatic region (circles) and the adiabatic region (diamonds). More details about the Landau-Zener sequence and its parameterizations can be found in \\cref{sec: LZ_pulse}. The general trend of our experimental results matches the expectation of Landau-Zener physics, where a more adiabatic transition causes more population to be ionized. The remaining population $P_0$ in the adiabatic region approaches the result measured in the steady-state experiment, shown as the gray dashed line in \\cref{fig: 4_LandauZener}(c). To obtain a quantitative comparison between the experiment and the theory, we use Floquet branch analysis. Similar to the method used in \\cref{sec: comparisons}, we first calculate the Floquet spectrum at different photon numbers and sort them into Floquet branches."}
{"input": "readout, as shown in Ref.~\\cite{bengtsson2024}, which helps avoid unwanted transitions. % The observed Landau-Zener physics provides further strong evidence for a two-level resonance being the origin of ionization. The Landau-Zener physics is another strong evidence of the two-level resonance. Using a pulse-shaping method, we demonstrate precise control of the photon number in the resonator, which allows us to pass through an avoided crossing over a wide range of adiabaticity. Our experimental results agree with the theoretical prediction that a more adiabatic process yields more population transfer. An intriguing question to answer in the future is whether a high-power QND readout is achievable by crossing the resonance diabatically. Moreover, the reported population in \\cref{fig: 4_LandauZener}(c) corresponds to the transition probability for a single passage through the avoided crossing, and it could be possible to observe Landau-Zener-Stückelberg interference upon a double passage in future work. Although ionization is often discussed in the context of qubit readout, related challenges can become more significant when using a transmon as a high-dimensional qudit~\\cite{wang2025, champion2024}, since higher excited states introduce additional resonance conditions in the spectrum. An example is shown in \\cref{fig: 2_experiments}(d), where population transfer between $\\ket{7}$ and $\\ket{9_+}$ occurs earlier than between $\\ket{7}$ and $\\ket{1}$. In addition, multitone readout—typically required for qudits~\\cite{chen2023, wang2025}—leads to more complex ionization dynamics that cannot be captured by modeling the transmon as being driven by a single periodic tone. These observations highlight the need for careful consideration of readout pulse parameters in qudit applications."}
{"input": "\\section{\\label{sec: device_params} Device parameters} \\begin{table*}[!ht] \\caption{\\label{tab: device_params}Device parameters.} \\begin{ruledtabular} \\begin{tabular}{ccc} Device & $Q_A$ & $Q_B$ \\\\ Usage & \\cref{sec: experiments,sec: comparisons} & \\cref{sec: LandauZener} \\\\ \\hline First anharmonicity $\\alpha_1/2\\pi = f_{12} - f_{01}$ (MHz) & -104 & -104 \\\\ 0-1 transition frequency $\\omega_{01}/2\\pi$ (GHz) & 4.8817 & 4.8334 \\\\ 1-2 transition frequency $\\omega_{12}/2\\pi$ (GHz) & 4.7778 & 4.7198 \\\\ 2-3 transition frequency $\\omega_{23}/2\\pi$ (GHz) & 4.6694 & 4.6007 \\\\ 3-4 transition frequency $\\omega_{34}/2\\pi$ (GHz) & 4.5557 & 4.4754 \\\\ 4-5 transition frequency $\\omega_{45}/2\\pi$ (GHz) & 4.4361 & 4.3428 \\\\ 5-6 transition frequency $\\omega_{56}/2\\pi$ (GHz) & 4.3098 & 4.2015 \\\\ 6-7 transition frequency $\\omega_{67}/2\\pi$ (GHz) & 4.1753 & 4.0497 \\\\ 7-8 transition frequency $\\omega_{78}/2\\pi$ (GHz) & 4.0310 & 3.8848 \\\\ 8-9 transition frequency $\\omega_{89}/2\\pi$ (GHz) & 3.8746 & - \\\\ \\hline Resonator frequency when transmon is at $\\ket{0}$ $\\omega_{r, \\ket{0}}/2\\pi$ (GHz) & 6.470366 & 6.415708 \\\\ Dispersive shift $(\\omega_{r, \\ket{1}} - \\omega_{r, \\ket{0}})/2\\pi$ (kHz) & -249 & -205 \\\\ Resonator linewidth $\\kappa/2\\pi$ (kHz) & 105 & 127 \\\\ Josephson energy $E_{J1}/h$ (GHz) \\footnote{Parameters here is estimated by $E_{J8}$ model.} & 29.7 & 26.8 \\\\ Charging energy $E_C/h$ (GHz) \\footnotemark[1] & 0.108 & 0.116 \\\\ $E_{J1}/E_C$ \\footnotemark[1] & 275 & 235 \\\\ Transmon-resonator coupling strength $g/2\\pi$ (MHz) \\footnotemark[1] & 31.0 & 26.5 \\\\ \\hline \\end{tabular} \\end{ruledtabular} \\end{table*} The device parameters used in this work are shown in \\cref{tab: device_params}. The two transmons used here are relabeled from the $Q_5$ and $Q_4$ in Ref.~\\cite{wang2025}."}
{"input": "\\section{\\label{sec: photon_extrapolation} Calibration of $\\bar n_{r, \\rm max}$ for the square stimulation pulse} \\begin{figure}[!b] \\includegraphics{figS1.pdf} \\caption{ \\label{fig: S1_photon_extrapolation} Measured $\\bar n_{r, \\rm{max}}$ and the extrapolation results for initially prepared states $\\ket{0}$, $\\ket{1}$, $\\ket{6}$, and $\\ket{7}$. } \\end{figure} In \\cref{sec: experiments} of the main text, we show the transmon populations as a function of the maximum photon number reached during the sequence. In practice, the experiments were performed by sweeping the instrument amplitude. For a small range of values, this amplitude is typically proportional to the drive amplitude $\\varepsilon$ on the resonator. For a classical linear resonator initialized in the vacuum state and evolving under a resonant drive, the mean intra-resonator photon number $\\bar n_r(t)$ at a given time $t$ is \\begin{equation} \\bar n_r(t) = \\left( \\frac{\\varepsilon}{\\kappa} \\right)^2 (1 - e^{-\\kappa t / 2})^2, \\label{eq: resonant_rampup_photons} \\end{equation} where $\\kappa$ is the decay rate of the resonator. \\Cref{eq: resonant_rampup_photons} suggests the quadratic relationship $\\bar n_r \\propto \\varepsilon^2$ for a fix time $t$. However, the actual photon number may deviate from this quadratic behavior due to the Kerr effect. To find the accurate maximum mean photon number $\\bar n_{r, \\rm max}$, we first fit the conversion between the instrument amplitude and the measured $\\bar n_{r, \\rm max}$ at low power. We then extrapolate the photon numbers at higher power based on the driven Kerr resonator model explained in \\cref{sec: pulses}. The fitting and extrapolations are repeated for different initial states $\\ket{j}$ because the effective $\\kappa$ weakly depends on the state of the transmon~\\cite{blais2021}."}
{"input": "\\section{\\label{sec: pulses} Pulse-shaping and calibration for the steady-state and Landau-Zener experiments} The steady-state experiment and the Landau-Zener experiment discussed in \\cref{sec: LandauZener} require precise control over the state of the resonator. In this section, we explain our pulse-shaping method and give examples of numerical simulations and experimental calibration results. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\subsection{\\label{sec:EOM}Classical resonator model} Consider a classical driven and damped Kerr resonator in a frame rotating at the drive frequency $\\omega_d$. The equation of motion of its field $\\alpha(t)$ is \\begin{equation} \\begin{split} \\dot{\\alpha}(t) &= i \\Delta \\alpha(t) - iK_r|\\alpha(t)|^2\\alpha(t) - \\frac{\\kappa}{2} \\alpha(t) \\\\ &- i\\frac{\\varepsilon(t)}{2} e^{-i\\phi_d}, \\label{eq:EOM_Kerr_resonator} \\end{split} \\end{equation} where $\\Delta \\equiv \\omega_d - \\omega_r$ is the drive-resonator detuning, $K_r$ is the Kerr coefficient of the resonator, and $\\varepsilon(t)$ and $\\phi_d$ are the amplitude and phase of the drive, respectively. When dispersively coupled to a transmon, the Kerr value is negative due to the negative anharmonicity of the transmon. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\subsection{\\label{sec:Linear_resonator}Linear resonator and three-segment pulse} For a linear resonator ($K_r=0$) under a constant resonant drive [$\\varepsilon(t)=\\varepsilon$, $\\Delta=0$], the solution of \\cref{eq:EOM_Kerr_resonator} is \\begin{equation} \\alpha(t) = C e^{-\\kappa t / 2} - i e^{-i\\phi_d} \\frac{\\varepsilon}{\\kappa}, \\label{eq:solution_linear_rampup} \\end{equation} where $C$ is an integral constant depending on the initial condition. If the resonator starts in the vacuum state, $\\alpha(0)=0$, then \\begin{equation} \\alpha(t) = - i e^{-i\\phi_d} \\frac{\\varepsilon}{\\kappa} (1 - e^{-\\kappa t / 2}), \\label{eq:solution_linear_rampup_vacuum} \\end{equation} and the mean photon number reduces to \\cref{eq: resonant_rampup_photons} with steady-state value $\\bar n_r=|\\alpha(t)|^2=\\varepsilon^2/\\kappa^2$."}
{"input": "the ramp-down amplitude $\\varepsilon_\\downarrow$ should satisfy \\begin{equation} \\begin{split} \\frac{\\varepsilon_\\downarrow}{\\varepsilon_s} &= \\frac{e^{-\\kappa t_\\downarrow / 2}}{e^{-\\kappa t_\\downarrow / 2} - 1} \\\\ &= 1 - \\frac{\\varepsilon_\\uparrow}{\\varepsilon_s} < 0, \\quad \\text{if } t_\\uparrow=t_\\downarrow. \\label{eq:rampdown_amplitude} \\end{split} \\end{equation} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\subsection{\\label{sec: Kerr_resonator}Kerr resonator and detuning} The pulse introduced in \\cref{sec:Linear_resonator} can stabilize linear resonators because the drive term in \\cref{eq:EOM_Kerr_resonator} balances the damping term. However, the Kerr effect induces field-dependent rotations in the phase plane, such that an initially resonant pulse becomes off-resonant as the field builds up. To balance this effect, we detune the pulse by $\\Delta=K_r\\bar n_{r, s}$ such that it is resonant for the desired steady-state photon number $\\bar n_{r,s}$. With this choice, the first two terms on the right of \\cref{eq:EOM_Kerr_resonator} cancel each other, approximating a linear resonator in the steady state. \\begin{figure}[!t] \\includegraphics{figS3.pdf} \\caption{ \\label{fig:S3_NumericalSSpulse} Numerical simulation of the detuned three-segment pulses. (a) $\\Delta=K_r\\bar n_{r, s}$. (b) $\\Delta=0.93 \\times K_r\\bar n_{r, s}$. (c) $\\Delta=1.07 \\times K_r\\bar n_{r, s}$. } \\end{figure} As a result of the Kerr effect, the resonant frequency $\\omega_r(n_r(t))$ changes during the ramp-up and ramp-down segments, which makes the phase and amplitude of the field deviate from those expected in the linear case. In principle, such deviations can be removed through chirped pulses where the detuning $\\Delta$ is updated during the ramping, or through calibrating the phase and amplitude of the resulting state after ramping and adjusting the drive accordingly."}
{"input": "the ramping times $t_{\\uparrow}$ and $t_{\\downarrow}$. From \\cref{eq:rampup_amplitude}, a shorter ramping time requires a stronger amplitude, and the resulting pulse will have a broader spectrum, which makes it possible for the pulse to remain near-resonant despite the Kerr effect. We choose a \\qty{40}{\\ns} ramping time, which gives $1/ t_\\uparrow=\\qty{25}{\\MHz}$. This is much larger than the frequency shift $|\\omega_r(n_r=3000) - \\omega_r(n_r=0)|/2\\pi \\approx \\qty{357}{\\kHz}$. Here, we take the same time length for ramp-up and ramp-down, $t_\\uparrow=t_\\downarrow$. We note there are also drawbacks of large amplitude ratio $\\varepsilon_\\uparrow/\\varepsilon_s$, which may cause relatively stronger pulse distortion and also have higher requirements for the resolution of the microwave instrument, such as DACs. % \\abc{I am confused about factors of $2\\pi$ here. 1/time is an angular frequency.} We show numerical simulations of the steady-state pulses in \\cref{fig:S3_NumericalSSpulse} using experimental parameters. In addition to the case where $\\Delta=K\\bar n_{r, s}$, we also show the results for an under-detuned pulse and an over-detuned pulse, which would be the case for possible miscalibration of the Kerr coefficient or the photon numbers. We find that the under-detuned pulse has better tolerance to such miscalibration, whereas the over-detuned pulse could easily fail to stabilize the photon numbers. \\begin{figure*}[!t] \\includegraphics{figS4.pdf} \\caption{ \\label{fig: S4_Calibrations} Calibration results of $R_4$, which is the resonator used in \\cref{fig: 4_LandauZener}. (a) Measured resonator frequency and extrapolation at different amplitudes. (b) Free decay measurement to extract the linewidth $\\kappa$ of the resonator. (c) Steady-state experiment with $\\Delta=0$ to extract the Kerr $K_r$ of the resonator."}
{"input": "with theoretical (blue dots) and calibrated (red dots) ramping amplitude ratio for compensating possible miscalibration and power compression of the experimental apparatus. (e) The measured and fitted relation between the instrument amplitude and the steady-state photon number $\\bar n_{r,s}$. (f) The analytically calculated and numerically simulated relation between the drive amplitude $\\varepsilon_s$ and the steady-state photon number $\\bar n_{r,s}$. } \\end{figure*} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\subsection{\\label{sec: LZ_pulse}Pulse in Landau-Zener sequence} In previous sections, we explained the method to construct the pulse in our steady-state experiments. In our Landau-Zener experiments, we want to control the resonator such that the average photon number changes from $\\bar n_{r, i}$ to $\\bar n_{r, f}$ in a given time $t_s$. Here, we choose the ramping amplitudes such that they correspond to the ramp-up amplitude $\\varepsilon_\\uparrow$ in a steady-state sequence with $\\bar n_{r, s}=\\bar n_{r, i}$ and the ramp-down amplitude $\\varepsilon_\\downarrow$ in a steady-state sequence with $\\bar n_{r, s}=\\bar n_{r, f}$. We leave the amplitude of the quasi-steady-state segment $\\varepsilon_s$ and the detuning $\\Delta$ as two free parameters in a numerical optimization for the target pulses. The cost function is designed to minimize the difference between numerical results and the desired average photon numbers, which are $\\bar n_{r, i}$, $\\bar n_{r, f}$, and $0$ at times $t=t_{\\uparrow}$, $t=t_{\\uparrow}+t_s$, and $t=t_{\\uparrow}+t_s+t_{\\downarrow}$. For relatively short $t_s$, this method results in monotonically increasing photon number during the quasi-steady-state segment, the derivative of which can be easily extracted."}
{"input": "of the photon number $d\\bar n_r(t)/dt$. Two different parameterizations are used to adjust the slope, as each of them can only reach a limited range of Landau-Zener speeds. The time-varying parameterization, where we fix $\\bar n_{r, i}=1300$ and $\\bar n_{r, i}=1700$ and then change the time $t_s$, fails when $t_s$ becomes comparable to the relaxation time. As a result, the adiabatic region cannot be reached. On the other hand, the number-varying parameterization, where we fix the time $t_s=\\qty{10}{\\us}$, the sum $\\bar n_{r, i} + \\bar n_{r, f}=3000$, and then change the difference $\\bar n_{r, f} - \\bar n_{r, i}$, has a maximum allowable difference $|\\bar n_{r, f} - \\bar n_{r, i}| < 3000$ and thus a limited diabaticity. It is the combination of the two parameterizations that enables us to explore a wide range of Landau-Zener speeds. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\subsection{\\label{sec: calibrations}Calibration procedures} In this section, we describe the calibration procedures for the steady-state sequence and the Landau-Zener sequence. We show the results for $R_4$, the readout resonator coupled to $Q_B$, and we focus on the case where $Q_B$ is prepared in $\\ket{0}$ at the beginning of the sequence. The first step to calibrate the shaped pulse is to measure the resonator frequency at the single-photon level. We perform resonator spectroscopy for various drive amplitudes and fit each spectroscopy result to a Lorentzian function to extract a frequency~\\cite{wang2025}."}
{"input": "shown in \\cref{fig: S4_Calibrations}(a). The resonator spectroscopy is also used to extract the dispersive shift $\\chi_{01} \\equiv \\omega_{r, \\ket{1}} - \\omega_{r, \\ket{0}} = 2\\pi \\times \\qty{-205}{\\kHz}$. Due to the finite duration of the spectroscopy pulse, the fitted quality factor may have systematic errors. In such cases, a free decay experiment is preferable to extract the linewidth $\\kappa$ of the resonator. At any time $t$, we use transmon spectroscopy to measure the ac-Stark shift $\\delta_{\\rm ac}(t)$ of the $\\ket{0}\\leftrightarrow\\ket{1}$ transition frequency of the coupled transmon. These ac-Stark shifts are then fitted to an exponential function, as shown in \\cref{fig: S4_Calibrations}(b), which gives $\\kappa_{\\ket{0}} = 2\\pi \\times \\qty{127}{\\kHz}$. Using the fitted linewidth, we can calculate the ramping amplitude ratio in \\cref{eq:rampup_amplitude}. The Kerr coefficient is then measured by applying the three-segment pulse. Here, we set $\\Delta=0$ without any prior knowledge of the value of the Kerr coefficient, and the photon number $\\bar n_r$ is not stabilized. In that case, the Kerr effect manifests through the time-dependence of $\\bar n_r(t)$. We calculate the measured photon number using \\begin{equation} \\bar n_r(t)=\\delta_{\\rm{ac}}(t)/\\chi_{01}, \\label{eq:photons_stark_shift} \\end{equation} The results are fitted to the numerical simulation of the three-segment pulse, where the Kerr value $K_r$, the effective drive amplitude $\\varepsilon_s$, and the detuning $\\Delta$ are treated as fitting parameters, as shown in \\cref{fig: S4_Calibrations}(c). We leave the detuning $\\Delta$ as a free parameter to further correct the single-photon frequency extrapolated from \\cref{fig: S4_Calibrations}(a). As a result, we find $K_{r, \\ket{0}}=2\\pi \\times \\qty{-119}{\\Hz}$."}
{"input": "we have the minimal parameters to run the steady-state experiment: we can set $\\Delta=K_r(\\varepsilon_s/\\kappa)^2$ and choose the ramping amplitude ratio based on \\cref{eq:rampup_amplitude}. The result is shown as blue dots in \\cref{fig: S4_Calibrations}(d). Although the photon number is stable for a long time, it does not reach its steady state immediately after the ramp-up segment because the actual amplitude ratio required to be applied on the device may deviate from the theoretical value $\\varepsilon_\\uparrow/\\varepsilon_s \\approx 63.3$ that we set to the instrument. Such deviation may come from miscalibration of the linewidth $\\kappa$ or the power compression of the instrument. We correct it by empirically adjusting the amplitude ratio to $\\varepsilon_\\uparrow/\\varepsilon_s = 68.5$, and the result is shown as red dots in \\cref{fig: S4_Calibrations}(d). Another consequence of power compression is that the effective drive amplitude $\\varepsilon_s$ is not proportional to the instrument amplitude. We perform the steady-state experiment at different instrument amplitudes and extract the corresponding steady-state photon numbers $\\bar n_{r,s}$. The results are fitted to a phenomenological model $y(x)=Ax^m$, and we get $m \\approx 1.776 \\neq 2$, as shown in \\cref{fig: S4_Calibrations}(e). This relation gives us the conversion between the instrument amplitude and the $\\bar n_{r,s}$ reported in \\cref{fig: 4_LandauZener}(b). We also calculate a similar conversion between the steady-state photon number $\\bar n_{r,s}$ and the drive amplitude $\\varepsilon_s$ on the resonator using numerical simulation, as shown in \\cref{fig: S4_Calibrations}(f). The results agree well with analytical prediction $\\bar n_{r,s} = \\varepsilon_s^2 / \\kappa^2$."}
{"input": "the calibration procedures discussed in \\cref{sec: calibrations}, the photon numbers are extracted using \\cref{eq:photons_stark_shift}. This relation is based on the dispersive Hamiltonian. For a multilevel system coupled to a single-mode harmonic oscillator, the dispersive Hamiltonian up to sixth order in perturbation is ($\\hbar=1$) \\begin{equation} \\begin{split} \\hat{H}_{\\rm{disp}} &= \\sum_j \\omega_j \\ket{j}\\bra{j} + \\omega_r \\hat{a}^{\\dagger}\\hat{a} \\\\ &+ \\sum_j \\chi_j \\hat{a}^{\\dagger}\\hat{a} \\ket{j}\\bra{j} + \\sum_j \\frac{\\eta_j}{2} \\hat{a}^{\\dagger}\\hat{a}^{\\dagger}\\hat{a}\\hat{a} \\ket{j}\\bra{j}\\\\ &+ \\sum_j \\frac{\\mu_j}{6} \\hat{a}^{\\dagger}\\hat{a}^{\\dagger}\\hat{a}^{\\dagger}\\hat{a}\\hat{a}\\hat{a} \\ket{j}\\bra{j}. \\label{eq:full_dispersive_hamiltonian} \\end{split} \\end{equation} Below, we discuss possible errors when using this equation. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\subsubsection{\\label{sec:Kerr_effect}Kerr effect} The dispersive Hamiltonian in \\cref{eq:full_dispersive_hamiltonian} is often truncated to the second order, with the dispersive shift defined as $\\chi_{01}=\\chi_1-\\chi_0$. However, at large photon numbers, the state-dependent four-wave mixing Kerr $\\eta_j$ and six-wave mixing Kerr $\\mu_j$ also have non-negligible contributions to the dispersive shift. In other words, the denominator of \\cref{eq:photons_stark_shift} depends on photon number, and the relation between photon number and ac-Stark shift is therefore not linear at large photon numbers. \\begin{figure}[!b] \\includegraphics{figS5.pdf} \\caption{ \\label{fig: S5_6WaveMixingKerr} Estimated ac-Stark shift as a function of photon number for different orders in perturbation theory. } \\end{figure} To estimate the possible errors from these higher-order effects, we calculate the ac-Stark shift $\\delta_{\\rm{ac}}$ using $\\eta_j$ and $\\mu_j$ extracted from numerical diagonalization with the parameters shown in \\cref{tab: device_params}. The results are shown in \\cref{fig: S5_6WaveMixingKerr}. We find that at 1500 photons, the critical photon number in Fig. 4 of the main text, there could be an underestimation of the photon number of about 100 to 200 photons for a given $\\delta_{\\rm{ac}}$."}
{"input": "to lowest-order in $\\Omega(t)$ is \\begin{equation} \\begin{split} P_{\\ket{1}} &= |\\bra{1}e^{-i\\bar{H}_{\\rm{spec}}^{(1)}T}\\ket{0}|^2 \\\\ &= 4 \\theta^2 {\\rm sinc}^2 \\left( \\frac{1}{2}\\sqrt{\\theta^2 + T^2 (\\Delta-\\chi_{01}\\bar n_r)^2} \\right). \\label{eq:AHT_P1} \\end{split} \\end{equation} Here, we introduced the rotation angle $\\theta \\equiv \\int_0^T \\Omega(t) dt$ and the time-averaged photon number $\\bar n_r \\equiv 1/T \\int_0^T n(t)dt$. The spectroscopic response peaks at $\\Delta=\\chi_{01}\\bar n_r$, which is proportional to the time-averaged photon number instead of the instantaneous photon number. Hence, when we fit the Kerr value $K_r$ in \\cref{fig: S4_Calibrations}(c), the numerical simulation results are uniformly averaged over a time window with duration $T$. Going to higher orders in perturbation, however, the excitation probability $P_{\\ket{1}}$ depends on the specific shape of $\\Omega(t)$. This leads to errors in the fitted Kerr value. We also note that the resonator state generated by a classical drive is usually not a Fock state. The photon number discussed here should thus be thought of as a weighted average over different Fock states. In the number-splitting regime where the coupling between the transmon and the resonator is strong, $\\chi_{01}>1/T$, there is more than one peak in the spectroscopic response~\\cite{schuster2007}."}
{"input": "\\section{\\label{sec: full_fig4b} Identification of final states in the steady-state experiment} In \\cref{sec: LandauZener}, we mentioned that $\\ket{6}$ is one of the final states in the steady-state experiment. Here we give more details about this identification. \\begin{figure}[!ht] \\includegraphics{figS6.pdf} \\caption{ \\label{fig: S6_full_fig4b} Identification of final states in the steady-state experiment. (a) A linecut of the steady-state experiment results shown in \\cref{fig: 4_LandauZener}(b) at 1500 photons. (b) Normalized Floquet quasieneregies $\\epsilon_j/\\omega_d$ for $Q_B$. We take $n_g=0$. } \\end{figure} As shown in \\cref{fig: S6_full_fig4b}(a), we found apparent population increase in $\\ket{6}$, leaving $\\ket{7}$ nearly unaffected. The large population observed in states $\\ket{8}$ or higher suggests that the population in state $\\ket{6}$ is then further transferred to higher excited states. These observations are consistent with the Floquet analysis shown in \\cref{fig: S6_full_fig4b}(b). Following the branch $\\ket{0_f}$, we encounter two nearby avoided crossings around $\\bar n_r \\sim 1800$, involving both branches $\\ket{6_f}$ and $\\ket{14_f}$. During the long steady-state segment, the population can be transferred to either of these branches. The $\\ket{6_f}$ and $\\ket{14_f}$ branches also show an avoided crossing at much lower photon number ($\\bar n_r \\sim 100$). As a result, in the experiment, the diabatic ramp-down segment leads to the population in branch $\\ket{14_f}$ being assigned as $\\ket{6}$ and to the population in branch $\\ket{6_f}$ being assigned as $\\ket{8_+}$. Notice that the critical photon number shown in \\cref{fig: S6_full_fig4b} is slightly larger than the measured results ($\\sim 1500$ photons) in \\cref{fig: 4_LandauZener}."}
{"input": "\\section{Introduction} \\label{sec:intro} Over the last four decades, supersymmetric solutions of supergravity theories have proven crucial for understanding various aspects of string theory and its implications on black hole physics. Supersymmetric soliton solutions played a central role in unraveling the underlying hidden dualities between the different string theories \\cite{Hull:1994ys} and understanding the microscopic origin of Bekenstein-Hawking entropy in string theory \\cite{Strominger:1996sh}. Notably, in $\\mathcal{N}=2$ supergravity, horizons of the extremal black holes are fully supersymmetric, and using this fact, a macroscopic entropy formula was derived, which facilitated a successful comparison with the microscopic state counting in various $\\mathcal{N}=2$ string compactifications \\cite{Mohaupt:2000mj}. The $\\mathcal{N}$-extended supergravity\\footnote{When we say $\\mathcal{N}$-extended supergravity, we mean ungauged $\\mathcal{N}$-extended Poincar\\'e supergravity throughout the paper.} theories are invariant under diffeomorphism, local Lorentz transformation as well as local supersymmetry with $\\mathcal{N}$ arbitrary spinor parameters $\\epsilon^i(x)$, where $i$ runs from $1$ to $\\cN$. Let us collectively denote the bosons and fermions in the theory as $\\{B(x)\\}$ and $\\{F(x)\\}$ respectively. A solution is said to be supersymmetric, if there exists a subset $\\{\\epsilon^j(x)\\}$ of local supersymmetries such that \\begin{equation}\\label{KSE} \\delta_{\\epsilon^j(x)}\\{B(x)\\}=0,~~\\delta_{\\epsilon^j(x)}\\{F(x)\\}=0, ~~j=1,\\dots,M\\leq \\mathcal{N} \\end{equation} The spinors in the subset are known as Killing spinors and the schematic equations in \\eqref{KSE} are the Killing Spinor equations. For classical field configurations, the fermions vanish, and the first condition of $\\eqref{KSE}$ is trivially satisfied. A fully supersymmetric solution supports the maximal number of Killing spinors, i.e., $M=\\mathcal{N}$ ( $4 \\mathcal{N}$ conserved supercharges).\\footnote{In four spacetime dimensions, the basic spinors are Majorana spinors with four real components."}
{"input": "in other dimensions, look at \\cite{Freedman:2012zz}. } For $M< \\mathcal{N}$, the solution is said to preserve $M/\\mathcal{N}$ of total supersymmetries. The Killing spinor equations serve a dual purpose. Firstly, for a given solution, one can find the Killing spinors by solving the vanishing gravitini field variations \\begin{equation} \\delta \\psi^{i}_{\\mu}= \\mathscr{D}_{\\mu}\\epsilon^i(x)+\\dots=0, \\end{equation}which are first order differential equations in the spinor parameters and the variation of the other fermions are linear in the spinor parameters. The form of the supercovariant derivative $\\mathscr{D}_{\\mu}$ depends on the details of the supergravity theory. The integrability conditions of these equations are ensured from the vanishing variation of the supercovariant gravitino field strength \\begin{equation} \\delta R_{\\mu\\nu}{}^{i}= \\mathscr{D}_{[\\mu}\\mathscr{D}_{\\nu]}\\epsilon^{i}(x)+\\dots =0 \\end{equation} Secondly, assuming the existence of the Killing spinors, one can consider \\eqref{KSE} as conditions imposed on the fields and find the field configurations that satisfy the conditions. To obtain $M/\\mathcal{N}$-supersymmetric solutions, one must impose appropriate embedding conditions to make $\\mathcal{N}-M$ Killing spinors dependent. Minkowski space is always a maximally supersymmetric solution to any $\\mathcal{N}$-extended supergravity in any spacetime dimensions possessing $\\mathcal{N}$ constant Killing spinors, but various other maximally supersymmetric solutions exist in different supergravity theories in diverse dimensions. The first classification of supersymmetric solutions using Killing spinor equations was performed by Tod in \\cite{Tod:1983pm} in the context of pure $\\mathcal{N}=2$ two-derivative supergravity. The fully supersymmetric solutions are Bertotti-Robinson geometry $(AdS_2\\times S^2)$, Minkowski space and pp-wave spacetimes \\cite{Kowalski-Glikman:1985fjy, LopesCardoso:1998tkj}."}
{"input": "supersymmetric solution in four-dimensional $\\mathcal{N}=4$ supergravity. In this paper, we prove this conjecture for $\\mathcal{N}=4$ supergravity in presence of higher derivative corrections. We also show that flat space remains the only maximally supersymmetric solution even for $\\cN=3$ supergravity in the presence of higher derivative corrections. We also discuss the reason behind the completely different structure of supersymmetric vacua for $\\cN=4$ and $\\cN=3$ supergravity in contrast with $\\cN=2$ supergravity which admits richer vacua. We work in the framework of conformal supergravity. Conformal supergravity \\cite{Fradkin:1985am} is a supersymmetric extension of conformal gravity or, equivalently, the conformal extension of Poincar\\'e supergravity. These theories possess a large number of local symmetries, namely diffeomorphism, local Lorentz transformations ($M$), dilatations ($D$), special conformal transformations ($K$), R-symmetries, ordinary supersymmetry (which we will often refer to as $Q$-supersymmetry), and special supersymmetry (often referred to as $S$-supersymmetry). A brief outline of the superconformal formalism is as follows \\cite{Freedman:2012zz}: \\begin{itemize} \\item One starts with a gauge theory of the corresponding superconformal group and applies appropriate curvature constraints to make it a theory of gravity. \\item Consequently, some of the gauge fields become dependent. \\item To close the algebra off-shell, one needs to introduce extra covariant fields, which together with the independent gauge fields form the so-called Weyl multiplet. The superconformal algebra is modified to a soft superconformal algebra where the structure constants become structure functions which depend on the extra covariant matter fields. These extra covariant fields are sometimes referred to as ``auxiliary fields''."}
{"input": "one needs to introduce extra matter multiplets such as vector multiplet, hypermultiplet etc, which form representations of the soft superconformal algebra introduced above. The superconformal algebra will close off-shell on some of these multiplets and they are referred to as off-shell multiplets and on some other multiplets the superconformal algebra will close upon using their field equations. Such multiplets are referred to as on-shell multiplets. \\item One constructs a superconformally invariant action for these multiplets (both Weyl and matter) using various known methods in the literature. See for example, chiral density formula \\cite{deRoo:1980mm, Mohaupt:2000mj} or vector-tensor density formula \\cite{deWit:1982na, Claus:1997fk, deWit:2006gn} for construction of actions in $\\cN=2$ conformal supergravity. For $\\cN=4$ conformal supergravity a more general action principle based on covariant superforms was introduced in \\cite{Butter:2016mtk, Butter:2019edc} which was also used in $\\cN=3$ conformal supergravity \\cite{Hegde:2022wnb} and $\\cN=2$ conformal supergravity \\cite{Hegde:2019ioy}. \\item A subset of the matter multiplets are used as compensating multiplets to transition from conformal to Poincar{\\'e} supergravity \\cite{Kaku:1978ea, deWit:1979dzm, Kaku:1978nz,deWit:1982na, Freedman:2012zz}. These subset of matter multiplets have the right number of degrees of freedom to compensate for the additional symmetries necessary to transition from conformal to Poincar{\\'e} supergravity. \\item Additionally, some of the auxiliary fields from the Weyl multiplet can be removed by using their field equations if one wants the theory purely in terms of the physical fields."}
{"input": "\\section{\\texorpdfstring{$\\cN=4$ Higher Derivative Supergravity} {N=4 Higher Derivative Supergravity}} \\label{N4sugra} In this section, we review four dimensional matter-coupled higher derivative $\\mathcal{N}=4$ supergravity using the framework of conformal supergravity. For a detailed construction of $\\cN=4$ conformal supergravity as well as the construction of matter coupled $\\cN=4$ supergravity using the superconformal framework, see \\cite{Bergshoeff:1980is,deRoo:1984zyh,Ciceri:2015qpa, Butter:2016mtk,Butter:2019edc}. The relevant multiplets are the $\\cN=4$ Weyl multiplet and the $\\cN=4$ vector multiplet. Among the vector multiplets, there are a fixed number of ``compensating multiplets'' that provide the necessary degrees of freedom to compensate for the extra symmetries required to go from conformal supergravity to Poincar{\\'e} supergravity. They also provide the necessary gauge fields which play the role of graviphotons in Poincar{\\'e} supergravity. Typically these multiplets come with the wrong sign of the kinetic term in their action in conformal supergravity. This ensures that the Einstein-Hilbert term and its supersymmetrization come with the right sign of the kinetic term. There are also vector multiplets that play the role of physical matter multiplets coupled to $\\cN=4$ Poincar\\'e supergravity. Such vector multiplets always come with the right sign of the kinetic term in their action in conformal supergravity so that their kinetic terms remains with the right sign even in Poincar{\\'e} supergravity. The Weyl multiplet is an off-shell multiplet. It contains superconformal gauge and matter fields that describe $128+128$ bosonic and fermionic off-shell degrees of freedom."}
{"input": "gauge fields $e_{\\mu}{}^{a}$, $\\omega_{\\mu}{}^{ab}$ and $\\psi_{\\mu}{}^i$, can be read off from their gauge transformations and are given as follows: \\begin{subequations} \\begin{align} R(P)_{\\mu\\nu}{}^{a}=\\,&2\\mathscr{D}_{[\\mu}e_{\\nu]}{}^a-\\Bar{\\psi}_{[\\mu}{}^i\\gamma^a \\psi_{\\nu]i}, \\label{eq:RP} \\\\ R(M)_{\\mu\\nu}{}^{ab}=\\,&2 \\partial_{[\\mu}\\omega_{\\nu]}{\\!}^{ab}-2\\omega_{[\\mu}{\\!}^{ab}\\omega_{\\nu] c}{\\!}^b-4e_{[\\mu}{\\!}^{[a}f_{\\nu]}{\\!}^{b]} \\nonumber \\\\ &+\\frac{1}{2}[\\Bar{\\psi}_{[\\mu}\\gamma^{ab}\\phi_{\\nu]i}-2\\Bar{\\psi_{[\\mu i}}\\gamma_{\\nu]}R(Q)^{ab i}+ 2\\Bar{\\psi}_{\\mu i}\\psi_{\\nu j}T^{ab ij}+\\text{h.c}], \\label{eq:RM} \\\\ R(Q)_{\\mu\\nu}^{i}=\\,& 2\\mathscr{D}_{[\\mu}\\psi_{\\nu]}^i-\\gamma_{[\\mu}\\phi_{\\mu]}{\\!}^i-\\frac{1}{2}\\gamma^{ab}\\gamma_{[\\mu}\\psi_{\\nu]j}T_{ab}{\\!}^{ij}+\\frac{1}{2}\\varepsilon^{ijkl}\\Bar{\\psi}_{[\\mu j}\\psi_{\\nu]k}\\Lambda_l\\;, \\label{eq:RQ} \\end{align} \\end{subequations} where, the derivative $\\mathscr{D}_{\\mu}$ is covariant with respect to all bosonic symmetries except the special conformal transformation. The curvatures satisfy the following constraints: \\begin{subequations}\\label{eq:Constraints} \\begin{align} R(P)_{\\mu\\nu}{}^{a} &= 0, \\label{eq:RPZero} \\\\ e^{\\nu}{}_{b}R(M)_{\\mu\\nu}{}^{ab} &= 0, \\label{eq:RMZero} \\\\ \\gamma^{\\mu}R(Q)_{\\mu\\nu}^{i} &= 0. \\label{eq:RQZero} \\end{align} \\end{subequations} The above constraints make some of the gauge fields such as $\\omega_{\\mu}^{ab}$, $f_{\\mu}^a$ and $\\phi_{\\mu}^i$ dependent and their expressions can be found by solving the above constraints \\eqref{eq:Constraints}. In particular the expression for the gauge field $f_{\\mu}^a$ corresponding to the special conformal transformation can be obtained by solving the constraint \\eqref{eq:RMZero} and is given as below (up to fermionic terms which we have omitted). \\begin{align}\\label{eq:FM} f_{\\mu}{\\!}^a &= \\frac{1}{2} R_{\\mu}{\\!}^{a}-\\frac{1}{12}Re_{\\mu}{\\!}^{a}\\;. \\end{align} In the above equations, we have used the following definitions: \\begin{align} R_{\\mu}{}^{a}=e^{\\nu}{}_{b}R_{\\mu\\nu}{}^{ab}\\;, \\;\\; R=e^{\\mu}{}_{a}R_{\\mu}{}^{a} \\end{align} where $R_{\\mu\\nu}{}^{ab}\\equiv \\left(R(M)_{\\mu\\nu}{}^{ab}\\right)_{|f=0}$. In the Poincar{\\'e} gauge $b_\\mu=0$, $R_{\\mu\\nu}{}^{ab}$, $R_{\\mu}{}^{a}$ and $R$ have the interpretation of Riemann tensor, Ricci tensor and Ricci scalar respectively. The supersymmetry transformations (both $Q$ as well as $S$) of all the curvatures can be obtained and are given in \\cite{Bergshoeff:1980is,Ciceri:2015qpa,Butter:2019edc}. For our purpose, we need the transformation for the fermionic curvature $R(Q)_{ab}^i$ which we give below. \\begin{align}\\label{rqvary} \\delta {R(Q)_{ab}}^{i}=\\,&-\\frac{1}{2}R(M)_{abcd}\\gamma^{cd}\\epsilon^{i}+\\frac{i}{8}(\\gamma^{cd}\\gamma_{ab}+\\frac{1}{3}\\gamma_{ab}\\gamma^{cd})[R(V)_{cd}{\\!}^i{\\!}_{j}\\epsilon^j+\\tfrac12 i F_{cd}\\epsilon^i+\\slashed{D}T_{cd}{\\!}^{ij}\\epsilon_{j}]&\\nonumber\\\\&+\\frac{1}{2}(\\gamma^{cd}\\gamma_{ab}+\\tfrac13 \\gamma_{ab}\\gamma^{cd})T_{cd}{\\!"}
{"input": "\\section{\\texorpdfstring{$\\mathcal{N} =4$ Fully Supersymmetric Solution}{N=4 Fully Supersymmetric Solution}} \\label{N4susy} In this section, we find the fully supersymmetric solution in $\\cN=4$ higher derivative Poincar\\'e supergravity. A fully supersymmetric $\\cN=4$ solution implies that the background geometry possesses four covariantly constant Killing spinors ($16$ conserved supercharges). We assume the existence of such spinors and demand the $Q$- supersymmetry transformation of the fermions to vanish. The vanishing $Q$-supersymmetry variations of the fermions put stringent conditions on the bosonic fields. We have to find field configurations that satisfy these conditions. Since we are in the superconformal formalism, the fermions also transform under $S$-supersymmetry. We can only demand the fermions to vanish under $Q$-supersymmetry up to an $S$- transformation. An alternative way to surpass this is to find combinations of spinors that are $S$-invariant. This is done by first identifying a spinor that transforms as a pure gauge under $S$-supersymmetry. A linear combination of other spinors with this spinor can give us $S$-invariant spinors. We construct such $S$-invariant spinors and demand that their $Q$-supersymmetry transformations vanish. This procedure is independent of the $S$-gauge. We are going to follow this procedure along the lines of \\cite{Mohaupt:2000mj, LopesCardoso:2000qm}. The spinor that transforms as a pure gauge under $S$-supersymmetry can be obtained using the fields of $\\mathcal{N}=4$ vector multiplet as shown below: \\begin{equation}\\label{in} \\zeta_{i}= 2\\phi^{-2} \\phi_{ij}^{I}{\\psi^{j J}}\\eta_{IJ},~\\phi^2=\\phi^{Iij}\\phi^{J}_{ij}\\eta_{IJ} \\end{equation} This spinor transforms under under $Q$- and $S$- supersymmetry as \\begin{flalign}\\label{zeta} \\delta \\zeta^{i}= 2\\phi^{-2}\\phi^{I ij}\\left[-\\frac{1}{2\\Phi}\\gamma^{ab}\\epsilon_{j}\\left(\\hat{F}_{ab}^{+J}+\\Phi^{*}T_{ablk}\\phi^{Jlk}\\right)-2\\epsilon^{k}\\slashed D\\phi^{J}_{jk} + \\epsilon_{k}E_{jl}\\phi^{Jlk}\\right]\\eta_{IJ}+\\eta^{i}. \\end{flalign} We now start analyzing the fermionic variations."}
{"input": "back in \\eqref{electriccon} implies \\begin{align}\\label{fab0} \\hat{F}_{ab}^{+I}=0 \\end{align} Similarly contracting \\eqref{phicon} with an $M\\eta$ and using the property \\eqref{Mprop}, we get \\begin{equation}\\label{deriPhi} D_{\\mu}\\phi_{ij}^{I}=0\\;. \\end{equation} Since $\\phi^{I}_{ij}$ is inert under special conformal transformation, the fully supercovariant derivative $D_{\\mu}$ is the same as the covariant derivative $\\mathscr{D} _{\\mu}$ without the $K$-connection. Hence \\eqref{deriPhi} also implies $\\mathscr{D}_{\\mu}\\phi^{I}_{ij}=0$. The condition on the matrix valued scalar field $M^{IJ}$ \\eqref{Mdef} that follows from this is \\begin{equation}\\label{Mder} \\partial_{\\mu}M^{IJ}=\\mathscr{D}_{\\mu}M^{IJ}=\\phi^{I ij}\\mathscr{D}_{\\mu}\\phi_{ij}^{I}+\\phi_{ij}^{I}\\mathscr{D}_{\\mu}\\phi^{I ij}=0\\;. \\end{equation} In the second step of the above equation, we have used the fact that the only non-trivial bosonic superconformal transformation possessed by $M^{IJ}$ is dilatation and because of the $K$-gauge condition $b_\\mu=0$, we have $\\partial_{\\mu}M^{IJ}=\\mathscr{D}_{\\mu}M^{IJ}$. The equation \\eqref{Mder} further implies: \\begin{align}\\label{mijcon} M^{IJ}=\\text{constants} \\end{align} Taking a covariant derivative on the equation-\\eqref{phicon}, we get: \\begin{align} D_{\\mu}D_{\\nu}\\phi_{ij}^{I}=\\,& -\\tfrac23\\Big(M^{IK}D_{\\mu}D_{\\nu}\\phi_{ij}^{J}\\eta_{JK}+D_{\\mu}M^{IK}D_{\\nu}\\phi_{ij}^{J}\\eta_{JK}\\Big) \\end{align} Multiplying this equation by an $M\\eta$ and using the condition \\eqref{deriPhi} and the property \\eqref{Mprop}, we obtain: \\begin{align}\\label{deri2phi} D_{\\mu}D_{\\nu}\\phi^{I}_{ij}=0. \\end{align} Since $D_{\\mu}\\phi^{I}_{ij}$ has a nontrivial special conformal transformation, one gets: \\begin{equation}\\label{double} D_{\\mu}D_{\\nu}\\phi_{ij}^{I}=\\mathscr{D}_{\\mu}\\mathscr{D}_{\\nu}\\phi_{ij}^{I}+f_{\\mu\\nu}\\phi^{I}_{ij}. \\end{equation} Now plugging back \\eqref{deriPhi} and \\eqref{deri2phi} in \\eqref{double}, we obtain that the $K$-gauge field vanishes, \\begin{align}\\label{kgauge} f_{\\mu\\nu}=0 \\end{align} Combining this with the condition \\eqref{rm0} implies that all the components of the Riemann curvature tensor vanishes i.e. \\begin{align} R(M)_{\\mu\\nu}{\\!}^{cd}=2 \\partial_{[\\mu}\\omega_{\\nu]}{\\!}^{ab}-2\\omega_{[\\mu}{\\!}^{ab}\\omega_{\\nu] c}{\\!}^b= R_{\\mu\\nu}{}^{ab}=0 \\end{align} Based on our analysis in this section, we can conclude that there is a unique solution in $\\cN=4$ supergravity with or without higher derivative corrections that is fully supersymmetric. In this solution, the underlying geometry is a flat space with no electromagnetic fluxes."}
{"input": "\\section{\\texorpdfstring{$\\mathcal{N} =3$ Higher Derivative Supergravity}{N=3 Higher Derivative Supergravity}}\\label{N3sugra} In this section, we review four dimensional $\\mathcal{N}=3$ supergravity using the framework of conformal supergravity. For a detailed construction of $\\cN=3$ conformal supergravity, see \\cite{vanMuiden:2017qsh,Hegde:2018mxv, Hegde:2021rte}. Using these results, higher derivative $\\mathcal{N}=3$ Poincar\\'e supergravity was constructed in \\cite{Hegde:2022wnb}. The crucial ingredients are the $\\mathcal{N}=3$ Weyl and vector multiplets. A Poincar\\'e supergravity coupled to $n_V$ vector multiplets is realised as a Weyl multiplet coupled to $n_V+3$ vector multiplets in conformal supergravity where the three compensating vector multiplets have a wrong sign kinetic term. As explained in the introduction, one gauge fixes additional symmetries using the compensators and solves the auxiliary field equations to write the action in Poincar\\'e supergravity variables. In \\cite{Hegde:2022wnb} these further steps were carried out only for the case of pure supergravity. However, for our analysis it is not necessary to eliminate the auxiliary fields since our supersymmetry analysis for the solution is best done in off-shell conformal supergravity variables. We will proceed therefore by considering fully supersymmetric solutions corresponding to the action in conformal supergravity with $n_V+3$ vector multiplets, which is equivalent to higher derivative matter coupled Poincar\\'e supergravity. The $\\mathcal{N}=3$ Weyl multiplet is an off-shell multiplet and contains superconformal gauge and matter fields that describe $64+64$ bosonic and fermionic off-shell degrees of freedom. The independent gauge fields include the vierbein $e_{\\mu}^a$, the gravitini $\\psi_{\\mu}^i$ ($Q$-supersymmetry gauge fields), the $SU(3)$ gauge field ${{V_{\\mu}}^i}_{j}$ and the dilatation gauge field $b_{\\mu}$."}
{"input": "${\\omega_{\\mu}}^{ab}$, the gauge field associated with the special conformal transformation $f_{\\mu}^{a}$, the chiral $U(1)$ gauge field $A_{\\mu}$ and $S$-supersymmetry gauge field $\\phi_{\\mu}^{i}$, where $i,j=1,2,3$. \\begin{table}[H] \\small \\centering \\begin{tabular}{|p{1cm}|p{4cm}| p{4cm}| p{1 cm}| p{1cm}| p{1cm}|} \\hline Fields& Symmetries(Generators)& Name/Restrictions& SU(3)& $w$ & $c$\\\\ \\hline ${e}_{a}^{\\mu}$& Translations($P$)& vierbein& \\textbf{1}& -1&0\\\\ \\hline ${\\omega_\\mu}^{ab}$& Lorentz($M$)& spin connection& \\textbf{1}& 0&0\\\\ \\hline $b_{\\mu}$&Dilatations& Dilatation gauge field& \\textbf{1}&0&0\\\\ \\hline ${{V_{\\mu}}^i}_{j}$& $SU(3)$& $SU(3)_{R}$ gauge field;\\newline ${{V_{\\mu}}^i}_{i}=0$, \\newline ${V_{\\mu}i}^{j}\\equiv ({{V_{\\mu}}^i}_{j})^*=-{{V_{\\mu}}^j}_{i}$ &\\textbf{8}&0&0\\\\ \\hline ${f_{\\mu}}^a$&conformal boosts& $K$-gauge field &\\textbf{1}&1&0\\\\ \\hline $A_{\\mu}$& $U(1)_R$& $U(1)_R$ gauge field & \\textbf{1}&0&0\\\\ \\hline $E_{i}$& & Complex & $\\overline{\\textbf{3}}$&1&-1\\\\ \\hline ${T_{ab}}^{i}$& & $T_{ab}^{i}=\\frac{1}{2} {\\varepsilon_{ab}}^{cd}T_{cd}^{i}$&\\textbf{3} &1 &1\\\\ \\hline ${D^{i}}_{j}$& & ${D}_{i}{\\!}^{j}\\equiv (D^{i}{\\!}_{j})^*=D^{j}{\\!}_{i}$& $\\textbf{8}$&2 &0\\\\ \\hline $\\phi_{\\mu i}$ & $S$-supersymmetry & $S$-gauge field; $\\gamma_{5}\\phi_{\\mu i}=\\phi_{\\mu i}$ & $\\overline{\\textbf{3}}$ & $1/2$ &$1/2$\\\\ \\hline ${\\psi_{\\mu}}^{i}$& $Q$-supersymmetry & gravitini; $\\gamma_5 {\\psi_{\\mu}}^{i}=\\psi_{\\mu}^{i}$ & $\\textbf{3}$ & $-1/2$ & $-1/2$\\\\ \\hline $\\Lambda_{L}$& & $\\gamma_{5}\\Lambda_{L}=\\Lambda_{L}$ &$\\textbf{1}$ & $1/2$ & $-3/2$\\\\ \\hline $\\chi_{ij}$ & &$\\gamma_5\\chi_{ij}=\\chi_{ij}$ & $\\overline{\\textbf{6}}$ & $3/2$ & $-1/2$\\\\ \\hline $\\zeta^i$& &$\\gamma_{5}\\zeta^{i}=\\zeta^{i}$&\\textbf{3} &$3/2$& $-1/2$\\\\ \\hline \\end{tabular} \\caption{Field Content of $\\mathcal{N}=3$ Weyl Multiplet} \\label{tab:weyl3} \\end{table} Apart from the gauge fields, the Weyl multiplet contains several covariant matter fields: both bosonic as well as fermionic. Table \\ref{tab:weyl3} summarizes all the fields of the Weyl multiplet along with their algebraic restrictions, $SU(3)$ representations, Weyl weight $w$ under local dilatations and $U(1)$ chiral weight $c$. We follow the notations and conventions of \\cite{Hegde:2022wnb}. The $Q$- and $S$- supersymmetry transformations, parametrized by $\\epsilon^i$ and $\\eta^i$ respectively, of the entire Weyl multiplet can be found in \\cite{Hegde:2018mxv, Hegde:2021rte, Hegde:2022wnb}."}
{"input": "variations. We take the linear combination $(\\psi_i^{I}-2\\varepsilon_{ijk}\\xi^{Ij} \\Theta^k)$ and demand this to vanish under $Q$-supersymmetry. \\begin{align} \\,&(\\delta\\psi_i^{I}-2\\varepsilon_{ijk}\\xi^{Ij} \\delta\\Theta^k)\\,\\nonumber\\\\& = A^{I}{}_{ab}\\gamma^{ab}\\epsilon_{i}+B^{Ik}{}_{a}\\gamma^a \\varepsilon_{ijk}\\epsilon^j+ C^{I j}{}_{iab}\\gamma^{ab}\\epsilon_j+D^{I jn}{}_{ia}\\gamma^a \\varepsilon_{jmn}\\epsilon^n+E^{I jk}{}_{la}\\gamma^a\\varepsilon_{ijk}\\epsilon^l\\overset{!}{=}0 \\end{align} where \\begin{align} A^{I}{}_{ab}=\\,&-\\frac{1}{2}(\\mathcal{F}_{ab}^{+I}+\\xi^{-2}\\xi^{Ij}\\xi^{K}_{j}\\mathcal{F}_{ab}^{+L}\\eta_{KL})\\,\\nonumber\\\\ B^{Ik}{}_{a}=\\,& -2(D_{a}\\xi^{Ik}+\\xi^{-2}\\xi^{I l}\\xi_{l}^{J}D_a\\xi^{L k}\\eta_{JL})\\,\\nonumber\\\\ C^{I j}{}_{i ab}=\\,& \\frac{1}{2}\\xi^{-2}\\xi^{Ij}\\xi_{i}^{K}\\mathcal{F}_{ab}^{+L}\\eta_{KL}\\nonumber\\\\ D^{I jn}{}_{ia}=\\,& 2\\xi^{-2}\\xi^{I j}\\xi^{K}_{i}D_a\\xi^{L n}\\eta_{KL}\\nonumber\\\\ E^{I jk}{}_{la}=\\,&-2\\xi^{-2}\\xi^{Ij}\\xi^{Pk}D_a\\xi^Q_{l}\\eta_{PQ} \\end{align} We will consider in particular, \\begin{align}\\label{AIab-N3} A^{I}{}_{ab}=\\,&-\\frac{1}{2}(\\mathcal{F}_{ab}^{+I}-M^{KI}\\mathcal{F}_{ab}^{+L}\\eta_{KL})=0, \\end{align} where we have used \\eqref{D-gauge-N3} in the units $\\kappa=1$. We can contract the above with $M^{MN}\\eta_{IM}$ and sum over the repeated indices to obtain, \\begin{align}\\label{M-eta-F-N3} \\frac{2}{3}M^{KN}\\eta_{KL}\\mathcal{F}^{+L}_{ab}=0, \\end{align} where we have used \\eqref{M-eta-M-N3}. Note that we have argued below \\eqref{RV-D-zero-N3} on how \\eqref{M-eta-M-N3} holds for the fully supersymmetric solution at any order in the derivative expansion. Now, using \\eqref{AIab-N3} and \\eqref{M-eta-F-N3}, we obtain, \\begin{align} \\mathcal{F}_{ab}^{+I}=0. \\end{align} We can similarly consider, \\begin{align} B^{Ik}{}_{a}=\\,& -2(D_{a}\\xi^{Ik}-M^{JI}D_a\\xi^{L k}\\eta_{JL})=0. \\end{align} Upon using \\eqref{M-eta-M-N3} this gives, \\begin{align} D_a\\xi^{Ii}=0. \\end{align} From this, it follows that, \\begin{align} M^{IJ}=\\text{constants.} \\end{align} Further analysis then gives, \\begin{align}\\label{kgauge-N3} f_{\\mu}^a=0. \\end{align} The detailed arguments for the above two equations are completely analogous to how we obtained \\eqref{mijcon} and \\eqref{kgauge} in the case of $\\mathcal{N}=4$ supergravity. Combining \\eqref{kgauge-N3} above with \\eqref{eq:FM2}, \\eqref{Weyl-zero-N3} and \\eqref{bmu-zero-N3}, we get, \\begin{align} R_{\\mu\\nu\\rho\\sigma}=0. \\end{align} Thus, we obtain the flat space again as the unique fully supersymmetric solution in higher derivative matter coupled $\\mathcal{N}=3$ Poincar\\'e supergravity. The scalars can take arbitrary constant values, while the metric remains flat and the fluxes are zero. The result is summarized below in Table-\\ref{tab:n3-flat}."}
{"input": "these $\\cN=2$ supermultiplets via a supersymmetric truncation of the $\\cN=3$ multiplets. Truncating the $\\cN=3$ Weyl multiplet, one obtains the off-shell $\\cN=2$ Weyl multiplet and an off-shell $\\cN=2$ vector multiplet. Under similar truncation, the on shell $\\cN=3$ vector multiplet splits into an on-shell $\\cN=2$ vector multiplet and an on-shell $\\cN=2$ hypermultiplet. We do not provide the full truncation results here. For details, see \\cite{Aikot:2024cne}. For our purpose, the following observation is important. In the truncation procedure, a set of fields are set to zero that would otherwise form an $\\cN=2$ gravitino multiplet. This includes the S-invariant spinor $\\Lambda_L$. As a consequence, there is no Killing spinor equation in $\\cN=2$ supergravity, that would require us to set the auxiliary field $T_{ab}^{ij}$ to zero. In the Poincar\\'e picture, the non-vanishing value of the $T_{ab}^{ij}$ field becomes the radius of the $AdS_2 \\times S^2$ geometry and the moduli fields take constant values which are completely determined by the electromagnetic charges \\cite{LopesCardoso:1998tkj}."}
{"input": "\\section{Introduction} The safety of reinforcement learning (RL) during both training and deployment phases has garnered increasing attention \\cite{Lavan24L4DC,pmlr-v242-vaskov24a,pmlr-v242-buerger24a}, particularly due to the safety-critical nature of many robotic systems. A core challenge lies in ensuring provable safety throughout these phases. Traditional RL methods commonly address safety by penalizing unsafe behaviors, which inevitably leads to the exploration of unsafe actions during training and fails to guarantee the safety of the learned policy during deployment. Recent solutions can be divided into two categories: constrained optimization-based methods and safety filter-based methods. For constrained optimization involving multiple safety constraints, Lagrangian-based safe RL methods \\cite{xu2021crpo,pmlr-v242-yao24a} are proposed to improve training efficiency with constraint satisfaction. Safety filter based methods typically rely on certificate functions such as Control Barrier Functions (CBFs), or Hamilton-Jacobi Reachability value functions. However, there remains a lack of efficient and generalizable approaches to ensure safety across all phases of the RL process. The CBF-based approach \\cite{Wang17TRO,Ames19ECC,Agrawal21CDC,Wang23RAL,Xiao22TAC} theoretically ensures safety for control strategies and has been widely applied to various control-affine robotic systems, such as autonomous vehicles \\cite{Wang23RAL}, bipedal robots \\cite{csomay2021episodic} and etc. The core idea involves formulating safety constraints for the control strategy, defining a safe set through these constraints, and deriving forward invariance conditions for the safe set to impose decision-variable constraints that ensure safety. These constraints are then integrated into an optimization problem to generate safe strategies. Typically, the safety optimization is based on system models and nominal controllers derived from control theory."}
{"input": "controllers, leveraging the powerful approximation capabilities and superior task performance of learning techniques \\cite{Cheng_Orosz_Murray_Burdick_2019}. Integrating safety optimization into the pipeline of control policy learning can be framed as a decision-focused learning paradigm \\cite{Shah22NIPS}. In this framework, the prediction phase is handled by a RL policy network, followed by downstream safety optimization to generate the final safe strategy and evaluate its performance. This end-to-end approach requires the safety optimization process to be differentiable, which is often challenging due to issues like solution discontinuity \\cite{ferber2020mipaal} and gradient approximation \\cite{wilder2019melding}. Recent works in decision-focused learning address these challenges through various methods: using surrogates to replace the original optimization problem and learning loss functions \\cite{wilder2019melding} or constructing differentiable optimization tools \\cite{amos2017optnet,NEURIPS2022_18596929,agrawal2019differentiable}. For control-affine systems, safety behavior optimization benefits from linear relaxation of decision variables via Nagumo's theorem \\cite{Ames19ECC}, which avoids the complexity of differentiable nonlinear programming \\cite{NEURIPS2022_18596929} or mixed-integer programming \\cite{ferber2020mipaal}. This allows the use of differentiable Quadratic Programming (QP) solvers \\cite{amos2017optnet,agrawal2019differentiable}. Recent research on differentiable QP-based safe control \\cite{Emam22RAL,Ma22ECC,amos2018differentiable,romero2024actor} primarily focuses on three aspects: (1) addressing the impact of constraint parameters, such as environmental changes on safety strategies, e.g., \\cite{Ma22ECC}, by adjusting the class-$\\mathcal{K}$ functions within safety constraints via differentiable QP; (2) constructing linear MPC problems \\cite{amos2018differentiable} and tuning receding horizon parameters during optimization through differentiable QP to enhance task performance \\cite{romero2024actor}; and (3) imitating safe behaviors \\cite{Xiao23TRO} or integrating safe QP as the final layer of RL policy networks \\cite{Emam22RAL} to generate safe optimization strategies. Differentiable QP frameworks offer several notable advantages."}
{"input": "\\section{Main results} \\subsection{Composite CBF for multiple constraints} To solve the CBF-based optimization under multiple constraints, the constraints are regarded as the intersection of safe sets defined by these CBFs. Each safe constraint $h_i$ is defined by a 0-superlevel set and their intersection is defined as \\begin{equation}\\label{intersection} \\bigcap_{i = 1, \\dots, I} C_i = \\{ x \\in \\mathbb{R}^n : h_i(x) \\geq 0 \\}, \\end{equation} where $I$ denotes the number of the safety constraints. In other words, the intersection of sets captures the logical AND relationship between multiple safety constraints, which is denoted as \\begin{equation} x \\in \\bigcap_{i = 1, \\dots, I} C_i \\iff x \\in C_1 \\text{ AND } x \\in C_2 \\cdots \\text{ AND } x \\in C_I. \\end{equation} When there are multiple constraints, the complexity of the QP problem increases, generally making it impossible to derive a closed-form solution, thus requiring numerical optimization methods such as active set or interior point methods. However, inspired by existing literature \\cite{Molnar23CSL} solving complex safety specifications, this paper employs a Log-Sum-Exp approximation technique to transform multiple constraints into a single constraint, thereby enabling a closed-form solution for the safe QP. The approximated composite single CBF is constructed as: \\begin{equation}\\label{composite} h(x) = -\\frac{1}{\\kappa} \\ln \\left( \\sum_{i=1}^I e^{-\\kappa h_i(x)} \\right), \\end{equation} whose Lie derivatives are expressed by: \\begin{equation} L_f h(x) = \\sum_{i=1}^I \\lambda_i(x) L_f h_i(x), \\quad L_g h(x) = \\sum_{i=1}^I \\lambda_i(x) L_g h_i(x), \\end{equation} where \\begin{equation} \\lambda_i(x) = e^{-\\kappa (h_i(x) - h(x))}, \\end{equation} with $\\sum_{i \\in I} \\lambda_i(x) = 1$ and $\\kappa > 0$."}
{"input": "an equivalent substitution for the constraints of optimization problem \\eqref{QP1} is $\\min_{}h_i(x)\\ge 0, i=1,\\cdots,I$. The composite CBF in \\eqref{composite} shares the following property. \\textbf{Lemma 1:} \\cite{Molnar23CSL} Consider sets $C_i$ in \\eqref{Ci} and their intersection in \\eqref{intersection}. Continuous function $h(x)$ in \\eqref{composite} under approximates $\\min_{i=1,\\cdots,I}h_i(x)\\ge 0$ with bounds: \\begin{equation} \\min_{i=1,\\cdots,I} h_i(x) - \\frac{\\ln I}{\\kappa} \\leq h(x) \\leq \\min_{i=1,\\cdots,I} h_i(x) \\quad \\forall x \\in \\mathbb{R}^n, \\end{equation} such that $\\lim_{\\kappa \\to \\infty} h(x) = \\min_{i=1,\\cdots,I} h_i(x)$. The corresponding set $C=\\left\\lbrace x\\in \\mathbb{R}^n:h(x)\\ge 0\\right\\rbrace$ lies inside the intersection, $C \\subseteq \\bigcap_{i=1,\\cdots,I} C_i$, such that $\\lim_{\\kappa \\to \\infty} C = \\bigcap_{i=1,\\cdots,I} C_i$. See Proof of Theorem 4 in \\cite{Molnar23CSL}. $h(x)\\ge0$ guarantees $ \\min_{i=1,\\cdots,I}h_i(x)\\ge0$, indicating all constraints $h_i\\ge0, i=1,\\cdots,I$ are satisfied. \\subsection{Closed-form solution for CBF-based QP} The safety-oriented framework offers a QP-based optimization approach to modify a nominal policy to ensure safety. The nominal policy $\\bar{u}$, typically designed to achieve a specific task objective, can be derived from model-based control or generated through RL. Based on the established composite CBF $h(x)$ in \\eqref{composite}, the optimization problem ensuring system safety can be formulated as the following QP: \\begin{equation} \\label{QP2} u_s(x) = \\underset{{u} \\in \\mathbb{R}^m}{\\arg \\min} \\frac{1}{2} \\|{u} - \\bar{u}(x)\\|_2^2 \\end{equation} subject to \\begin{equation}\\label{QP2cons} L_f h(x) + L_g h(x) {u} \\geq -\\alpha(h(x)). \\end{equation} When the nominal policy satisfies the safety constraint, the constraint \\eqref{QP2cons} is inactive, and the safe policy aligns with the nominal policy."}
{"input": "the requirement for differentiable optimizations within the RL framework, thereby substantially simplifying the gradient computation in the safe policy generation and alleviating the complexity associated with gradient-based optimization, as will be elaborated in the next subsection. %Furthermore, $\\mathbf{k}_{\\text{QP}}$ is continuous and $\\mathbf{k}_{\\text{QP}}(x) \\in K_{\\text{CBF}}(x)$ for all $x \\in \\mathbb{R}^n$. \\begin{figure}[h] \\centering \\includegraphics[scale=0.47]{framework.eps} \\caption{An illustration of an end-to-end training safe RL framework.} \\label{framework} \\end{figure} \\subsection{Safety layer via closed-form solution in RL framework} In conventional RL architectures, the final layer of the control policy network typically consists of a fully connected layer, particularly for continuous control actions in affine systems. The output is bounded by the final activation function, such as the hyperbolic tangent, to ensure bounded action outputs. For safe policy generation, an intuitive approach is to correct the RL-derived control policy by adjusting it through safety-oriented mechanisms, such as correcting the control policy to a safe policy using a CBF-based QP \\cite{Cheng_Orosz_Murray_Burdick_2019}. However, in this approach, the reward from the safe output cannot backpropagate to the RL network, due to the absence of a gradient pathway connecting the safe policy to the RL policy. Recently, decision-focused learning have proposed architectures based on differentiable optimization, embedding optimizable and differentiable structures to achieve an end-to-end learning pipeline, thus enabling CBF-QP-based safe learning, as illustrated in Figure \\ref{framework}."}
{"input": "computationally expensive and challenging to large-scale multi-constraint problems. To address this issue, we integrate a closed-form solution for the safe policy directly into the RL policy generation pipeline. This approach leverages a composite single-constraint approximation to handle multi-constraint scenarios, alongside explicit QP solutions to circumvent forward optimization and its gradient backpropagation. We replace the final layer in RL policy generation with an analytically computed ``safety layer'', which, due to its analytical properties, can be integrated into any actor-critic RL method. An illustration of safe policy networks in actor-critic framework is shown in Figure \\ref{NNnetwork} with different safety layers. The proposed framework is demonstrated in Figure \\ref{NNnetwork}(a), where the closed-form solution \\eqref{us} and \\eqref{etax} are integrated into the final layer before safety policy generation. As a comparison, Figure \\ref{NNnetwork}(b) demonstrates that taking nominal policy as the input, the differentiable QP layer compute the forward solution with multiple constraints, which is potentially infeasible and computationally expensive. We illustrate the proposed approach using the SAC method, where the loss functions in this framework are given by \\begin{figure}[h] \\centering \\includegraphics[scale=0.34]{NNnetwork.eps} \\caption{An illustration of safe policy networks with different safety layers. Subfigure (a) demonstrates the proposed framework where $N$ constraints are composited to $h(x)$ using a continuous Log-Sum-Exp approximation. The safety layer is analytical based on the closed-form solution of the composite CBF-based optimization. Subfigure (b) demonstrates the existing framework with differentiable QP layer. The safety layer solves the forward CBF-based optimization and computes the gradient during backpropagation."}
{"input": "\\sim \\mathcal{D}_R} \\left[ \\frac{1}{2} \\left( Q_\\theta(x_t, u_s^{\\phi}) - \\left( r(x_t, u_s^{\\phi}) + \\gamma \\mathbb{E}_{x_{t+1} \\sim p} \\left[ V_{\\bar{\\theta}}(x_{t+1}) \\right] \\right) \\right)^2 \\right], \\end{equation} \\begin{equation} V_{\\bar{\\theta}}(x_t) = \\mathbb{E}_{u_s^{\\phi} \\sim \\pi_\\phi} \\left[ Q_{\\bar{\\theta}}(x_t, u_s^{\\phi}) - \\alpha_e \\log \\pi_\\phi (u_s^{\\phi} | x_t) \\right], \\end{equation} \\begin{equation}\\label{Policy_loss_proposed} \\begin{aligned} J_{\\pi}(\\phi) = \\mathbb{E}_{x_t\\thicksim \\mathcal{D}_r} \\left[ \\mathbb{E}_{u_s^{\\phi}\\thicksim \\pi_{\\phi}} [\\alpha_e\\log \\pi_{\\phi}(u_s^{\\phi}|x_t)-Q_{\\theta}(x_t,u_s^{\\phi})] \\right], \\end{aligned} \\end{equation} % where $\\pi_{\\phi}$ denotes the policy generated by the entire policy network, including both the fully connected layers and the QP-based adjustment. %where $u_t^C$ is the compensation term computed by differentiable QP layer. In this case, \\( u_s^{\\phi} \\sim \\pi_{\\phi} \\) would mean that the sample \\( u_s^{\\phi} \\) is drawn from the distribution defined by the entire policy network, which inherently includes the safety layer for the QP adjustment. %This approach keeps the notation compact and still correctly represents the process, provided it is clear that \\(\\pi_{\\phi}\\) now encapsulates both components."}
{"input": "0 $ holds. Therefore, the safe policy is actively filtered and corrected based on \\eqref{us} and \\eqref{etax}, ensuring that $ h $ remains positive for all time. \\begin{figure}[h] \\centering \\includegraphics[width=1.0\\textwidth]{minh_steps.eps} \\caption{Evolution of $h_1,h_2,h_3$ and the composite $h$ over 1000 episodes. The safe learning during training is guaranteed.} \\label{minh_steps} \\end{figure} \\begin{table*}[h] % \\centering \\caption{Comparison of different approaches} \\begin{tabular}{|l|c|c|c|} \\hline \\textbf{Method} & \\textbf{ATTS$(s)$ with $I=3$ } & \\textbf{ATTS$(s)$ with $I=10$} & \\textbf{ATTS$(s)$ with $I=30$} \\\\ \\hline \\multirow{1}{*}{Closed-form solution} & 0.018 & 0.024& 0.043 \\\\ \\hline \\multirow{1}{*}{CBF Batch QP } & 0.13 & 0.25 & 0.40\\\\ \\hline \\multirow{1}{*}{CBF CVXPYlayer } & 0.84 & 1.45 & 2.26 \\\\ % CVX fail to work in I=30 \\hline \\end{tabular} \\label{table_approach_comparison} \\end{table*} As previously noted, the closed-form solution eliminates the need for differentiable QP solvers, thereby reducing computational costs. This represents another significant advantage of the proposed method. In the same scenario for collision avoidance of multiple obstacles, we compared the proposed method with the differentiable QP solver Batched-QPFunction \\cite{amos2017optnet} and the CVXPYLayer \\cite{agrawal2019differentiable} in terms of computational performance, which are commonly used in similar works within CBF-based safe learning \\cite{Emam22RAL,Ma22ECC,jiang2024differentiable,romero2024actor}. The comparative results are summarized in Table~\\ref{table_approach_comparison}, where the performance metric is the average solving time per time step (ATTS) during the RL training process. In addition, scenarios with 10 and 30 constraints are also tested to validate the scalability of the proposed method in solving larger-scale safe RL problems."}
{"input": "\\section{Conclusion} This paper addresses the challenges of ensuring multiple safety constraints and improving training efficiency in safe RL. We propose a safe RL framework based on the closed-form solution of composite CBF. The framework constructs a composite CBF by using the Log-Sum-Exp approximation of the min function to integrate multiple safety constraints in the optimization problem. It also inherits the safety guarantees based on the composite CBF defining the safe set. By serving as a surrogate of the differentiable QP architecture with a closed-form solution, the proposed method significantly enhances training efficiency. Comparative experiments demonstrate that the proposed method is up to 7 times faster than the current state-of-the-art differentiable batch QP solvers, and at least 46 times faster than the differentiable convex optimization layers CVXPYlayer, showcasing its potential for solving optimization in large-scale safe RL problems. Future work will further investigate the composite CBF-QP under explicit input constraints, with a focus on guaranteeing feasibility and improving efficiency within the framework of differentiable optimization. %\\acks{We thank a bunch of people.} \\newpage \\acks{This work was supported by National Natural Science Foundation of China under Grant 62303316, in part by the Science Center Program of National Natural Science Foundation of China under Grant 62188101, in part by the Fellowship of China National Postdoctoral Program for Innovative Talents under Grant BX20240224, and the Oceanic Interdisciplinary Program of Shanghai Jiao Tong University (project number SL2022MS010)."}
{"input": "of generality, we will work with the boundary condition corresponding to $\\alpha = 0$. To describe appropriate spectral data for $\\Di_{q}$, consider the matrix-valued solution $N = \\left(\\begin{smallmatrix} n_{11} & n_{12} \\\\ n_{21} & n_{22} \\end{smallmatrix}\\right)$ of the Dirac system $JN'(x,z) + Q(x)N(x, z) = zN(x, z)$, $N(0, z) = \\idm$, $z \\in \\C$. The Weyl function of $\\Di_{q}$ is defined by $$ m_{q}(z) = \\lim_{x \\to +\\infty} \\frac{n_{22}(x,z)}{n_{21}(x,z)}, \\qquad \\Im z > 0. $$ This function belongs to the Herglotz class in the upper half-plane $\\C_+ = \\{z \\in \\C:\\; \\Im z > 0\\}$, i.e., it is analytic and takes $\\C_+$ into itself. See \\cite{Den06} for the Weyl theory of Dirac operators from the perspective of Krein systems. It is well-known that the Weyl function $m_q$ determines $\\Di_{q}$ uniquely. From the point of view of spectral correspondence it is more convenient to work with the Cayley transform of $m_q$, i.e., with the Schur function $f_q$ of $\\Di_{q}$. Recall that an analytic function $f$ on $\\C_+$ is said to belong to the Schur class $S(\\C_+)$ if $|f(z)| \\le 1$ for all $z \\in \\C_+$. The Schur function $f_{q}$ of $\\Di_{q}$ is determined by \\begin{equation}\\label{sf} m_{q} = i\\frac{1+f_{q}}{1-f_{q}}. \\end{equation} Each function $f \\in S(\\C_+)$ has the nontangential boundary values almost everywhere on $\\R$ \\cite{Garnett}. As usual, we use the same letter $f$ for the function in the unit ball of $L^\\infty(\\R)$ defined by these boundary values."}
{"input": "belong to the set \\begin{equation}\\label{eq: spaceS} S_{2}(\\C_+) = \\big\\{f \\in S(\\C_+):\\; \\log(1-|f|^2) \\in L^1(\\R) \\big\\}. \\end{equation} The set $S_{2}(\\C_+)$ is a complete metric space with respect to the metric \\begin{equation}\\label{eq42} \\rho_{S_{2}}(f, g) = \\sqrt{\\int_{\\R}-\\log\\Bigl(1 - \\Bigl|\\frac{f - g}{1 - \\ov{f}g}\\Bigr|^2\\Bigr)\\,dx}. \\end{equation} It can be shown that $f_n \\to g$ in $S_{2}(\\C_+)$ if and only if $\\|\\log(1-|f_n|^2)\\|_{L^1(\\R_+)} \\to \\|\\log(1-|g|^2)\\|_{L^1(\\R_+)}$ and $f_n \\to g$ in Lebesgue measure on $\\R$, see Lemma \\ref{lem: conv in S2 and conv in measure v2}. \\subsection{Sylvester-Winebrenner theorem} Our starting point is the following fundamental result that stems from the paper \\cite{Sylvester} by Sylvester and Winebrenner. For Dirac operators, it was first proved by Denisov \\cite{Den06} in a somewhat restricted form, see details in Section \\ref{section: tSWD} below. \\begin{Thm}[Sylvester-Winebrenner theorem]\\label{tSW} The correspondence $\\F: q \\mapsto f_{q}$ is a homeo\\-morphism from $L^2(\\R_+)$ onto $S_{2}(\\C_+)$. Moreover, we have \\begin{equation}\\label{eqSW} \\int_{\\R_+}|q(x)|^2\\,dx = \\frac{1}{\\pi}\\int_{\\R}-\\log(1-|f_q(x)|^2)\\,dx. \\end{equation} \\end{Thm} Theorem \\ref{tSW} belongs to a general direction in spectral theory that relies on the usage of trace formulae (or {\\it sum rules}, in the terminology of B.\\,Simon \\cite{SimonDes}). This direction often leads to the most general results when one is interested in complete characterization theorems ({\\it ``spectral gems''} \\cite{SimonDes}). See, e.g., \\cite{KS03, KS09, DKS10, Yu18, BD20, BD21b, DEY21}. The proofs of such theorems, however, do not involve reconstruction procedures for potentials from the spectral data, and in particular, they do not imply any {\\it continuity estimates} related to the spectral correspondence."}
{"input": "\\|q-0\\|_{L^2(\\R_+)}^{2} = \\rho_{S_2}(f_q, 0)^{2}/\\pi. $$ Having this identity, it is natural to expect that quantities $\\|q - \\tilde q\\|_{L^2(\\R_+)}$, $\\rho_{S_2}(f_q, f_{\\tilde q})$ control each other for $q$, $\\tilde q \\in L^2(\\R)$. Moreover, Theorem \\ref{tSW} says that $\\|q_n - \\tilde q\\|_{L^2(\\R_+)} \\to 0$ if and only if $\\rho_{S_2}(f_{q_n}, f_{\\tilde q}) \\to 0$, making this expectation even more plausible. It turns out, however, that it is false. In fact, we have the following theorem. \\begin{Thm}\\label{t5} There are potentials $u_n$, $\\tilde u_n$, $q_n$, $\\tilde q_n$ in the unit ball of $L^2(\\R_+)$ such that \\begin{align} &\\lim_{n \\to \\infty}\\|u_n - \\tilde u_n\\|_{L^2(\\R_+)} = 0, \\;\\; \\mbox{but} \\;\\; \\lim_{n \\to \\infty} \\rho_{S_{2}}(f_{u_n}, f_{\\tilde u_n}) > 0, \\label{eq12}\\\\ &\\lim_{n \\to \\infty}\\|q_n - \\tilde q_n\\|_{L^2(\\R_+)} \\,> 0, \\;\\; \\mbox{but} \\;\\;\\, \\lim_{n \\to \\infty} \\rho_{S_{2}}(f_{q_n}, f_{\\tilde q_n}) = 0. \\label{eq11} \\end{align} In other words, the homeomorphisms $\\F$, $\\F^{-1}$ in Theorem \\ref{tSW} are not uniformly continuous on bounded subsets of $L^2(\\R_+)$, $S_2(\\C_+)$. \\end{Thm} As an ``explanation'' for \\eqref{eq12}, \\eqref{eq11}, let us mention that the continuous operators $\\F$, $\\F^{-1}$ are not linear and the closed unit balls in $L^2(\\R_+)$, $S_{2}(\\C_+)$ are not compact. In particular, the standard general arguments are not applicable here and the lack of uniform continuity of $\\F$, $\\F^{-1}$ is possible. To prove \\eqref{eq12}, we construct some explicit sequences of potentials $u_n$, $\\tilde u_n$. The proof of \\eqref{eq11} is more delicate. Here we use a very important observation of Volberg and Yuditskii \\cite{VYu02} on the non-injectivity of the scattering map for Jacobi matrices."}
{"input": "the setting of the nonlinear Fourier transform by Tao and Thiele \\cite{TT} and to Dirac operators by the first author and Denisov \\cite{BD24}. \\medskip \\subsection{The main result} Let us now turn to positive results. For every $f \\in S_{2}(\\C_+)$, the function $|f|^2$ is comparable to $|\\log(1-|f|^2)|$ on the set $E$ where $|f| \\le 1/2$, and the complement $\\R \\setminus E$ has a finite measure as $\\log(1-|f|^2) \\in L^1(\\R)$. It follows that $f \\in L^2(\\R)$. Thus, the Fourier transform of any element $f \\in S_{2}(\\C_+)$ is well defined and belongs to $L^2(\\R)$. We will denote it by $\\hat f$, so that \\begin{gather} \\label{eq: fourier on R def} \\hat f(\\xi) = \\frac{1}{\\sqrt{2\\pi}}\\int_{\\R}f(x)e^{-i\\xi x}\\,dx \\end{gather} if $f$ is integrable. We will need the following Wiener-type norm and the weighted $L^1$-norm: $$ \\|f\\|_{W^1_A(\\R_+)} = \\int_{\\R_+}|\\hat f(\\xi)|e^{-A\\xi}\\,d\\xi, \\qquad \\|q\\|_{L^1_A(\\R_+)} = \\int_{\\R_+}|q(\\xi)|e^{-A\\xi}\\,d\\xi. $$ Our main result is the following theorem. \\begin{Thm}\\label{t1} Let $q$, $\\tilde q \\in L^2(\\R_+)$, and let $f_{q}, f_{\\tilde q}$ be the Schur functions \\eqref{sf} of the corresponding Dirac operators $\\Di_{q}$, $\\Di_{\\tilde q}$ \\eqref{do}. Then we have \\begin{equation}\\label{t1est} c_{1}\\|q - \\tilde q\\|_{L^1_{2A}(\\R_+)} \\le \\|f_{q} - f_{\\tilde q}\\|_{W^1_{A}(\\R_+)} \\le c_2\\|q - \\tilde q\\|_{L^1_{2A}(\\R_+)} \\end{equation} for $c_1 = \\sqrt{\\pi/2}$, $c_2 = 2\\sqrt{2\\pi}$ and any $A \\ge 12\\max\\big(\\|q\\|_{L^2(\\R_+)}^2, \\|\\tilde q\\|^2_{L^2(\\R_+)}\\big)$. \\end{Thm} We would like to stress that \\eqref{t1est} is a uniform estimate. This makes it much stronger than the continuity property in Theorem \\ref{tSW}, cf. \\eqref{eq12}, \\eqref{eq11}. Note also that \\eqref{t1est} is nontrivial and new even in the case $\\tilde q = 0$."}
{"input": "with further results, let us give a few historical remarks. Perhaps, the most general stability result in the one-dimensional spectral theory is the Krein-de Branges spectral theorem (see Section 5.2 of \\cite{Remlingb}). It gives stability of the solution of direct and inverse spectral problems for canonical Hamiltonian systems and implies spectral stability for various other classical operators (Schr\\\"odinger and Dirac operators, Krein strings, Jacobi matrices). However, the proof of Krein-de Branges theorem, at least in its current form, cannot give explicit stability estimates, because it uses the following general topological argument to prove the fact that the solution map is a homeomorphism: \\begin{equation}\\label{continuity} \\mbox{\\it a continuous bijection between two Hausdorff compacts is a homeomorphism.} \\end{equation} Moreover, the usage of compactness arguments similar to \\eqref{continuity} forces to deal with very weak variants of stability, because for this approach closed bounded subsets in the metric spaces under consideration need to be compact. Even implicit stability control with respect to the norms like $\\|\\cdot \\|_{L^p(\\R_+)}$ or $\\|\\cdot\\|_{L^1_A(\\R_+)}$ via general Krein-de Branges theory is not possible because these norms define topologies that are not locally compact. \\medskip For classical operators, there are well-known constructive methods to solve inverse spectral problems developed by Gelfand-Levitan and Krein (see Marchenko \\cite{March06} for an excellent historical overview) and more recent methods by Belishev-Mikhaylov \\cite{BM12} and Makarov-Poltoratski \\cite{MP23}. In principle, one can use these methods to prove spectral continuity by accurate estimation of all quantities appearing in the proofs. See, e.g."}
{"input": "Schur's algorithm for periodic functions in the the upper half-plane $\\C_+$. Fix $\\ell > 0$ and define \\begin{equation}\\label{eq103} S_{\\ell, *}(\\C_+) = \\bigl\\{f:\\C_+ \\to \\D: \\;\\; f(z) = F(e^{2i\\ell z}),\\; F \\in S_*(\\D)\\bigr\\}. \\end{equation} A function $f \\in S(\\C_+)$ belongs to $S_{\\ell, *}(\\C_+)$ if and only if $f(z+\\pi/\\ell) = f(z)$ for every $z \\in \\C_+$, and there is no finite Blaschke product $B$ in $\\D$ such that $f = B(e^{2i\\ell z})$. Relation \\eqref{sa} for $f(z) = F(e^{2i\\ell z})$, $F \\in S_{*}(\\D)$, takes the form \\begin{equation}\\label{sa-hp} f_0 = f, \\qquad e^{2i\\ell z}f_{k+1} = \\frac{f_k - f_k(\\infty)}{1 - \\ov{f_k(\\infty)}f_k}, \\qquad k \\ge 0, \\qquad z \\in \\C_+, \\end{equation} where $f_k(\\infty) = \\lim_{y \\to +\\infty}f_k(iy) = F_{k}(0)$, see Lemma \\ref{l10}. It is natural to call the numbers $\\{f_{k}(\\infty)\\}_{k \\in \\Z_+}$ the recurrence coefficients of $f \\in S_{\\ell, *}(\\C_+)$. They determine functions $f$, $F$ uniquely. In fact, the knowledge of first $n$ recurrence coefficients of $f$ allows to approximate it with accuracy $2e^{-2n\\ell y}$ in the half-plane $\\Im z \\ge y > 0$, cf. $(1.3.43)$ in \\cite{Simonbook1}. \\medskip \\subsection{Kronig-Penney model} Let us consider the half-line Dirac operators $\\Di_{q}$ \\eqref{do} on $\\R_+$ whose potentials \\begin{equation}\\label{dp} q = \\sum_{k \\in \\Z_+}c_k \\delta_{\\ell k}, \\qquad c_k \\in \\C, \\end{equation} now are linear combinations of point masses (usually, they are called $\\delta$-interactions) supported on the half-lattice $\\ell\\Z_+ = \\{\\ell k:\\; k \\in \\Z, \\; k \\ge 0\\}$ of sparseness $\\ell > 0$."}
{"input": "relativistic Kronig-Penney model for massless particles (for comparison, in the original Kronig-Penney model \\cite{KP31} for a non-relativistic electron in a one-dimensional crystal, Schr\\\"odinger operators with $\\delta$-interactions on the lattice $\\Z$ were used, and $c_k \\equiv c$ in the classical case). The Kronig-Penney model and its various generalizations are called exactly solvable models, meaning that the resolvents of the operators under consideration (Schr\\\"odinger or Dirac) as well as associated quantities (spectra, resonances, eigenfunctions, etc) can be often found explicitly in terms of the potential $q$. Solvable models attracted an enormous attention in theoretical physics and mathematics. We refer the reader to the classical monograph \\cite{AGHH88} (1988) by Albeverio, Gesztesy, H{\\o}egh-Krohn, and Holden, to the review chapter by Exner in the second edition \\cite{AGHH05} (2005) of this monograph, and to later survey by Kostenko and Malamud \\cite{KM13} (2013). The most close mathematical references to our work are \\cite{CMP, LS14, Hug98, GS87}. The main distinction of our setting from the previous considerations comes from the fact that exactly solvable relativistic models are usually studied for radial massive Dirac operators $$ \\Di_{\\mathbf{m}, q}: X \\mapsto c\\begin{pmatrix}0 & -1 \\\\ 1 & 0\\end{pmatrix} X'(x) + \\mathbf{m}c^2 \\begin{pmatrix}1 & 0 \\\\ 0 & -1\\end{pmatrix} X(x) + Q(x) X(x), $$ with some positive parameters $c$, $\\mathbf{m}$ (corresponding to the velocity of light and the mass of the particle, see Section 4.6.6 in \\cite{Thaller}). We consider the case where $\\mathbf{m} = 0$ and choose physical units so that $c=1$."}
{"input": "in the second canonical form, i.e., $Q = Q^*$, $\\trace Q = 0$, see~\\eqref{do}. These assumptions are standard for the spectral theory of Dirac operators \\cite{LSb} and for its applications to the nonlinear Schr\\\"odinger equation \\cite{FTbook} (the massless Dirac operator is the auxiliary operator for the inverse scattering transform method for NLSE) but less common in the area of exactly solvable models. \\medskip Essential part of the literature devoted to exactly solvable models deals with direct problems: knowing potential $q$ (a measure on some discrete subset of $\\R$), one determines some spectral characteristics of the corresponding Schr\\\"odinger or Dirac operator. On the other hand, the full spectral characterization (the Weyl function or the spectral measure and the corresponding Fourier transform) is not known even for potentials $q$ supported on a lattice in the simplest massless case $\\mathbf{m} = 0$. Moreover, it is not immediate if it is possible to describe spectral measures in closed form in terms of $q$. Indeed, for general $q$ of the form \\eqref{dp} the corresponding spectral measures could have a complex structure and arbitrary spectral type (e.g., singular continuous component is not excluded). Below we show that such a description indeed exists, and, moreover, it turns out to be very simple and explicit (modulo nonlinearity and generality of the problem). To state our second main result, we need the following bijection $\\varkappa: \\D \\to \\C$: \\begin{equation}\\label{eq44} \\varkappa(w) = \\frac{\\ov{w}}{2|w|}\\log\\frac{1+|w|}{1-|w|}, \\qquad w \\in \\C."}
{"input": "\\C_+. \\end{equation} It is known that the limit above exists for every $z \\in \\C_+$ and does not depend on the choice of $\\omega$. Moreover, the Weyl function, $m_{\\Hh}$, is analytic in $\\C_+$ and takes $\\C_+$ into $\\C_+$ unless it coincides with a constant $c \\in \\R \\cup \\{\\infty\\}$. Weyl's theory for canonical Hamiltonian systems can be found in \\cite{HSW2000}, \\cite{Remlingb}, \\cite{Romanov}. We also define the Schur's function $f_\\Hh$ by \\begin{gather}\\label{sf cs} f_\\Hh = \\frac{m_\\Hh - i}{m_\\Hh + i},\\qquad\\qquad m_\\Hh = i\\frac{1+f_\\Hh}{1-f_\\Hh}. \\end{gather} Analytic functions taking $\\C_+$ into $\\C_+$ form the Herglotz-Nevanlinna class $N(\\C_+)$. The set $$ \\ov{N}(\\C_+) = N(\\C_+) \\cup \\R \\cup \\{\\infty\\} $$ is the compactification of $N(\\C_+)$ when the latter is equipped with the topology of convergence on compact subsets in $\\C_+$. This topology (we extend it to $\\ov{N}(\\C_+)$) is metrizable with the metric, e.g., $$ \\rho_{c}(m, \\tilde m) = \\max_{|z-i|\\le 1/2}\\frac{2|m(z) - \\tilde m(z)|}{\\sqrt{1+|m(z)|^2}\\sqrt{1+|\\tilde m(z)|^2}}, \\qquad \\rho_{c}(m, \\infty) = \\max_{|z-i| \\le 1/2}\\frac{2}{\\sqrt{1+|m(z)|^2}}. $$ One can choose other metrics on $\\ov{N}(\\C_+)$ determining the same compact topological space, see discussion on page 109 in \\cite{Remlingb}. \\medskip Different singular Hamiltonians $\\Hh, \\tilde\\Hh$ can have equal Weyl functions. For instance, it is not difficult to check that if $\\tilde\\Hh(x) = \\xi'(x)\\Hh(\\xi(x))$ almost everywhere on $\\R_+$ for some locally absolutely continuous increasing bijection $\\xi: \\R_+ \\to \\R_+$, then $m_{\\Hh} = m_{\\tilde\\Hh}$."}
{"input": "on }\\R_+\\}/\\sim. $$ One can check that each class of equivalence in $\\Hbbs$ contains the unique (up to values on a set of measure zero) element $\\Hh^{tr}$ such that $\\trace \\Hh^{tr} = 1$ on $\\R_+$. One can turn $\\Hbbs$ into a compact Hausdorff space by defining the topology via the metric, e.g., \\begin{equation}\\label{eq8} d(\\Hh, \\tilde\\Hh) = \\sum_{n \\ge 0}2^{-n}\\frac{d_n(\\Hh^{tr}, \\tilde\\Hh^{tr})}{1+d_n(\\Hh^{tr}, \\tilde\\Hh^{tr})}, \\qquad d_n(\\Hh^{tr}, \\tilde\\Hh^{tr})= \\sup_{0 \\le t \\le n}\\left\\|\\int_{0}^{t}(\\Hh^{tr}(s) - \\tilde \\Hh^{tr}(s)) \\,ds\\right\\|. \\end{equation} Compactness of $\\Hbbs$ follows from Riesz representation theorem, see the proof of Theorem 5.4 in \\cite{Remlingb}. \\medskip The following theorem is a key result of Krein -- de Branges spectral theory of canonical Hamiltonian systems \\cite{KK68}, \\cite{dBbook}, \\cite{DMbook}, \\cite{Remlingb}, \\cite{Romanov}. \\begin{Thm}[Krein -- de Branges theorem]\\label{KdB} The correspondence $\\Hh \\mapsto m_{\\Hh}$ is the homeomorphism of compact metric spaces $\\Hbbs$, $\\ov{N}(\\C_+)$. \\end{Thm} For a discussion (and some surprisingly deep applications) of the continuity part in Theorem \\ref{KdB}, see Section 5 in \\cite{Remlingb}, Section 2 in \\cite{EKT18}, or Section 3 in \\cite{ELS21}. Currently, explicit estimates related to the continuity properties of the homeomorphism in Theorem \\ref{KdB} are not known. However, in contrast to the Sylvester-Winebrenner theorem (Theorem \\ref{tSW}), the homeomorphism in Krein-de Branges theorem are uniformly continuous in both directions by compactness and the Heine-Cantor theorem. \\medskip Let $\\Hh \\in \\Hbbs$ be a singular Hamiltonian on $\\R_+ = [0, +\\infty)$."}
{"input": "\\begin{equation}\\label{cs} JM'(x, z) = z\\Hh(x) M(x,z), \\qquad M(0, z) = \\idm, \\end{equation} almost everywhere on $\\R_+$, where the differentiation is taken with respect to $x \\in \\R_+$ and $z \\in \\C$ is a spectral parameter. Note that we have $M = \\bigl(\\Theta, \\Phi \\bigr)$ in terms of the solutions $\\Theta$, $\\Phi$ of \\eqref{csv} satisfying $\\Theta(0, z) = \\oz$, $\\Phi(0,z) = \\zo$. Let us write $w_1 \\doteq A w_2$ for two complex numbers $w_1$, $w_2$ and a matrix $A$ with $\\det A \\neq 0$ if $$ w_1 = \\frac{a_{11}w_2 + a_{12}}{a_{21}w_2 + a_{22}}, \\qquad A = \\begin{pmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{22} \\end{pmatrix}. $$ It is straightforward to generalize this definition to the case where $w_1$, $w_2$ can admit the value $\\infty$. For a matrix $A \\in \\sltc$, we have $w_1 \\doteq A w_2$ if and only if $w_2 \\doteq A^{-1} w_1$, $w_{1,2} \\in \\C \\cup \\{\\infty\\}$. Note also that the definition \\eqref{wf} for $m_{\\Hh}$ can be rewritten in the following form: \\begin{equation}\\label{wfbis} m_{\\Hh} \\doteq \\lim_{x \\to \\infty}\\sigma_1 M(x,z)^{T} \\sigma_1 \\omega, \\end{equation} where $\\sigma_1 = \\left(\\begin{smallmatrix} 0 &1\\\\1 & 0\\end{smallmatrix}\\right)$ and $M^{T}$ stands for the transposed matrix $M$. Relation \\eqref{sf cs} between the Weyl and Schur functions reads as \\begin{gather} \\label{eq: LR matrices} m_q \\doteq Lf_q,\\quad f_q \\doteq Rm_q,\\qquad L = \\begin{pmatrix} i & i\\\\ -1 & 1 \\end{pmatrix}, \\quad R = \\begin{pmatrix} 1 & -i\\\\ 1 & i \\end{pmatrix}. \\end{gather} \\begin{Lem}\\label{l16} For $\\Hh \\in \\Hbbs$ and $A\\in\\sltr$ define $\\Hh_{A} = A^*\\Hh A$."}
{"input": "\\begin{Lem}\\label{l110} Let $H$ be a singular Hamiltonian on $\\R_+$. Define \\begin{gather*} \\Hh(x) = \\begin{cases} \\idm, & x \\in [0, \\ell),\\\\ H(x-\\ell), & x \\ge \\ell, \\end{cases} \\qquad E_{lz} = \\begin{pmatrix} \\cos \\ell z & \\sin \\ell z\\\\ -\\sin \\ell z & \\cos \\ell z \\end{pmatrix}. \\end{gather*} Then we have $m_{\\Hh} \\doteq E_{lz}m_H$ and $f_{\\Hh} = e^{2ilz}f_H$. \\end{Lem} \\beginpf Notice that $M(t,x) = E_{xz}$ solves \\eqref{csv} with the Hamiltonian $\\Hh$ for $x\\le \\ell$. Lemma \\ref{l15} then gives \\begin{align} m_{\\Hh} &\\doteq \\sigma_1 \\begin{pmatrix} \\cos \\ell z & \\sin \\ell z\\\\ -\\sin \\ell z & \\cos \\ell z\\\\ \\end{pmatrix}^{T} \\sigma_1 m_H \\doteq \\begin{pmatrix} \\cos \\ell z & \\sin \\ell z\\\\ -\\sin \\ell z & \\cos \\ell z\\\\ \\end{pmatrix} m_H \\end{align} as claimed. Applying relation \\eqref{eq: LR matrices} we get \\begin{align*} f_{\\Hh} \\doteq R m_\\Hh &\\doteq RE_{\\ell z}m_H = RE_{\\ell z}Lf_H \\\\ &\\doteq \\begin{pmatrix} 1 & -i\\\\ 1 & i \\end{pmatrix} \\begin{pmatrix} \\cos \\ell z & \\sin \\ell z\\\\ -\\sin \\ell z & \\cos \\ell z \\end{pmatrix} \\begin{pmatrix} i & i\\\\ -1 & 1 \\end{pmatrix} f_H \\doteq \\begin{pmatrix} e^{2ilz} & 0\\\\ 0 & e^{2ilz} \\end{pmatrix}f_H. \\end{align*} The proof of the lemma is concluded. \\qed \\medskip \\subsection{Dirac operators with measures}\\label{section: measure-valued potentials} Let us denote by $\\cM$ the set of all signed complex Borel measures on $\\R$ with $\\supp\\mu \\subset \\R_+$ such that the total variation of $\\mu$ is finite on all intervals $[0, L]$, $L \\ge 0$."}
{"input": "are due to Persson \\cite{Persson} (see Theorem 3.1 in \\cite{Persson}). The reader might note appearance of $J^*$ in the expression $\\mathbf{g}(Q(\\{x_1\\})J^*)$ in \\eqref{eq27}. This multiplicative factor $J^*$ does not appear in the work \\cite{Persson} of Persson. The explanation is simple: \\eqref{eq26} is in fact the differential equation for $JN_q$ (namely, $JN_q' + QJ^* \\cdot JN_q = 0$), and the coefficient in front of $JN_q$ in \\eqref{eq26} is $QJ^*$, not $Q$. Then, we need to use same coefficient in \\eqref{eq27}. Finally, $\\mathbf{g}(Q(\\{x_1\\})J^*)Q(x_1)N_q(x_1)$ is just the short way to write $\\mathbf{g}(Q(\\{x_1\\})J^*)Q(x_1)J^* \\cdot JN_q(x_1)$. \\medskip For regular potentials, the matrix-valued function $N_{q}$ solving \\eqref{eq27} coincides with the classical solution of \\eqref{eq26}. More precisely, if $q = s\\,dx$ for some $s\\in L^1_{\\loc}(\\R_+)$, and we define $S$ by $S\\,dx = J Q$, then $g(Q(\\{x_1\\})J^*) = \\idm$ for every $x_1\\in\\R_+$, and \\eqref{eq26}, \\eqref{eq27} are both equivalent to the integral equation $$ N_q(x) = \\IDM + \\int_{0}^{x}S(x_1)N_q(x_1)\\,dx_1, \\;\\; x \\ge 0, \\qquad N_q(x) = \\IDM \\mbox{on } (-\\infty, 0), $$ that can be solved by iterations: \\begin{align} \\label{eq: series representation} N_q(x) = \\IDM + \\int_{0}^{x} S(x_1)\\,dx_1 &+ \\int_{0}^{x}S(x_1)\\int_{u}^{x_1} S(x_2)\\,dx_2\\,dx_1 \\\\ \\nonumber &+ \\int_{0}^{x}S(x_1)\\int_{0}^{x_1} S(x_2)\\int_{0}^{x_2}S(x_3)\\,dx_3\\,dx_2\\,dx_1 + \\ldots. \\end{align} Here, the series converges in the matrix norm and the $n$-th term can be bounded by \\begin{gather} \\label{eq: series bound} \\left\\|\\int_0^x\\ldots\\int_0^{x_{n - 1}}S(x_1)S(x_2) \\cdots S(x_n)\\,dx_n \\ldots dx_1\\right\\|\\le \\frac{1}{n!}\\left(\\int_0^x\\|S(x_1)\\|\\,dx_1\\right)^n\\le \\frac{(4\\|s\\|_{L^1[0,x]})^n}{n!}. \\end{gather} For a general $q \\in \\cM$, the solution $N_{q}$ can be approximated by solutions of regular equations."}
{"input": "$\\|\\phi\\|_{L^1(\\R)} = 1$, and $\\phi_\\eps = \\eps^{-1}\\phi(x/\\eps)$, $\\eps> 0$, is the corresponding approximate unity, then the solutions of regularized equations \\begin{equation}\\label{eq30} JN_{q^{(\\eps)}}'(x) + Q^{(\\eps)}(x)N_{q^{(\\eps)}}(x) = 0, \\quad N_{q^{(\\eps)}}(-1) = \\IDM, \\quad q^{(\\eps)}(x) = \\int_{\\R}\\phi_\\eps(y-x)q(y), \\end{equation} converge pointwise on $\\R$ to $N_{q}$, see \\cite{Persson} (we use initial condition at $x= -1$ instead of $x=0$ because $\\supp q^{(\\eps)} \\subset [-1, \\infty)$ for small $\\eps > 0$, one can also use any other point in $(-\\infty, 0)$ for the initial condition). In particular, for every $q \\in \\cM$ we have $\\det N_{q} = 1$ on $\\R$ and the multiplicative property \\begin{equation}\\label{eq29} N_q(w_2) = N_q(w_2, w_1)N_q(w_1), \\qquad -\\infty < w_1 < w_2 < \\infty, \\end{equation} holds, where $N_{q}(w_2, w_1)$ is the value at $w_2 - w_1$ of the solution of \\eqref{eq26} for the potential $x \\mapsto \\chi_{\\R_+}(x)q(x + w_1)$, $\\chi_{\\R_+}$ being the indicator function of $\\R_+$. Indeed, it is enough to use the multiplicative property for regularized solutions $N_{q^{(\\eps)}}$ and take the limit. \\medskip For regular potentials $q$ (i.e., for $q = s\\,dx$ with some $s\\in L^1_{\\loc}(\\R_+)$) the corresponding Dirac operators are related to canonical systems as follows. One need to take the solution of \\eqref{eq26} and define the Hamiltonian \\begin{equation}\\label{eq28} \\Hh_{q}(x) = N_q^*(x)N_q(x), \\qquad x \\ge 0. \\end{equation} Then, the Dirac operator $\\Di_{q}$ and the operator of canonical system $\\Di_{\\Hh_{q}}$ are unitary equivalent, see, e.g., \\cite{Romanov} or Section 2.4 in \\cite{B2020}. As we will see in a moment, the same relation holds for $q \\in \\cM$. %see page 92 in \\cite{Zeng23}."}
{"input": "first, we need some notation. By ${\\rm BV}_{\\loc}^{r}(\\R_+)$ we will denote the set of all continuous from the right functions $X: \\R \\to \\C^2$ such that their coordinate functions have a finite variation on any interval $[0, L]$, $L> 0$, and the restriction of $X$ to the set $(-\\infty, 0)$ is a constant vector in $\\C^2$. This constant vector will be denoted by $X(0-)$. This agrees with the standard notation $$ X(0-) = \\lim_{x \\to 0,\\; x < 0} X(x), $$ because much stronger property $X(x) = X(0-)$ is assumed for all $x < 0$ if $X \\in {\\rm BV}_{\\loc}^{r}(\\R_+)$. Recall that for $\\alpha \\in [0, 2\\pi)$ we denoted by $e_{\\alpha}$ the vector $$ e_{\\alpha} = \\begin{pmatrix} \\sin \\alpha \\\\ \\cos \\alpha \\end{pmatrix}. $$ For $q \\in \\cM$ and $X \\in {\\rm BV}_{\\loc}^{r}(\\R_+)$, we will say that $\\Di_q X \\in L^2(\\R_+, \\C^2)$ if there exists a function $Y \\in L^2(\\R_+, \\C^2)$ solving equation $$ JX'(x) + QX(x) = Y(x), \\qquad x \\ge 0, $$ in the sense of Persson, i.e., such that for all $x \\in \\R$ we have \\begin{equation}\\label{eq111} JX(x) + \\int_{(-\\infty, x]}g(Q(\\{x_1\\})J^*)Q(x_1)X(x_1) = JX(0-) + \\int_{(-\\infty, x]}Y(x_1)\\,dx_1, \\end{equation} where we extended $Y$ by zero to $(-\\infty, 0)$. Here, $Q$ is defined by \\eqref{eq45}, in particular, $QX$ is a vector-valued measure. Let us define the domain of the Dirac $\\Di_{q}$ on $\\R_+$ corresponding to the boundary condition $\\alpha \\in [0, 2\\pi)$ by $$ \\dom \\Di_q = \\Bigl\\{X \\!\\in\\! {\\rm BV}_{\\loc}^{r}(\\R_+) \\cap L^2(\\R_+, \\C^2)\\!"}
{"input": "X \\!\\in\\! L^2(\\R_+, \\C^2) \\mbox{ in\\,the\\,sense\\,of\\,\\eqref{eq111}} \\Bigr\\}. $$ For $X \\in \\dom \\Di_q$, we define $$ \\Di_q: X \\mapsto Y $$ for the unique (up to values on sets of Lebesgue measure zero) function $Y \\in L^2(\\R_+, \\C^2)$ in \\eqref{eq111}. With this definition, $\\Di_{q}$ sends functions on $\\R$ into functions in $L^2(\\R_+, \\C^2)$. Since we are interested in $D_{q}$ as a densely defined self-adjoint operator on $L^2(\\R_+, \\C^2)$, an additional step is needed to place $\\dom \\Di_q$ into $L^2(\\R_+, \\C^2)$. For this, we note that the values of $X \\in \\dom \\Di_q$ on $\\R$ as well as the the function $Y = \\Di_q X$ in \\eqref{eq111} depend solely on the restriction of $X$ to $\\R_+$. Indeed, we only need to check that for $X \\in \\dom D_q$ the value $X(0-)$ is determined by the restriction of $X$ to $\\R_+$. For this, we substitute $x =0$ into \\eqref{eq111} and get \\begin{align*} JX(0-) - JX(0) &= g(Q(\\{0\\})J^*)Q(\\{0\\})X(0) \\\\ &= \\left[\\IDM + \\sum_{n=2}^{\\infty}\\frac{(Q(\\{0\\})J^*)^{n-1}}{n!}\\right]Q(\\{0\\})X(0)\\\\ &= J\\left[\\sum_{n=1}^{\\infty}\\frac{(J^*Q(\\{0\\}))^{n}}{n!}\\right]X(0) = Je^{J^* Q(\\{0\\})}X(0) - JX(0). \\end{align*} Thus, the value \\begin{equation}\\label{eq1130} X(0-) = e^{J^* Q(\\{0\\})}X(0) \\end{equation} is determined by the restriction of $X$ onto $\\R_+$ and we can consider $\\dom \\Di_{q}$ as the subset of $L^2(\\R_+, \\C^2)$. A similar argument applies to any point $x \\in \\R$. In fact, we have \\begin{equation}\\label{eq113} X(x-) = e^{J^* Q(\\{x\\})}X(x), \\qquad x \\in \\R, \\end{equation} while $Q(\\{x\\}) \\neq 0$ for at most countable set of points $x \\in \\R$."}
{"input": "\\R. \\end{equation} Let us now recall the definition of the operator of canonical system. We will deal with Hamiltonians $\\Hh$ of {\\it rank two} almost everywhere on $\\R_+$, for the general case see \\cite{Romanov} or \\cite{Remlingb} (the latter book considers linear relations instead of operators to cover the most general situation). At first, we denote by ${\\rm AC}_{\\loc}(\\R_+)$ the set of all functions $X: \\R_+ \\to \\C^2$ such that their coordinate functions are absolutely continuous on any compact subset of $\\R_+$. The Hilbert space $L^2(\\Hh)$ is defined by $$ L^2(\\Hh) = \\Bigl\\{X: \\R_+ \\to \\C^2: \\;\\; \\|X\\|^{2}_{L^2(\\Hh)} = \\int_{\\R_+}\\left\\langle\\Hh(x)X(x), X(x)\\right\\rangle_{\\C^2} < \\infty \\Bigr\\}. $$ For $\\tilde X \\in {\\rm AC}_{\\loc}(\\R_+)$, we will say that $\\Di_{\\Hh} \\tilde X \\in L^2(\\Hh)$ if there exists a function $\\tilde Y \\in L^2(\\Hh)$ solving the equation $$ J\\tilde X'(x) = \\Hh(x)\\tilde Y(x), \\qquad x\\ge 0, $$ or, in other words, such that \\begin{equation}\\label{eq112} J\\tilde X(x) = J\\tilde X(0) + \\int_{0}^{x}\\Hh \\tilde Y(x_1)\\,dx_1, \\qquad x \\in \\R_+. \\end{equation} The domain of the canonical system operator $\\Di_{\\Hh}$ on $\\R_+$ corresponding to the boundary condition $\\alpha \\in [0, 2\\pi)$ is defined by $$ \\dom \\Di_{\\Hh} = \\Bigl\\{\\tilde X \\in {\\rm AC}_{\\loc}(\\R_+) \\cap L^2(\\Hh): \\bigl\\langle \\tilde X(0), e_{\\alpha}\\bigr\\rangle_{\\C^2} = 0,\\; \\Di_{\\Hh} \\tilde X \\in L^2(\\Hh) \\mbox{ in the sense of \\eqref{eq112}} \\Bigr\\}. $$ For $\\tilde X \\in \\dom \\Di_{\\Hh}$, we define $$ \\Di_{\\Hh}: \\tilde X \\mapsto \\tilde Y $$ for the unique (up to values on sets of Lebesgue measure zero) function $\\tilde Y \\in L^2(\\Hh)$ in \\eqref{eq111}."}
{"input": "\\begin{Prop}\\label{P5} Let $q \\in \\cM$, define $\\Hh_q$ by \\eqref{eq28}. Then the Dirac operator $\\Di_{q}$ in $L^2(\\R_+, \\C^2)$ defined on the domain $\\dom \\Di_{q}$ for the boundary condition $\\alpha \\in [0, 2\\pi)$ is unitarily equivalent to the operator $\\Di_{\\Hh_q}$ in $L^2(\\Hh_q)$ defined on the domain $\\dom \\Di_{\\Hh_q}$ for the same boundary condition $\\alpha$. In particular, $\\Di_{q}$ is a densely defined self-adjoint operator on $L^2(\\R_+, \\C^2)$. The unitary equivalence is given by the multiplication operator $U: X \\mapsto N_{q}^{-1}X$ from $L^2(\\R_+, \\C^2)$ to $L^2(\\Hh_q)$. \\end{Prop} \\beginpf Recall that $\\Hh_q = N_q^* N_q$. In particular, we have $\\det\\Hh_q = \\det (N_q)^2 = 1$ everywhere on~$\\R_+$. The same relation $\\Hh_q = N_q^* N_q$ tells us that $X \\in L^2(\\R_+, \\C^2)$ if and only if $UX \\in L^2(\\Hh)$, and, moreover, $\\|X\\|_{L^2(\\R_+, \\C^2)} = \\|UX\\|_{L^2(\\Hh)}$. Thus, the multiplication operator $U: X \\mapsto N_{q}^{-1}X$ is unitary from $L^2(\\R_+, \\C^2)$ to $L^2(\\Hh_q)$. For functions $X \\in {\\rm BV}^{r}_{\\loc}(\\R_+)$, $\\tilde X \\in {\\rm AC}_{\\loc}(\\R_+)$, related by $\\tilde X = N_{q}^{-1}X$ we have $\\bigl\\langle \\tilde X(0), e_{\\alpha}\\bigr\\rangle_{\\C^2} = 0$ if and only if $\\bigl\\langle N_{q}^{-1}(0) X(0), e_{\\alpha}\\bigr\\rangle_{\\C^2} = 0$, i.e., $\\bigl\\langle N_{q}^{-1}(0) e^{-J^*Q(\\{0\\})} X(0-), e_{\\alpha}\\bigr\\rangle_{\\C^2} = 0$, where $X(0-)$ is defined by \\eqref{eq1130}. However, \\eqref{eq1131} gives $N_{q}(0) = e^{-J^*Q(\\{0\\})}N_{q}(0-) = e^{-J^*Q(\\{0\\})}$. Thus, $N_{q}^{-1}(0) e^{-J^*Q(\\{0\\})}= I$ and we have $\\bigl\\langle \\tilde X(0), e_{\\alpha}\\bigr\\rangle_{\\C^2} = 0$ if and only if $\\bigl\\langle X(0-), e_{\\alpha}\\bigr\\rangle_{\\C^2} = 0$. Now, the result is a consequence of the following two lemmas."}
{"input": "formula to $\\Theta_{1} = M$ and to the continuous matrix-valued function $\\Theta_{2} = (\\tilde X, 0)$ with columns $\\tilde X$, $\\left(\\begin{smallmatrix} 0\\\\0 \\end{smallmatrix}\\right)$, and considering the first column in the resulting expressions, we get $$ \\int_{(-\\infty, x]}\\mathbf{G}(x_1)Q(x_1)N_q(x_1)\\tilde X(x_1) = M(x)\\tilde X(x) - \\int_{(-\\infty, x]}M(x_1)\\,d\\tilde X(x_1). $$ Equality \\eqref{eq27} gives $M(x) = J - JN_q(x)$, $x \\in \\R$, hence \\begin{align*} \\int_{(-\\infty, x]}\\mathbf{G}(x_1)Q(x_1)N_q(x_1)\\tilde X(x_1) &= (J - JN_q(x))\\tilde X(x) - \\int_{(-\\infty, x]}(J - JN_q(x_1))\\,d\\tilde X(x_1) \\\\ &= \\int_{(-\\infty, x]}JN_q(x_1)\\,d\\tilde X(x_1) - JN_q(x)\\tilde X(x) + J\\tilde X(-\\infty). \\end{align*} Combining this with \\eqref{eq1111}, we get (recall that $X$ is constant and $N_q = I$ on $(-\\infty, 0)$, so $\\tilde X(-\\infty) = \\tilde X(0-) = \\tilde X(0)$ by continuity) \\begin{gather*} \\int_{(-\\infty, x]}JN_q(x_1)\\,d\\tilde X(x_1) = \\int_{(-\\infty, x]}Y(x_1)\\,dx_1. \\end{gather*} We see that the measures $JN_q(x_1)\\,d\\tilde X(x_1)$ and $Y(x_1)\\,dx_1$ coincide, hence $\\tilde X$ is absolutely continuous and \\begin{align*} J\\tilde X(x) - J\\tilde X(0) &= \\int_0^x J\\,d\\tilde X(x_1) = \\int_0^x JN_q(x_1)^{-1}J^{-1}JN_q(x_1)\\,d\\tilde X(x_1) \\\\ &= \\int_0^x JN_q(x_1)^{-1}J^{-1} Y(x_1)\\,dx_1 = \\int_0^x JN_q(x_1)^{-1}J^{-1} N_q(x_1) \\tilde Y(x_1)\\,dx_1\\\\ &= \\int_0^x \\Hh(x_1) \\tilde Y(x_1)\\,dx_1, \\end{align*} because $JA^{-1}J^{-1} = A^*$ for every real $2\\times 2$ matrix $A$ with unit determinant. The lemma follows. \\qed \\medskip Knowing that operators $\\Di_q$ and $\\Di_{\\Hh_q}$ are unitary equivalent, it is of no surprise that they have the same spectral measures and Weyl functions."}
{"input": "operators with measures is the spectral theorem for these operators. For potentials $q \\in L^1_{\\loc}(\\R_+)$, it can be found in Sections 7, 14 of \\cite{Den06}. \\begin{Prop}\\label{P8} Let $q \\in \\cM$. Define the main spectral measure $\\mu_q$ of the Dirac operator $\\Di_{q}$ on $\\R_+$ to be the representing measure of the harmonic function $\\Im m_q = \\Re(\\tfrac{1+f_q}{1-f_q})$, i.e., by \\begin{equation}\\label{eq81} \\frac{1-|f_{q}(z)|^2}{|1 - f_{q}(z)|^2} = \\frac{1}{\\pi}\\int_{\\R} \\frac{\\Im z}{|x - z|^2}\\, d\\mu_{q}(x), \\qquad z \\in \\C_+, \\end{equation} where $f_q$ is the Schur function of $\\Di_q$. Then the operator $\\Di_q$ on $L^2(\\R_+, \\C^2)$ is unitarily equivalent to the multiplication operator by the independent variable in $L^2(\\mu_q)$. The operator $V: L^2(\\R_+, \\C^2) \\to L^2(\\mu_q)$ of unitary equivalence is densely defined by $$ V: X \\mapsto \\frac{1}{\\sqrt{\\pi}}\\int_{\\R_+}\\bigl\\langle X(x), N_1(x, \\ov{z}) \\bigr\\rangle_{\\C^2}\\,dx, \\qquad N_1(x, z) = N(x, z)\\oz, $$ on smooth functions $X: \\R_+\\to \\C$ with compact support. \\end{Prop} \\beginpf In view of \\eqref{eq114}, the result is a direct consequence of Proposition \\ref{P5} and the spectral theorem for operators of canonical systems (see Section 8 in \\cite{Romanov}, or Chapter 3 in \\cite{Remlingb}). \\qed \\medskip In what follows we will deal with a subclass of $\\cM$ -- potentials $q$ supported on lattices. To simplify notation, we define $$ \\cM_{\\ell} = \\{q \\in \\cM: \\; \\supp q \\subset \\ell\\Z_+\\}. $$ Take $q\\in \\cM_{\\ell}$ and let $Q$ be the matrix potential associated with $q$ via \\eqref{eq45}. Define also the constant $2\\times 2$ matrices $Q[\\ell k] = Q(\\{\\ell k\\})$."}
{"input": "\\setminus\\{0\\}$. The point $0$ is a removable singularity of $G$ because $G$ is bounded. From here we see that there exists the limit $$ G(0) = \\lim_{\\eps\\to 0}G(\\eps) = \\lim_{\\eps\\to 0}g(\\omega_\\ell(\\eps)) = \\lim_{y\\to+\\infty}g(iy) = g(\\infty). $$ To prove $(b)$, notice that $g(\\infty) = 0$ implies $G(0) =0$. Then by Schwarz lemma we have $G(\\lambda) = \\lambda F(\\lambda)$, $\\lambda \\in \\D$, for some $F\\in S(\\D)$. In particular, $g(z) = \\omega_\\ell^{-1}(z)F(\\omega_\\ell^{-1}(z)) = e^{2 i \\ell z} f(z)$ for $f\\in S(\\C_+)$. \\qed \\medskip \\begin{Lem}\\label{l38} Let $A = \\left(\\begin{smallmatrix} a & b\\\\b &-a \\end{smallmatrix}\\right)$ be a zero-trace symmetric matrix. Then we have \\begin{gather*} \\exp(A) = \\cosh(\\lambda)\\IDM + \\frac{\\sinh(\\lambda)}{\\lambda}A, \\quad \\lambda = \\sqrt{a^2 + b^2}. \\end{gather*} \\end{Lem} \\beginpf We have $A^2 = \\left(\\begin{smallmatrix} a & b\\\\b &-a \\end{smallmatrix}\\right)\\left(\\begin{smallmatrix} a & b\\\\b &-a \\end{smallmatrix}\\right) = \\lambda^2 \\idm$. Therefore \\begin{gather*} \\exp(A) = \\sum_{k= 0}^{\\infty}\\frac{A^k}{k!} = \\sum_{k= 0}^{\\infty}\\frac{A^{2k}}{(2k)!} + \\sum_{k= 0}^{\\infty}\\frac{A^{2k + 1}}{(2k + 1)!} = \\sum_{k= 0}^{\\infty}\\frac{\\lambda^{2k}}{(2k)!}\\IDM + \\sum_{k= 0}^{\\infty}\\frac{\\lambda^{2k}}{(2k + 1)!}A \\end{gather*} The series in the right hand-side are Taylor series of $\\cosh(\\lambda)$ and $\\sinh(\\lambda)/\\lambda$. The lemma follows. \\qed \\subsection{Proof of Theorem \\ref{t2}}\\label{s32} Given a Hamiltonian $\\Hh$ on $\\R_+$, and $0\\le u<v\\le\\infty$, let us denote by $M(v, u, z)$ the value at $v-u$ of the fundamental matrix solution for the Hamiltonian $\\Hh(u+\\cdot)$ on $\\R_+$. \\begin{Lem}\\label{pl1} If $q \\in \\cM_{\\ell}$ and $f_q$ is the Schur function of $\\Di_q$, then $f_q(z + \\pi/\\ell)=f_q(z)$ for all $z\\in \\C_+$."}
{"input": "of the corresponding canonical system \\eqref{cs}. The Hamiltonian $\\Hh_q$ is a constant rank two Hamiltonian on $[\\ell k, \\ell (k + 1))$ for every $k\\in \\Z_+$. By Lemma \\ref{l16}, the function $M_k(z) = M_q(\\ell (k + 1), \\ell k, z)$ has the form $M_k(z) = A_k^{-1}E_{\\ell z} A_k$ for $A_k = \\sqrt{\\Hh_q(\\ell k)}$ and the function $E_{\\ell z}$ defined in Lemma \\ref{l110}. In particular, $M_k$ is $\\pi/\\ell$--antiperiodic for every $k\\in \\Z_+$, i.e., it satisfies $M_k(z + \\pi/\\ell) = -M_q(z)$ for all $z\\in \\C_+$. For each $n$ the chain rule for solutions of ODE gives \\begin{equation}\\notag M_q(\\ell n, z) = M_{n-1}(z) M_{n-2}(z) \\cdots M_0(z). \\end{equation} Therefore, $M_q(2\\ell n, z)$ is $\\pi/\\ell$-periodic for every $n\\in \\Z_+$. Now periodicity of $m_{\\Hh_q}$ follows from the definition \\eqref{wf} of Weyl function: $$ m_{\\Hh_q}(z) \\doteq \\lim_{x \\to \\infty}\\sigma_1 M_q(x,z)^{T} \\sigma_1 \\omega \\doteq \\lim\\limits_{n \\to \\infty,\\; n \\in \\Z}\\sigma_1 M_q(2\\ell n,z)^{T} \\sigma_1 \\omega, \\qquad z \\in \\C_+. $$ To conclude, recall that $f_q$ is related to $m_q = m_{\\Hh_q}$ via \\eqref{sf}. \\qed \\begin{Lem}\\label{lem: induction base} Let $q \\in \\cM_{\\ell}$ for some $\\ell > 0$, and let $f_q$ be the Schur function of $\\Di_q$. Then $f_q(\\infty)$ exists and satisfies $$ f_q(\\infty) = \\varkappa^{-1}(q(\\{0\\})), $$ where $\\varkappa$ is defined in \\eqref{eq44}. Moreover, the Schur function of $\\Di_{q_\\ell}$, $q_\\ell = q(\\ell + \\cdot)$, satisfies \\begin{gather}\\label{eq251} e^{2i \\ell z}f_{q_{\\ell}}(z) = \\frac{f_{q}(z)-f_q(\\infty)}{1-\\ov{f_q(\\infty)}f_{q}(z)}, \\qquad z \\in \\C_+."}
{"input": "e^{JQ[0]}, \\hspace{12,5mm} n = \\lfloor x/\\ell\\rfloor, \\qquad x\\ge 0,\\\\ N_{q_\\ell}(x) &= e^{JQ[\\ell (n + 1)]}e^{JQ[\\ell(n-1)]} \\cdots e^{JQ[1]}, \\qquad n = \\lfloor x/\\ell\\rfloor, \\qquad x\\ge 0. \\end{align*} In particular, we have \\begin{gather*} \\Hh_q(x) = \\begin{cases} N_q(0)^*N_q(0), & x \\in [0, \\ell),\\\\ N_q(0)^*\\Hh_{q_\\ell}(x-\\ell)N_q(0), & x \\ge \\ell. \\end{cases} \\end{gather*} Lemma \\ref{l16}, Lemma \\ref{l110} and relation \\eqref{eq: LR matrices} give \\begin{gather*} m_q\\doteq \\sigma_1 N_q(0)^{*}\\sigma_1 E_{lz}m_{q_\\ell},\\qquad f_q\\doteq R\\sigma_1 N_q(0)^{*}\\sigma_1 E_{lz}Lf_{q_\\ell}. \\end{gather*} Denote $\\zeta = q(\\{0\\})$. We have $JQ[0] = \\left(\\begin{smallmatrix}-\\Re \\zeta & \\Im \\zeta \\\\ \\Im \\zeta & \\Re \\zeta\\end{smallmatrix}\\right)$. Lemma \\ref{l38} gives \\begin{gather*} N_q(0)^* = N_q(0) = e^{JQ[0]} = \\cosh(|\\zeta|) \\cdot \\IDM + \\frac{\\sinh(|\\zeta|)}{|\\zeta|} \\cdot J Q[0]. \\end{gather*} Furthermore, the straightforward calculation shows \\begin{align*} RE_{\\ell z}L &= \\begin{pmatrix} 1 & -i\\\\ 1 & i \\end{pmatrix} \\begin{pmatrix} \\cos \\ell z & \\sin \\ell z\\\\ -\\sin \\ell z & \\cos \\ell z \\end{pmatrix} \\begin{pmatrix} i & i\\\\ -1 & 1 \\end{pmatrix} = 2i \\begin{pmatrix} e^{i\\ell z} & 0\\\\ 0 & e^{-i\\ell z} \\end{pmatrix}, \\\\ R \\sigma_1 J Q[0]\\sigma_1 E_{\\ell z}L &= \\begin{pmatrix} 1 & -i\\\\ 1 & i \\end{pmatrix} \\begin{pmatrix} \\Re \\zeta & \\Im \\zeta\\\\ \\Im \\zeta & -\\Re \\zeta \\end{pmatrix} \\begin{pmatrix} \\cos \\ell z & \\sin \\ell z\\\\ -\\sin \\ell z & \\cos \\ell z \\end{pmatrix} \\begin{pmatrix} i & i\\\\ -1 & 1 \\end{pmatrix},\\\\ &=2i \\begin{pmatrix} 0 & \\ov{\\zeta}e^{-i\\ell z}\\\\ \\zeta e^{i\\ell z} & 0 \\end{pmatrix}."}
{"input": "1$. For $F$, $\\nu$ related by \\eqref{eq76} it is known that \\begin{equation}\\label{eq77} w_{\\nu}(\\xi) = \\frac{1-|F(\\xi)|^2}{|1 - F(\\xi)|^2} \\mbox{ for Lebesgue almost all } \\xi \\in \\T, \\end{equation} and \\begin{equation}\\label{eq78} \\nu = *\\lim_{r \\to 1}\\frac{1-|F(r\\xi)|^2}{|1 - F(r\\xi)|^2}\\,d m_{\\T}(\\xi), \\end{equation} where $*\\lim$ stands for the *-weak convergence of measures. For the proof of relations \\eqref{eq77}, \\eqref{eq78}, see Theorem I.5.3, Theorem I.3.1 in \\cite{Garnett}. Similar relations hold for the measure $\\sigma$ (for $\\sigma$, one need to replace $F$ by $\\xi F$ in \\eqref{eq77}, \\eqref{eq78}). In the next three results we assume that a Schur function $F \\in S_*(\\D)$ and measures $\\sigma$, $\\nu$ on $\\T$ are related by \\eqref{eq75}, \\eqref{eq76} and $\\{F_k(0)\\}_{k \\ge 0} = \\{\\alpha_{k}\\}_{k \\ge 0}$ are the recurrence coefficients of $F$, $\\sigma$. \\begin{Thm}[Rakhmanov's theorem] \\label{tR} %If $w_{\\sigma} > 0$ almost everywhere on $\\T$, then $\\lim_{k \\to \\infty}F_k(0) = 0$. If $|F| < 1$ almost everywhere on $\\T$, then $\\lim_{k \\to \\infty}F_k(0) = 0$. \\end{Thm} \\beginpf By \\eqref{eq77} if $|F| < 1$ almost everywhere on $\\T$, then $w_{\\sigma} > 0$ almost everywhere on $\\T$. By the classical Rakhmanov's theorem \\cite{R77}, see also Section 9.1 in \\cite{Simonbook1}, this implies $\\lim_{k \\ge 0} \\alpha_k = 0$. It remains to use Geronimus theorem. \\qed \\medskip Below, $W^1(\\T)$ stands for the Wiener algebra of all absolutely convergent Fourier series on $\\T$."}
{"input": "$w_\\sigma \\in W^1(\\T)$, $\\inf_{\\T}w_\\sigma > 0$\\textup{;} \\item[$(4)$] $\\nu = w_\\nu \\, dm_{\\T}$, where $w_\\nu \\in W^1(\\T)$, $\\inf_{\\T}w_\\nu > 0$. \\end{itemize} \\end{Thm} \\beginpf Equivalence $(2) \\Leftrightarrow (3)$ is the original Baxter's theorem \\cite{Baxter} (modulo Geronimus' theorem \\ref{tG}). Equivalence $(1) \\Leftrightarrow (2) \\& (3)$ was proved by Golinskii in \\cite{Gol97}. So, it suffices to prove the equivalence $(2) \\Leftrightarrow (4)$. For this we take $\\alpha \\in \\T$, set $F_{\\alpha} = \\alpha F$ and note that $(2)$ is equivalent to $\\sum_{k \\ge 0} |F_{\\alpha, k}(0)| < \\infty$ because $F_{\\alpha, k}(0) = \\alpha F_{k}(0)$ for all $k \\ge 0$. Assumption $\\sum_{k \\ge 1} |F_{\\alpha, k}(0)| < \\infty$ is equivalent to the assumption $(2)$ for $F_{\\alpha, 1}$, the first Schur iterate of $F_{\\alpha}$. In turn, the latter is equivalent to the fact that the probability measure $\\sigma_{\\alpha,1}$ corresponding to $F_{\\alpha, 1}$ satisfies assumption $(3)$ with $\\sigma$ replaced by $\\sigma_{\\alpha, 1}$. Now, choosing $\\alpha = \\frac{1-\\ov{F(0)}}{1-F(0)}$ we see from Lemma \\ref{lReduction} that $\\sigma_{\\alpha, 1} = \\nu/\\nu(\\T)$, and the result follows. \\qed \\medskip Next theorem deals with the Sobolev space $H_{1/2}(\\T)$ -- the set of functions $$ H_{1/2}(\\T) = \\biggl\\{g \\in L^1(\\T): \\; \\sum_{k \\in \\Z} |k||\\hat g(k)|^2 < \\infty\\biggr\\}, $$ where $\\hat g(k)$ is the $k$-th Fourier coefficient of a function on the unit circle $\\T$, see \\eqref{eq80}."}
{"input": "= e^{\\psi}$ for some $\\psi \\in H_{1/2}(\\T)$. \\end{itemize} \\end{Thm} \\beginpf The proof is similar to the proof of Theorem \\ref{tB}. The equivalence $(1) \\Leftrightarrow (2)$ is the original Szeg\\H{o}-Golinskii-Ibragimov theorem \\cite{GI71} (modulo Geronimus theorem \\ref{tG}). Let us show that $(3)$ is equivalent to $(1)$. For this we take $\\alpha \\in \\T$, set $F_{\\alpha} = \\alpha F$ and note that $(1)$ is equivalent to $\\sum_{k \\ge 0} k|F_{\\alpha, k}(0)|^2 < \\infty$ because $F_{\\alpha, k}(0) = \\alpha F_{k}(0)$ for all $k \\ge 0$. Assumption $\\sum_{k \\ge 0} k|F_{\\alpha, k}(0)|^2 < \\infty$ is equivalent to the assumption $(1)$ for $F_{\\alpha, 1}$, the first Schur iterate of $F_{\\alpha}$. In turn, the latter is equivalent to the fact that the probability measure $\\sigma_{\\alpha,1}$ corresponding to $F_{\\alpha, 1}$ satisfies assumption $(2)$ with $\\sigma$ replaced by $\\sigma_{\\alpha, 1}$. Now, choosing $\\alpha = \\frac{1-\\ov{F(0)}}{1-F(0)}$ we see from Lemma \\ref{lReduction} that $\\sigma_{\\alpha, 1} = \\nu/\\nu(\\T)$, and the result follows. \\qed \\medskip Let $q \\in \\cM$, and let $f_q$ is the Schur function of $\\Di_q$. Recall that the main spectral measure $\\mu_q$ of the Dirac operator $\\Di_{q}$ on $\\R_+$ is defined by \\begin{equation}\\label{eq81bis} \\frac{1-|f_{q}(z)|^2}{|1 - f_{q}(z)|^2} = \\frac{1}{\\pi}\\int_{\\R} \\frac{\\Im z}{|x - z|^2}\\, d\\mu_{q}(x), \\qquad z \\in \\C_+. \\end{equation} Writing $\\mu_q = w_{q}\\,dx+\\mu_{q,s}$ for $\\mu_{q,s} \\bot dx$, we obtain \\begin{equation}\\label{eq85} w_{\\sigma}(x) = \\frac{1-|f_q(x)|^2}{|1 - f_q(x)|^2} \\; \\mbox{ for Lebesgue almost all } \\; x \\in \\R, \\end{equation} and \\begin{equation}\\label{eq83} \\mu_q = *\\lim_{\\eps \\downarrow 0}\\frac{1-|f_q(x + i\\eps)|^2}{|1-f_q(x+i \\eps)|^2}\\,dx, \\end{equation} where the limit is understood in the $*$-weak sense."}
{"input": "to \\eqref{eq77}, \\eqref{eq78} and have essentially the same proofs. \\medskip Given $q \\in \\cM_{\\ell}$, we know that $f_q \\in S_{\\ell, *}(\\C_+)$, see Theorem \\ref{t2}. By Lemma \\ref{l10}, there exists $F_{q} \\in S_*(\\D)$ such that $F_{q}(e^{2i\\ell z}) = f_{q}(z)$, $z \\in \\C_+$. Below we prove Corollaries \\ref{c3}-\\ref{c5} based on Theorem \\ref{t2} and the following proposition. \\begin{Prop}\\label{p-reduction} Let $q\\in \\cM_{\\ell}$ and $f_q \\in S_{\\ell, *}(\\C_+)$ be the measure-valued potential and its Schur function. Also let $F_{q} \\in S_*(\\D)$ be such that $F_{q}(e^{2i\\ell z}) = f_{q}(z)$, $z \\in \\C_+$. Then the spectral measure $\\mu_q$ of $\\Di_{q}$ is periodic with period $\\pi/\\ell$ and satisfies \\begin{equation}\\label{eq89} \\frac{1-|F_{q}(\\lambda)|^2}{|1 - F_{q}(\\lambda)|^2} = \\frac{\\ell}{\\pi}\\int_{[-\\frac{\\pi}{2\\ell},\\frac{\\pi}{2\\ell})} \\frac{1-|\\lambda|^2}{|1 - \\bar \\lambda e^{2i\\ell x}|^2}\\, d\\mu_{q}(x), \\qquad \\lambda \\in \\D. \\end{equation} In particular, if $\\nu_{q}$ is the measure on $\\T$ generated by $F_{q}$ via \\eqref{eq76}, then $\\frac{\\ell}{\\pi}\\mu_{q}(E) = \\nu_{q}(E^*)$, where $E^* = \\{e^{2i\\ell x}, \\; x \\in E\\}$ for every Borel set $E \\subset [-\\frac{\\pi}{2\\ell},\\frac{\\pi}{2\\ell})$. \\end{Prop} \\beginpf Set $w_\\eps(x) = \\frac{1-|f_q(x + i\\eps)|^2}{|1-f_q(x+i \\eps)|^2}$, $x \\in \\R$, and take arbitrary $x_0 \\in \\R$. By \\eqref{eq83}, for the proof of \\eqref{eq89} it suffices to show that for every $\\lambda \\in \\D$ we have \\begin{equation}\\label{eq84} \\frac{1-|F_{q}(\\lambda)|^2}{|1 - F_{q}(\\lambda)|^2} = \\lim_{\\eps \\to 0}\\frac{\\ell}{\\pi}\\int_{[x_0-\\frac{\\pi}{2\\ell},x_0+\\frac{\\pi}{2\\ell})} \\frac{1-|\\lambda|^2}{|1 - \\bar \\lambda e^{2i\\ell x}|^2} w_{\\eps}(x)\\, dx. \\end{equation} For $\\eps> 0$, denote $r_\\eps = e^{-2\\ell \\eps}$."}
{"input": "a continuous $\\pi$-periodic function on $\\R$ with $\\inf_{x\\in \\R} |w_q (x)| = \\inf_{\\xi\\in \\T} |w_\\nu(\\xi)| > 0$. Moreover, for every $k\\in \\Z$ we have \\begin{gather*} \\frac{1}{\\pi}\\int_{-\\pi/2}^{\\pi/2}e^{-2ikx}w_q(x)\\,dx = \\int_{\\T}\\ov{\\xi}^kw_{\\nu}(\\xi)\\, dm_{\\T}. \\end{gather*} It follows that assumption $\\sum_{k \\ge 0}|q(\\{k\\})| < \\infty$ implies $\\mu_q = w_{q}\\,dx$ for a $\\pi$-periodic function $w_q$ such that $\\inf_{x\\in \\R} |w_q (x)| > 0$ and \\begin{gather*} w_{q} = \\sum_{k \\in \\Z} c_k e^{2i k x}, \\qquad \\sum_{k \\in \\Z} |c_k| = \\sum_{k \\in \\Z} |\\hat{w}_\\nu(k)| = \\|w_\\nu\\|_{W^1(\\T)} < \\infty. \\end{gather*} The argument is reversible: any measure $\\mu_q$ with these properties gives rise to the function $F_q$ such that $\\sum_{k \\ge 0} |F_{q, k}(0)| < \\infty$, i.e., $\\sum_{k \\ge 0}|q(\\{k\\})| < \\infty$ by Theorem \\ref{t2}. \\qed \\medskip {\\noindent \\bf Proof of Corollary \\ref{c5}.} By Theorem \\ref{t2}, assumption $\\sum_{k \\ge 0}k|q(\\{k\\})|^2 < \\infty$ is equivalent to the assumption $\\sum_{k \\ge 0}k|F_{q, k}(0)|^2 < \\infty$. Then, Szeg\\H{o}-Golinskii-Ibragimov theorem \\ref{tSGI} says that the last condition is equivalent to $\\nu = e^{\\psi} \\,dm_{\\T}$, $\\psi \\in H_{1/2}(\\T)$ for the measure $\\nu$ related to $F_{q}$ via \\eqref{eq76}. By Proposition \\ref{p-reduction}, this can be further equivalently reformulated in the following way: $\\mu_q = w_q\\,dx$ for a positive $\\pi$-periodic function $w_q$ that satisfies $\\log w_q(x) = \\psi(e^{2ix})$ on $\\R$. For these objects, we have \\begin{gather*} \\log w_q(x) = \\sum_{k \\in \\Z} c_k e^{2i k x}, \\qquad c_k = \\frac{1}{\\pi}\\int_{-\\pi/2}^{\\pi/2}e^{-2ikx}\\log w_q\\,dx = \\int_{\\T}\\ov{\\xi}^k\\psi(\\xi)\\, dm_{\\T}. \\end{gather*} In particular, the series $\\sum_{k \\in \\Z} |k||c_k|^2 = \\sum_{k \\in \\Z} |k||\\hat{\\psi}(k)|^2$ converge or not simultaneously."}
{"input": "where $\\eta(\\cdot)$ is defined in \\eqref{eq: szego theorem}. \\begin{Thm}\\label{t7} For all $F, G \\in \\szc$, $r\\in (0,1)$ such that $1-r\\ge 12\\log(\\eta(F,G)^{-1})$ we have \\begin{equation}\\label{eq18} \\frac{1}{2}\\rho_{B^1(r\\T)}(F,G) \\le \\|F - G\\|_{W^1(r\\T)} \\le 2\\rho_{B^1(r\\T)}(F,G). \\end{equation} \\end{Thm} By definition, we have $B^1(r\\T) \\subset S_*(\\D)$ for all $r \\in (0,1)$. The situation changes for $r=1$ and $r > 1$. Schur functions $F$ lying in $B^1(\\T)$ were considered by Baxter in his famous work \\cite{Baxter}, see Theorem \\ref{tB} above. For $r > 1$, Schur functions have exponentially decaying recurrence coefficients. This case was studied by P.\\,Nevai and V.\\,Totik in \\cite{NT89}, see also Section 7 in \\cite{Simonbook1}, \\cite{Simon06}, \\cite{GM06} and references in these works. However, we are not aware of inequalities analogous to \\eqref{eq18} for $r \\ge 1$. This remains an interesting open direction. We also do not know if \\eqref{eq18} holds (possibly with some different constants) if we change $12$ in the statement of Theorem \\ref{t7} by arbitrary $\\eps> 0$. See Lemma \\ref{l2p3} below for the case where $g = 0$. \\medskip For the proof of Theorem \\ref{t7} we need some auxiliary results. Take a function $F \\in \\szc$. The inequality $x \\le -\\log(1 - x)$ holds for all $x\\in [0,1)$, let us apply it to \\eqref{eq: szego theorem}. We have \\begin{align} \\label{eq: rec coeffs norm} \\sum_{k = 0}^\\infty|F_k(0)|^2 \\le \\sum_{k = 0}^\\infty-\\log(1 - |F_k(0)|^2) &= \\log \\eta(F)^{-1}, \\\\ \\label{eq: fourier coeffs norm} \\sum_{k = 0}^\\infty|\\hat{F}(k)|^2 = \\int_{\\T}|F|^2\\,dm_{\\T} \\le \\int_{\\T}-\\log(1 - |F|^2)\\,dm_{\\T} &= \\log \\eta(F)^{-1}."}
{"input": "claim}. Together with Lemma \\ref{lem sum prod norm coeff} this gives (we use again the multiplicative property of the norm $\\| \\cdot \\|_{W^1(r\\T)}$) \\begin{align} \\nonumber \\bigg\\|\\sum_{l =1 }^{\\infty}z^{l }&(z\\ov{G_{k + l}(0)}G_{k + l + 1} + \\ov{G_{k + l - 1}(0)} F_{k + l - 1})\\big(F_{k + l} - G_{k + l}\\big)\\bigg\\| \\le \\\\ \\nonumber &\\le 2\\sum_{l =1 }^{\\infty}r^{l }\\big(|G_{k + l}(0)|\\cdot\\|G_{k + l + 1}\\| + |G_{k + l - 1}(0)|\\cdot\\| F_{k + l - 1}\\|\\big)\\sum_{t = 0}^\\infty r^{t}|F_{k + l + t}(0) - G_{k + l + t}(0)| \\\\ \\nonumber &\\le 2\\sum_{l = 1}^{\\infty}\\big(|G_{k + l}(0)|\\cdot\\|G_{k + l + 1}\\| + |G_{k + l - 1}(0)|\\cdot\\| F_{k + l - 1}\\|\\big) \\cdot \\sup_{\\ell \\ge 1} \\sum_{t = 0}^\\infty r^{t+\\ell}|F_{k + \\ell + t}(0) - G_{k + \\ell + t}(0)| \\\\ \\label{eq: fk - gk 3} &\\le \\frac{4(T + 1)}{T^2} \\sum_{s = 1}^\\infty r^s|F_{k + s}(0) - G_{k + s}(0)|. \\end{align} Denote $C(T) = \\frac{4(T + 1)}{T^2} + \\frac{1}{T} = \\frac{5T + 4}{T^2}$. The substitution of \\eqref{eq: fk - gk 2} and \\eqref{eq: fk - gk 3} into \\eqref{eq: fk - gk 1} implies \\begin{multline}\\label{eq: dsbnidsere} \\left(1 - C(T)\\right)\\sum_{s = 0}^\\infty r^s|F_{k + s}(0) - G_{k + s}(0)|\\le \\\\ \\le \\|(F_k - G_k) - z\\ov{G_{k}(0)}G_{k + 1}(F_{k} - G_{k})\\| \\le \\\\ \\le \\left(1 + C(T)\\right)\\sum_{s = 0}^\\infty r^s|F_{k + s}(0) - G_{k + s}(0)|."}
{"input": "- G_k) - z\\ov{G_{k}(0)}G_{k + 1}(F_{k} - G_{k}) \\| \\le \\frac{T+1}{T}\\|F_k - G_k\\|. \\end{gather} Therefore, for $T > 1$ from \\eqref{eq: dsbnidsere} and \\eqref{eq: dsbnidsere2} we see that \\begin{align*} \\frac{T(1-C(T))}{T+1} \\sum_{l = 0}^{\\infty}r^{l}|F_{k + l}(0) - G_{k + l}(0)| \\le \\|F_k - G_k\\|_{W^1(r\\T)},\\\\ \\frac{T(1+C(T))}{T-1}\\sum_{l = 0}^{\\infty}r^{l}|F_{k + l}(0) - G_{k + l}(0)| \\ge \\|F_k - G_k\\|_{W^1(r\\T)}. \\end{align*} To complete the induction step, it remains to check that for $T = 12$ we have $$ \\frac{1}{2} \\le \\frac{T(1-C(T))}{T+1}, \\qquad \\frac{T(1+C(T))}{T-1} \\le 2, $$ where $C(T) = \\frac{5T + 4}{T^2}$. This is indeed the case. \\medskip To finish the proof, we need to get rid of the assumption $F_{n + 1} = G_{n + 1} = 0$. For this, we take arbitrary Schur functions $F, G \\in \\szc$, fix $n \\in \\Z_+$, and consider the functions $F^{(n)}, G^{(n)} \\in \\szc$ such that $$ F^{(n)}_k(0) = \\begin{cases} F_k(0), & k \\le n,\\\\ 0, & k > n, \\end{cases} \\qquad G^{(n)}_k(0) = \\begin{cases} G_k(0), & k \\le n,\\\\ 0, & k > n. \\end{cases} $$ From \\eqref{eq: szego theorem} we know that $\\eta(F^{(n)}, G^{(n)})\\ge \\eta(F, G)$, hence the previous part of the proof works for $F^{(n)}$ and $G^{(n)}$. It gives \\begin{gather*} \\frac{1}{2}\\|F^{(n)} - G^{(n)}\\|_{W^1(r\\T)} \\le \\rho_{B^1(r\\T)}(F^{(n)}, G^{(n)}) \\le 2\\|F^{(n)}-G^{(n)}\\|_{W^1(r\\T)}. \\end{gather*} As $n\\to\\infty$, we have $$ \\rho_{B^1(r\\T)}(F^{(n)}, G^{(n)}) \\to \\rho_{B^1(r\\T)}(F, G),\\qquad \\|F^{(n)} - G^{(n)}\\|_{W^1(r\\T)}\\to \\|F - G\\|_{W^1(r\\T)}."}
{"input": "L]$. Relation \\eqref{tmp: 5.16} implies $$ \\sum_{k = 0}^{k_*}\\|N_q(\\ell(k+1), \\ell k) - e^{JQ[\\ell k]}\\| = O\\left(\\ell \\|q\\|^2_{L^2([0,x])}\\right), $$ which tends to zero as $\\ell \\to 0$ (the constant in $O(\\cdot)$ is uniform in $\\ell \\in [0,1]$, $x\\in [0,L]$). Finally, \\eqref{tmp: 5:15} implies that $\\|N(x) - N_q(\\ell k_*)\\| \\to 0$ as $\\ell \\to 0$ uniformly in $x\\in [0,L]$, and the proof is completed. \\qed \\medskip \\begin{Lem}\\label{lem: first discr} For every $q\\in L^2(\\R_+)$ we have \\begin{equation}\\label{eq91} \\lim_{\\ell \\to 0}\\frac{1}{\\ell }\\sum_{k\\ge 0} |q[\\ell k]|^2 = \\|q\\|_{L^2(\\R_+)}^2. \\end{equation} \\end{Lem} \\beginpf Recall that $\\Delta_{\\ell, k} = [\\ell k, \\ell k+\\ell)$. The Cauchy-Schwarz inequality gives \\begin{gather}\\label{eq: lem1 ineq} \\frac{1}{\\ell}\\sum_{k\\ge 0} |q[\\ell k]|^2 = \\frac{1}{\\ell}\\sum_{k\\ge 0}\\bigg|\\int_{\\Delta_{\\ell, k}}q(x)\\,dx\\bigg|^2 \\le \\frac{1}{\\ell}\\sum_{k\\ge 0} \\ell\\cdot \\int_{\\Delta_{\\ell, k}}|q(x)|^2\\,dx = \\|q\\|_{L^2(\\R_+)}^2. \\end{gather} In particular, both sides of \\eqref{eq91} with $\\lim$ replaced by $\\limsup$ or $\\liminf$ depend continuously on $q$ in $L^2(\\R_+)$-norm. Hence, it suffices to prove \\eqref{eq91} only on a dense subset of $L^2(\\R_+)$. Let $C_0^\\infty(\\R_+)$ be the set of infinitely smooth functions on $\\R_+$ with compact support. Take some $q\\in C_0^\\infty(\\R_+)$ and let $R> 0$ be such that $\\supp q\\subset [0, R]$. Then \\begin{gather*} \\frac{1}{\\ell}|q[\\ell k]|^2 - \\int_{\\Delta_{\\ell, k}}|q(x)|^2\\,dx = \\int_{\\Delta_{\\ell, k}}q(x)\\bigg(\\frac{1}{\\ell}\\int_{\\Delta_{\\ell, k}}\\ov{q(y)}\\,dy - \\ov{q(x)}\\bigg)\\,dx = O(\\ell^2), \\quad \\ell \\to 0."}
{"input": "k|e^{-A\\ell k}. \\end{gather*} It follows that \\begin{gather*} \\left|h(\\xi)e^{-A\\xi/2} - h(2\\ell k)e^{-A\\ell k}\\right| \\le 2Ce^{-A\\ell k}|\\xi - 2\\ell k|\\cdot\\|g_{q_{\\ell}} - g_{\\tilde q_{\\ell}}\\|_{L^1(\\R)}. \\end{gather*} Recall that $g_{q_{\\ell}}, g_{\\tilde q_{\\ell}}$ are supported on the interval of length $\\pi/\\ell$ and that $\\|g_{q_{\\ell}}\\|_{L^2(\\R)}, \\|g_{\\tilde q_{\\ell}}\\|_{L^2(\\R)}$ are uniformly bounded in $\\ell$ by \\eqref{eq: gr to g in L2}. We get \\begin{gather*} \\|g_{q_{\\ell}} - g_{\\tilde q_{\\ell}}\\|_{L^1(\\R)}\\le \\sqrt{\\pi/\\ell}\\|g_{q_{\\ell}} - g_{\\tilde q_{\\ell}}\\|_{L^2(\\R)} \\le \\sqrt{\\pi/\\ell}\\big(\\|g_{q_{\\ell}}\\|_{L^2(\\R)} + \\|g_{\\tilde q_{\\ell}}\\|_{L^2(\\R)}\\big) = O(1/\\sqrt{\\ell}), \\qquad \\ell \\to 0. \\end{gather*} Therefore, the $k$-th term in the right-hand side of \\eqref{eq: darboux sum v2} is $O(\\ell^{3/2}e^{-A\\ell k})$ as $\\ell \\to 0$ and the total sum can be estimated by \\begin{gather*} O\\bigg(\\sum_{k\\ge 0}\\ell^{3/2}e^{-A\\ell k}\\bigg) = O\\bigg(\\frac{\\ell^{3/2}}{1 - e^{-A\\ell}}\\bigg) = O(\\ell^{1/2}),\\qquad \\ell \\to 0. \\end{gather*} In particular, it tends to $0$ as $\\ell \\to 0$, as was required in \\eqref{eq: darboux sum}. The proof is concluded. \\qed\\medskip \\subsection{Proof of Theorem~\\ref{t1}} Recall that we need to prove \\begin{gather} \\label{eq: theorem 2.1 expand} \\sqrt{\\frac{\\pi}{2}}\\int_{\\R_+}|q-\\tilde q|e^{-2Ax}\\,d x\\le \\int_{\\R_+}|\\hat f_q - \\hat f_{\\tilde q}|e^{-A\\xi}\\,d\\xi \\le 2\\sqrt{2\\pi}\\int_{\\R_+}|q-\\tilde q|e^{-2Ax}\\,dx, \\end{gather} where $q, \\tilde q\\in L^2(\\R_+)$ and $A\\in \\R$ satisfies $A \\ge 12\\max\\big(\\|q\\|_{L^2(\\R_+)}^2, \\|\\tilde q\\|^2_{L^2(\\R_+)}\\big)$. Without loss of generality, we can assume that \\begin{gather} \\label{eq: strict assumption on A} A > 12\\max\\big(\\|q\\|_{L^2(\\R_+)}^2, \\|\\tilde q\\|^2_{L^2(\\R_+)}\\big), \\end{gather} the claim for $A = 12\\max\\big(\\|q\\|_{L^2(\\R_+)}^2, \\|\\tilde q\\|^2_{L^2(\\R_+)}\\big)$ will follow by a limiting argument (it worth be mentioned that the constant $12$ is not optimal). Fix $\\eps > 0$. Let us show that the inequality \\begin{gather} \\label{eq: prev lemma 8."}
{"input": "\\sum_{k\\ge 0}e^{-A\\ell k}|\\hat F_{q_{\\ell}}(k) - \\hat F_{\\tilde q_{\\ell}}(k)| \\le \\frac{2}{1 - \\eps}\\sum_{k \\ge 0}e^{-A\\ell k}|q[\\ell k]-\\tilde q[\\ell k]| \\end{gather} holds for all sufficiently small $\\ell$. Then \\eqref{eq: theorem 2.1 expand} will follow if we take $\\ell \\to 0$ in \\eqref{eq: prev lemma 8.6}, apply Lemma \\ref{final lem 6.2} and Lemma \\ref{final lem 6.11}, and send $\\eps\\to 0$. Thus, we can now focus on \\eqref{eq: prev lemma 8.6}. Let $r = e^{-A\\ell}$ and recall the definition \\eqref{eq181} of the metric in $W^1(r\\T)$. We have \\begin{gather} \\label{eq: tmp 2} \\sum_{k\\ge 0}e^{-A\\ell k}|\\hat F_{q_{\\ell}}(k) - \\hat F_{\\tilde q_{\\ell}}(k)| = \\sum_{k\\ge 0}r^k|\\hat F_{q_{\\ell}}(k) - \\hat F_{\\tilde q_{\\ell}}(k)| =\\|F_{q_{\\ell}}- F_{\\tilde q_{\\ell}}\\|_{W^1(r\\T)}. \\end{gather} Note that $1 - r = 1 - e^{-A\\ell} = A\\ell + o(\\ell)$ as $\\ell\\to 0$. Lemma \\ref{lem: eta in limit} shows that \\begin{gather*} \\max\\big(\\|q\\|_{L^2}^2, \\|\\tilde q\\|^2_{L^2}\\big)\\ell= \\max\\big(\\log\\eta(F_{q_{\\ell}})^{-1}, \\log\\eta(F_{\\tilde q_{\\ell}})^{-1}\\big) + o(\\ell),\\quad \\ell\\to 0. \\end{gather*} Then assumption \\eqref{eq: strict assumption on A} for small $\\ell$ implies \\begin{gather*} 1 - r \\ge 12\\max\\big(\\log\\eta(F_{q_{\\ell}})^{-1}, \\log\\eta(F_{\\tilde q_{\\ell}})^{-1}\\big). \\end{gather*} Therefore, Theorem \\ref{t7} applies to Schur functions $F_{q_{\\ell}}, F_{\\tilde q_{\\ell}}$ on the circle of radius $r = e^{-A\\ell}$ if $\\ell$ is small enough. It gives \\begin{gather} \\label{eq: tmp 3} \\frac{1}{2}\\rho_{B^1(r\\T)}(F_{q_{\\ell}}, F_{\\tilde q_{\\ell}}) \\le \\|F_{q_{\\ell}}- F_{\\tilde q_{\\ell}}\\|_{W^1(r\\T)} \\le 2\\rho_{B^1(r\\T)}(F_{q_{\\ell}}, F_{\\tilde q_{\\ell}}). \\end{gather} According to the definition \\eqref{eq: SW metric def} of metric $\\rho_{B^1(r\\T)}$ and \\eqref{eq: fqr and Fqr}, we have \\begin{gather} \\label{eq: norm on hT} \\rho_{B^1(r\\T)}(F_{q_{\\ell}}, F_{\\tilde q_{\\ell}}) = \\sum_{k\\ge 0}r^k|F_{q_{\\ell},k}(0) - F_{\\tilde q_{\\ell},k}(0)|. %= \\sum_{k\\ge 0}e^{-A\\ell k}|f_{q_{\\ell},k}(\\infty) - f_{\\tilde q_{\\ell},k}(\\infty)|."}
{"input": "\\ref{t2} state \\begin{gather*} q[\\ell k] = \\varkappa(F_{q_{\\ell},k}(0)),\\qquad \\tilde q[\\ell k] = \\varkappa(F_{\\tilde q_{\\ell},k}(0)). \\end{gather*} The straightforward calculation shows that for $u,v\\in \\C$ small enough we have \\begin{gather} \\label{eq: kappa property} |(\\varkappa(u) - \\varkappa(v)) + (\\ov{u} - \\ov{v})|\\le \\eps|u - v|. \\end{gather} From \\eqref{eq: sum fqrk(infty)} and \\eqref{eq: fqr and Fqr} we know that $\\lim\\limits_{\\ell \\to 0}F_{q_{\\ell},k}(0) = \\lim\\limits_{\\ell \\to 0}F_{\\tilde q_{\\ell},k}(0) = 0$ hence \\eqref{eq: kappa property} applies. It gives \\begin{align*} \\big|(q[\\ell k]-\\tilde q[\\ell k]) + (\\ov{F_{q_{\\ell},k}(0)} - \\ov{F_{\\tilde q_{\\ell}, k}(0)})\\big|\\le \\eps\\big|F_{q_{\\ell},k}(0) - F_{\\tilde q_{\\ell}, k}(0)\\big|,& \\\\ (1 - \\eps)\\big|F_{q_{\\ell},k}(0) - F_{\\tilde q_{\\ell}, k}(0)\\big|\\le \\big|q[\\ell k]-\\tilde q[\\ell k]\\big| \\le (1 + \\eps)\\big|F_{q_{\\ell},k}(0) - F_{\\tilde q_{\\ell}, k}(0)\\big|.& \\end{align*} It follows from \\eqref{eq: norm on hT} that \\begin{gather} \\label{eq: tmp 1} \\frac{1}{1 +\\eps}\\sum_{k \\ge 0}e^{-A\\ell k}|q[\\ell k]-\\tilde q[\\ell k]|\\le \\rho_{B^1(r\\T)}(F_{q_{\\ell}}, F_{\\tilde q_{\\ell}}) \\le \\frac{1}{1 -\\eps}\\sum_{k \\ge 0}e^{-A\\ell k}|q[\\ell k]-\\tilde q[\\ell k]|. \\end{gather} To establish \\eqref{eq: prev lemma 8.6}, we substitute \\eqref{eq: tmp 1} and \\eqref{eq: tmp 2} into \\eqref{eq: tmp 3}."}
{"input": "have $\\sup_n\\int_{\\R} |\\log(1 - |r_n|^2)|\\,dx = \\sup_n\\rho^2_X(0, r_n)< \\infty$. Moreover, $\\int_{\\R} |\\phi|^{2N}\\log(1 - |\\phi|^2)\\,dx \\to 0$ by the Lebesgue dominated convergence theorem with the majorant $|\\log (1 - |\\phi|^2)|$. Thus, we only need to check that $$ \\lim_{N \\to \\infty}\\lim_{n \\to \\infty}\\Re\\sum_{k = 1}^{N}\\int_{\\R}\\frac{(\\ov{r_n}\\phi)^k}{k}\\,dx = E(r, \\phi). $$ We have $\\phi \\in L^2(\\R)$ by Lemma \\ref{lem: conv in X implies conv in L2} for $r_1 = \\phi$, $r_2 =0$. In view of $\\|\\phi\\|_{L^\\infty(\\R)} \\le 1$, this implies $\\phi^k \\in L^2(\\R)$ for every $k\\ge 1$. Then for each fixed $N \\ge 1$ we obtain $$ \\lim_{n \\to \\infty}\\Re\\sum_{k = 1}^{N}\\int_{\\R}\\frac{(\\ov{r_n}\\phi)^k}{k}\\,dx = \\Re\\int_{\\R}\\sum_{k = 1}^{N}\\frac{(\\ov{r}\\phi)^k}{k}\\,dx $$ from the weak convergence $r_n^k\\to r^k$ in $L^2(\\R)$. Moreover, $$ \\left|\\sum_{k = 1}^{N}\\frac{(\\ov{r}\\phi)^k}{k}\\right| \\le \\sum_{k = 1}^{N}\\frac{|r\\phi|^k}{k} \\le \\sqrt{\\log(1 - |r|^2)\\log(1 - |\\phi|^2)}, $$ where the r.h.s.\\ belongs to $L^1(\\R)$ because $r, \\phi \\in X$. Thus, by the Lebesgue dominated convergence theorem we have $$ \\lim_{N \\to \\infty}\\Re\\int_{\\R}\\sum_{k = 1}^{N}\\frac{(\\ov{r}\\phi)^k}{k}\\,dx =\\Re\\int_{\\R}\\sum_{k = 1}^{\\infty}\\frac{(\\ov{r}\\phi)^k}{k} = \\int_{\\R}-\\log|1 - \\ov{r}\\phi|\\,dx = E(r, \\phi). \\eqno\\qed $$ \\begin{Lem}\\label{lem: conv in S2 and conv in measure v2} Let $r_n,r\\in X$. The following assertions are equivalent: \\begin{enumerate} \\item[(a)] $r_n$ converges to $r$ in $X$\\textup{;} \\item[(b)] $\\lim\\limits_{n\\to\\infty}E(r_n) = E(r)$ and $r_n$ converges to $r$ in Lebesgue measure on $\\R$\\textup{;} \\item[(c)] $\\lim\\limits_{n\\to\\infty}E(r_n) = E(r)$ and $\\lim\\limits_{n\\to\\infty}E(r_n, \\phi) = E(r, \\phi)$ for every $\\phi\\in X$."}
{"input": "\\infty}\\int_{\\R}|\\phi|^N\\sum_{k = 1}^{\\infty}\\frac{|\\phi|^k}{k}\\,dx = \\lim_{N\\to \\infty}\\int_{\\R}|\\phi|^N \\log(1-|\\phi|)\\,dx = 0, $$ because $\\log(1-|\\phi|) \\in L^{1}(\\R)$. Thus, to prove that $E(r_n, \\phi) \\to E(r, \\phi)$ it suffices to check that \\begin{align} \\label{eq: conv of terms in log} &\\lim_{n\\to\\infty}\\Re\\sum_{k = 1}^{N}\\int_{\\R}\\frac{(\\ov{r_n}\\phi)^k}{k}\\,dx = \\Re\\sum_{k = 1}^{N}\\int_{\\R}\\frac{(\\ov{r}\\phi)^k}{k}\\,dx,\\\\ &\\lim_{N\\to\\infty}\\Re\\sum_{k = 1}^{N}\\int_{\\R}\\frac{(\\ov{r}\\phi)^k}{k}\\,dx = \\int_{\\R}\\Re\\sum_{k = 1}^{\\infty}\\frac{(\\ov{r}\\phi)^k}{k}\\,dx =E(r, \\phi).\\label{eq102} \\end{align} As in the proof of Lemma \\ref{lem: convergence mixed energy}, relation \\eqref{eq102} is a consequence of the Lebesgue dominated convergence theorem (this time -- with the majorant $|\\log(1-|\\phi|)|$). So, we can focus on the proof of \\eqref{eq: conv of terms in log}. Since $|\\bar r_n^k|\\le 1$ and $\\phi^k \\in L^1(\\R)$ for all $n$, $k$, it suffices to prove that \\begin{equation}\\label{eq1031} \\lim_{n\\to\\infty}\\int_{\\R}\\ov{r_n}^{k}\\psi\\,dx = \\int_{\\R}\\ov{r}^{k}\\psi\\,dx \\end{equation} for functions $\\psi$ from a dense subset of $L^1(\\R)$. By our assumption, $\\lim_{n\\to\\infty} r_n^k(z) = r^k(z)$ for every $k \\ge 1$ and $z\\in \\C_+$. Thus, \\eqref{eq1031} holds for $\\psi$ from the set of all finite linear combinations of Poisson kernels $P_z: x \\mapsto \\frac{1}{\\pi}\\Im \\frac{1}{x - z}$, $z\\in \\C_+$. This set is dense in $L^1(\\R)$, which completes the proof. \\qed \\medskip \\noindent {\\bf Proof of Theorem \\ref{thm: convergence}}. Let $r_n, r\\in S_{2}(\\C_+)$ be such that $r_n \\to r$ in $S_{2}(\\C_+)$. Then $E(r_n) \\to E(r)$ by Lemma \\ref{lem: conv in S2 and conv in measure v2}. Moreover, we have $r_n \\to r$ in the Hardy space $H^2(\\C_+)$ by Lemma \\ref{lem: conv in X implies conv in L2}. In particular, $r_n \\to r$ uniformly on compact subsets of $\\C_+$."}
{"input": "The claim will then follow. Note that the first relation in \\eqref{eq: claim for constant potentials} is a simple consequence of the definition of $q_{T, 1}$, $q_{T, 1 - \\eps(T)}$, and we only need to check the second relation. To fix notation, take $p > 0$ and let $q\\in L^2(\\R_+)$ be the piece-wise constant potential such that $q = ip$ on $[0, T]$, $q = 0$ on $(T,\\infty)$. Then $Q(t) = \\left(\\begin{smallmatrix} p & 0 \\\\ 0 & -p \\end{smallmatrix}\\right)$ on $[0, T]$ and the fundamental solution $N(t,z)$ of the corresponding Dirac system $JN' + QN = zN$, $N(0, z)=\\idm$, for $0 \\le t \\le T$, $z \\in \\C$, is given by $$ N(t, z) =\\exp(tJ(Q - z)) = \\exp\\left(t\\left(\\begin{smallmatrix} 0 & p + z \\\\ p- z & 0 \\end{smallmatrix}\\right)\\right), $$ that can be checked by the differentiation with respect to $t$. Then, recall Lemma \\eqref{l38}, $$ N(T, z) = \\begin{pmatrix} \\cosh (T\\lambda) & \\frac{p + z}{\\lambda}\\sinh(T\\lambda)\\\\ \\frac{\\lambda}{p + z}\\sinh(T\\lambda) & \\cosh(T\\lambda) \\end{pmatrix}, \\qquad \\lambda = \\sqrt{p^2 - z^2}. $$ Notice that $N(T,z)$ is an entire function in $z$ and its does not depend on the choice of the root $\\lambda = \\sqrt{p^2 - z^2}$. Indeed, this follows from the fact that functions $\\cosh(T\\lambda)$, $\\sinh(T\\lambda)/\\lambda$ depend only on the value $\\lambda^2$. Further, for $t > T$ we have $Q(t) = \\left(\\begin{smallmatrix} 0 & 0 \\\\ 0 & 0 \\end{smallmatrix}\\right)$, hence \\begin{gather*} N(t,z) = \\exp(-(t - T)zJ)N(T, z), \\qquad t \\ge T."}
{"input": "system with the Hamiltonian $\\Hh_q = N(t, 0)^*N(t, 0)$ that corresponds to the Dirac operator with the potential $q$ is given by $M(t,z) = N(t, 0)^{-1}N(t, z)$, see, e.g., Section 2.4 in \\cite{B2020}. Therefore, the Weyl function of this canonical system equals $$ m_{\\Hh_q} \\doteq \\lim_{t \\to \\infty}\\sigma_1 M(t,z)^{T} \\sigma_1 \\omega \\doteq \\lim_{t \\to \\infty}\\sigma_1 N(t,z)^{T} (N(t, 0)^{-1})^T \\sigma_1\\omega \\doteq \\lim_{t \\to \\infty}\\sigma_1 N(t,z)^{T} \\sigma_1\\tilde\\omega(t), $$ where $\\tilde\\omega(t) = \\sigma_1(N(t, 0)^{-1})^T \\sigma_1 \\omega$ belongs to $\\C_+$ because $\\omega\\in \\C_+$ and $\\sigma_1(N(t, 0)^{-1})^T \\sigma_1 \\in \\sltr$. So, we have \\begin{align*} \\lim_{t \\to \\infty}\\sigma_1 N(t,z)^{T} \\sigma_1\\tilde\\omega(t) &\\doteq \\sigma_1 N(T,z)^{T} \\sigma_1 \\lim_{t \\to \\infty}\\sigma_1 \\exp(-(t - T)zJ)^{T} \\sigma_1\\tilde\\omega(t) \\\\ &\\doteq \\sigma_1 N(T,z)^{T} \\sigma_1 i \\doteq N(T,z) i, \\end{align*} where we used the fact that the fractional-linear transformation with the matrix $\\sigma_1 \\exp(-szJ)^{T} \\sigma_1$ for large $s > 0$ maps $\\C_+$ into a small disk with center at $i$. Then, \\begin{align*} f_q &= \\frac{m_q - i}{m_q + i} = \\frac{m_{\\Hh_q} - i}{m_{\\Hh_q} + i} \\doteq \\begin{pmatrix}1 &-i \\\\ 1 &i \\end{pmatrix} N(T,z) i \\doteq \\begin{pmatrix}1 &-i \\\\ 1 &i \\end{pmatrix} \\begin{pmatrix} i\\cosh(T\\lambda) + \\frac{p + z}{\\lambda}\\sinh(T\\lambda)\\\\i\\frac{\\lambda}{p + z}\\sinh(T\\lambda) + \\cosh(T\\lambda) \\end{pmatrix} \\\\ &= \\frac{\\left(\\frac{p + z}{\\lambda} + \\frac{\\lambda}{p + z}\\right)\\sinh(T\\lambda)}{2i\\cosh(T\\lambda) + \\left(\\frac{p + z}{\\lambda} - \\frac{\\lambda}{p + z}\\right)\\sinh(T\\lambda)} = \\frac{\\frac{2p}{\\lambda}\\sinh(T\\lambda)}{2i\\cosh(T\\lambda) + \\frac{2z}{\\lambda}\\sinh(T\\lambda)}\\\\ &=\\frac{-ip\\sinh(T\\lambda) }{\\lambda\\cosh(T\\lambda) - iz \\sinh(T\\lambda)}. \\end{align*} From now on we will assume that $z = x$ with real $x\\in [c_1p, c_2p]$, where $c_1 = \\sqrt{3/4}$, $c_2 = \\sqrt{8/9}$."}
{"input": "$\\lambda\\in [p/3, p/2]$. For brevity, we will write $f = f_q$ and $\\eps = \\eps(T)$ in the remaining part of the proof. We have \\begin{gather*} f(x) = \\frac{-ip(e^{T\\lambda} - e^{-T\\lambda})}{\\lambda(e^{T\\lambda} + e^{-T\\lambda}) - ix (e^{T\\lambda} - e^{-T\\lambda})} = \\frac{-ip + ipe^{-2T\\lambda}}{(\\lambda - ix) + (\\lambda + ix)e^{-2T\\lambda}} =g(x) + e^{-2T\\lambda}h(x), \\\\ g(x) = \\frac{-ip}{\\lambda - ix},\\qquad h(x) = e^{2T\\lambda}(f_q(x) - g(x)) = \\frac{-2ip\\lambda}{(x+i\\lambda)^2 - p^2 e^{-2T\\lambda}}. \\end{gather*} Let us denote $q = q_{T, 1}$ and $q_{\\eps} = q_{T, 1 - \\eps}$, and let $f_{\\eps}, g_\\eps, h_\\eps$ be the functions corresponding to $q_{\\eps}$. We have \\begin{align*} |g(x) - g_{\\eps}(x)| &= \\left|\\frac{p}{\\lambda - ix} -\\frac{p_\\eps}{\\lambda_\\eps - ix} \\right| = \\frac{|p\\lambda_\\eps - p_\\eps\\lambda + ix(p_\\eps - p)|}{|(\\lambda - ix)(\\lambda_\\eps - ix)|}\\ge \\frac{|p\\lambda_\\eps - p_\\eps\\lambda|}{pp_{\\eps}} \\\\ &= \\left|\\lambda_\\eps /p_\\eps - \\lambda/p\\right| = \\left|\\sqrt{1 - {x^2}/{p_\\eps^2}} - \\sqrt{1 - {x^2}/{p^2}}\\right| = \\frac{{x^2}/{p^2} - {x^2}/{p_\\eps^2}}{\\sqrt{1 - {x^2}/{p_\\eps^2}} + \\sqrt{1 - {x^2}/{p^2}}}\\\\ &\\ge \\frac{{x^2}/{p_\\eps^2} - {x^2}/{p^2}}{2} \\ge \\frac{x^2}{p^2}(1 - p_{\\eps}^2/p^2)\\ge c_1^2(1 - p_{\\eps}^2/p^2). \\end{align*} Recall that $p = 1/\\sqrt{T}$, $p_\\eps = (1-\\eps)/\\sqrt{T}$. Hence $1 - p_{\\eps}^2/p^2 = 1 - (1 - \\eps)^2 \\ge \\eps$ for small $\\eps$ (equivalently, for large $T$). It follows that \\begin{gather} \\label{eq: g - geps} |g(x) - g_{\\eps}(x)| \\ge \\frac{c_1^2\\eps}{2} = \\frac{3\\eps}{8},\\qquad x\\in [c_1p,c_2p]."}
{"input": "\\section{Introduction} Understanding how individuals find partners in modern marriage markets is central to the study of family economics \\citep{chiappori2023mating}. As marriage rates decline and matching technologies proliferate, questions surrounding search efficiency, partner availability, and market design have grown increasingly salient. While theoretical models of two-sided search abound, empirical research has been hampered by data limitations: traditional sources such as administrative records or surveys capture only realized marriages, omitting the entire search and matching process that precedes them, as well as the presence of singles who are not seriously seeking partners. Consequently, little is known about the dynamics of partner search, how modern platforms shape matching outcomes, or how efficiently modern markets operate. This study addresses these limitations by using novel data from IBJ, Japan's largest structured marriage matching platform, which accounts for approximately 3.3\\% of all marriages in Japan in 2024. The dataset includes over 10,000 confirmed engagements annually and allows for behavioral analysis of the full matching process, including search, proposals, and formal engagements. In this note, we focus on documenting macro-level trends on the platform as a first step toward more detailed analysis, leaving in-depth investigations of user behavior and underlying mechanisms to future work. There are four distinctive and advantageous features of the IBJ data compared to dating apps \\citep{hitsch2010matching,ong2015income,bapna2016one,egebark2021brains,rios2023improving}, speed date events \\citep{fisman2006gender,belot2013dating}, and government records \\citep{chiappori2017partner}. First, the data contain rich and verified user demographics, which are linked to official records such as tax withholding documents and certified health checkup results."}
{"input": "selected into serious partnership search due to substantial screening and financial costs. Third, the dataset records full, time-stamped logs of search and interaction behavior linked to confirmed outcomes, and includes information on whether users were active or inactive in their marriage search at each point in time. Fourth, the data are observed at the level of individual user behavior logs, enabling high-frequency analysis. Importantly, the IBJ data's richness and granularity, combined with its one-to-one matching structure, enable a direct analogy to labor market matching function estimation \\citep{petrongolo2001looking}, allowing us to assess platform-level efficiency and responsiveness. Using this dataset, I estimate a matching function following the nonparametric approach of \\citet{lange2020beyond} to evaluate the platform's performance.\\footnote{Their approach is widely applied to labor market settings. See \\cite{otani2024onthejob,otani2024nonparametric} and \\cite{kanayama2024nonparametric}.} I find that matching efficiency increased fourfold between 2014 and 2025, with sharp gains beginning in 2017, coinciding with major platform consolidation. Engagement formation also became more responsive to user composition, as indicated by rising elasticities. These results underscore the role of digital intermediation in shaping modern family formation. Unlike traditional arrangements mediated by kinship networks or local norms, platforms like IBJ provide structured, data-rich environments where frictions are minimized, outcomes are verified, and behavioral dynamics are observable, enabling new empirical insights."}
{"input": "\\section{Data} I use confidential data from IBJ covering the period 2014--2025. In 2024, IBJ accounted for 3.3\\% of all marriages in Japan, with over 10,000 engagements annually. While this share is smaller than administrative marriage registers, IBJ offers unique visibility into the pre-marriage process. The platform collects verified user data, including age, income, education, and marital status, and tracks the full sequence of user actions: search, proposals, messages, and dates. All records are time-stamped and linked to observed engagement outcomes. IBJ users are highly selected into serious long-term partnership search. Entry requires screening and significant upfront, monthly, and exit fees, which effectively exclude casual users. In contrast to dating apps that rely on self-reported profiles and open-ended interactions, IBJ functions as a closed, consultant-mediated two-sided matching platform where mutual consent leads to a confirmed engagement. This structured environment makes the data well-suited for analyzing matching efficiency and responsiveness over time. \\begin{figure}[!ht] \\begin{center} \\subfloat[Female $F$, Male $M$, and tightness ($\\frac{F}{M}$)]{\\includegraphics[width = 0.33\\textwidth]{figuretable/matching_function_project/country_month_male_female_count.png}} \\subfloat[Engagement $E$]{\\includegraphics[width = 0.33\\textwidth]{figuretable/matching_function_project/country_month_match_count.png}} \\subfloat[Patner Finding Rate ($\\frac{E}{F}$,$\\frac{E}{M}$)]{\\includegraphics[width = 0.33\\textwidth]{figuretable/matching_function_project/country_month_male_female_partner_finding_rate.png}} \\caption{Trends of key variables 2014-2025} \\label{fg:country_month_male_female_partner_finding_rate} \\end{center} \\footnotesize %Note: \\end{figure} Panels (a) and (b) in Figure \\ref{fg:country_month_male_female_partner_finding_rate} display trends in the number of active users and engagements on the IBJ platform from 2014 to 2025. Both female $ F $ and male $ M $ participation rise steadily, with male user growth outpacing that of females—particularly after 2020—leading to a gradual decline in market tightness, measured as the gender balance ratio $ \\frac{F}{M} $."}
{"input": "number of engagements $ E $ remains relatively flat until around 2020, after which it increases sharply. This acceleration likely reflects improvements in platform performance, such as better algorithmic matching, increased user activity, or broader social adoption of digital matching services. Panel (c) illustrates the partner finding rates for female and male users per month, measured respectively as $ \\frac{E}{F} $ and $ \\frac{E}{M} $. Both rates increase over time, with females consistently achieving higher matching rates. This suggests improved match formation efficiency on the platform, particularly for female users, despite the growing supply of male participants. The male rate displays greater month-to-month volatility, pointing to more sensitivity to changes in market conditions."}
{"input": "\\section{Empirical Framework} This paper conceptualizes the marriage market as a two-sided search environment, where female users seek partners and male users are potential partners.\\footnote{While an influential strand of the literature models marriage markets as stable, frictionless matching environments and estimates preference-based matching patterns using equilibrium assumptions (e.g., \\citealp{chiappori2023mating}), this study takes a different empirical approach. Rather than recovering structural preferences under the assumption of stable matching, I focus on the empirical measurement of platform-level matching efficiency and elasticity over time. Our interest lies in understanding how the IBJ platform transforms search input into actual matches.} A successful match occurs when both parties agree to engage. Drawing on the matching function framework from labor economics, I estimate the number of engagements as a function of efficiency input and market tightness captured by the gender balance ratio. Let $ E_t $ denote the number of engagements at time $ t $, $ F_t $ and $ M_t $ the number of active female and male users, and $ A_t $ a time-varying matching efficiency parameter. Let $(A, F, M)$ denote random variables corresponding with realizations subscripted by time $t$. Let assume that $M$ and $A$ are independent conditional on $F$, that is $ M\\mathop{\\perp} A| F$. I assume a matching function of the form $ E_t = m(A_t F_t, M_t) $ with constant returns to scale."}
{"input": "\\section{Results} Using nonparametric methods proposed by \\cite{lange2020beyond}, I estimate time-varying matching efficiency $ A_t $ and elasticities with respect to effective female search input $ A_t F_t $ and male users $ M_t $.\\footnote{Details are shown in Appendix. Finite sample performance via Monte Carlo simulations is also shown in \\cite{otani2024nonparametric}.} \\subsection{Matching efficiency and elasticity in the marriage platform} \\begin{figure}[!ht] \\begin{center} \\subfloat[Matching Efficiency ($A$)]{\\includegraphics[width = 0.45\\textwidth]{figuretable/matching_function_project/country_month_efficiency.png}} \\subfloat[Matching Elasticity ($\\frac{d\\ln m}{d \\ln AF}$, $\\frac{d\\ln m}{d\\ln M}$)]{\\includegraphics[width = 0.45\\textwidth]{figuretable/matching_function_project/country_month_elasticity.png}} \\caption{IBJ platform 2014-2025} \\label{fg:matching_efficiency_month_aggregate} \\end{center} \\footnotesize Note: I normalize matching efficiency in January 2014 to 100. \\end{figure} In Figure \\ref{fg:matching_efficiency_month_aggregate}, matching efficiency increases more than fourfold from baseline levels in 2014, with particularly rapid acceleration beginning around 2017 and continuing through the post-2020 period. Notably, the efficiency nearly triples between 2017 and 2019. This sharp improvement may be partly attributable to structural changes within the platform, including the consolidation of other major marriage agencies into the IBJ network. For example, the integration of ZWEI—another nationwide marriage consultation service—into the IBJ Group during this period likely expanded the effective user base and improved cross-platform coordination. Such developments may have enhanced the breadth of matching opportunities and the sophistication of recommendation mechanisms, thereby contributing to substantial gains in match formation efficiency. Elasticity with respect to female-side input ranges from 0.5 to 1.2, while male-side elasticity frequently exceeds 1.0, indicating that the platform has become increasingly responsive to shifts in both sides of the market. \\begin{figure}[!ht] \\begin{center} \\subfloat[Male ($M$)]{\\includegraphics[width = 0.45\\textwidth]{figuretable/matching_function_project/region_month_male_count.png}} \\subfloat[Female ($F$)]{\\includegraphics[width = 0.45\\textwidth]{figuretable/matching_function_project/region_month_female_count."}
{"input": "\\section{Online Appendix (Not for publication)} \\subsection{Estimation details} I begin by estimating the distribution function $ F(A_0 \\mid F) $, following the identification strategy in \\cite{lange2020beyond}. The logic hinges on the conditional distribution of engagements $ E $, given female and male user counts $ (F, M) $. Formally: \\[ F(A_0|\\psi F_0) = G_{E|F,M}(\\psi E_0|\\psi F_0, \\psi M_0), \\] \\[ F(\\psi A_0|\\lambda F_0) = G_{E|F,M}(\\psi E_0|\\lambda F_0, \\psi M_0), \\] where $ \\psi $ is an arbitrary scalar, and $ \\lambda $ is a scaling factor. By varying $ (\\psi, \\lambda) $, I trace out $ F(A \\mid F) $ over the support of $ (A, F) $. In practice, with finite data, I construct a nonparametric estimator of $ G_{E|F,M} $. For any evaluation point $ (E_\\tau, F_\\tau, M_\\tau) $, I compute the proportion of observations with fewer engagements than $ E_\\tau $ among those located near $ (F_\\tau, M_\\tau) $ in the $ (F, M) $-space. Kernel weights discount the influence of distant points. The estimator is written as: \\[ F(\\psi A_0 | \\lambda F_0) = G_{E|F,M}(\\psi E_0 | \\lambda F_0, \\psi M_0), \\] \\[ \\hat{F}(\\psi A_0 | \\lambda F_0) = \\sum 1(E_t < \\psi E_0) \\cdot \\kappa(F_t, M_t; \\lambda F_0, \\psi M_0), \\] where $ \\kappa(\\cdot) $ is a bivariate normal kernel function with bandwidth set to 0.01."}
{"input": "A_t = F^{-1}(G(E_t | F_t, M_t) \\mid F_t). \\] Using the recovered values of $ A_t $, I then invert the matching function: \\[ m(A_t F_t, M_t) = G^{-1}(F(A_t \\mid F_t) \\mid F_t). \\] Finally, I compute local elasticities of the matching function by regressing engagements $ E $ on female and male user counts, interacted with the estimated effective search input $ AF $. This allows us to estimate the marginal responsiveness of engagements to changes in female and male participation: \\[ \\text{Elasticity w.r.t. } F: \\quad \\frac{d \\log m(AF, M)}{d \\log F} = \\frac{d \\log m(AF, M)}{d \\log AF}, \\] which is obtained by estimating the derivative of the matching function with respect to $ AF $, multiplied by the ratio $ \\frac{AF}{E} $, using the regression coefficient from the empirical specification. \\subsection{Mobility across areas} Figure \\ref{fg:year_month_region_match_same_region_prop_island} documents the share of matches formed between users residing in the same geographic area (“within-area matches”) over time. Panel (a) shows trends for major regions on the Japanese mainland (Honshu), while Panel (b) displays patterns for island regions. Across all regions, the majority of engagements occur between users residing in the same region, suggesting limited cross-regional mobility in the partner search process. Mainland regions such as Kanto and Kansai consistently exhibit within-region matching rates above 70–80\\%, while smaller or less dense areas like Chugoku and Tohoku display greater fluctuation, especially in the earlier years of the sample."}
{"input": "a larger share of cross-regional matches, particularly in the early years, potentially reflecting more constrained local partner pools. Over time, the within-region matching rates for island areas also tend to converge toward mainland levels, suggesting an increase in local matching capacity or more geographically filtered search behavior. These patterns indicate that despite the digital nature of the platform, physical geography continues to play a substantial role in shaping match outcomes. \\begin{figure}[!ht] \\begin{center} \\subfloat[Main Land (Honshu) areas]{\\includegraphics[width = 0.45\\textwidth]{figuretable/matching_function_project/year_month_region_match_same_region_prop_mainland.png}} \\subfloat[Island areas]{\\includegraphics[width = 0.45\\textwidth]{figuretable/matching_function_project/year_month_region_match_same_region_prop_island.png}} \\caption{Within-area Marriage Share} \\label{fg:year_month_region_match_same_region_prop_island} \\end{center} \\footnotesize %Note: For confidentiality reasons, I normalize the efficiency to 2019 December for each platform."}
{"input": "a fundamental theory~\\cite{Bousso:2022hlz,Bousso:2023sya}. Other critical aspects of bulk reconstruction, though well understood in AdS, have not yet been studied in arbitrary spacetimes. One important concept is that of the simple (or outermost) wedge of a boundary region $B$. The simple wedge $z(B)$ is the homology region between $B$ and the quantum extermal surface closest to $B$ (and hence, from the bulk point of view, outermost). The simple wedge is known to be unique, to contain the causal wedge of $B$, and to be contained inside the entanglement wedge. Because the causal wedge can be smaller, the simple wedge contains some information whose reconstruction is not trivial. But it does not contain all reconstructible information, because the entanglement wedge can be larger. The complement of the simple wedge in the entanglement wedge is called the ``Python's lunch.'' There is compelling evidence that the reconstruction of operators in the Python's lunch is computationally hard. It requires an exponential (in certain geometric quantities) number of simple logical operations~\\cite{Brown:2019rox}. By contrast, semiclassical operators in the simple wedge can be reconstructed from CFT data with polynomial resources~\\cite{Engelhardt:2021mue}; hence the name. \\begin{figure}[h] \\centering \\includegraphics[width=16cm]{./omaxImages/adszigzag.pdf} \\vspace{-.8cm} \\caption{To start with a familiar setting, each Penrose diagram shows a simple wedge $z(a)$ (shaded red) in AdS. The bulk input region $a$ is shaded grey. The symbols $\\wedge$ etc. indicate the null directions in which areas are shrinking~\\cite{Bousso:1999cb}. In the top figures, the simple and entanglement wedges agree; the bottom example contains a Python's Lunch. \\textit{Top left:} Vacuum Schwarzschild-AdS."}
{"input": "properties that it is outermost and accessible. In general spacetimes, we do not know the analogue of the CFT, the fundamental theory dual to the semiclassical bulk description. Yet, holograms obey surprising properties such as strong subadditivity of the generalized entropy, which suggest that they capture aspects of states in a true quantum gravity theory. The construction and properties of simple wedges that we establish here suggest that the holographic dictionary will be similar to that of AdS/CFT in even more respects. We hope that this will further constrain and aid our search for the fundamental description of our own universe. It will be interesting to study whether the interpretation of the simple wedge in terms of computational complexity can be corroborated in general spacetimes. In particular, it would be nice to understand the relation between our zigzag construction and that of Ref.~\\cite{Engelhardt:2021mue}. The latter provides an explicit simple recovery protocol. Unlike our construction, it changes the spacetime and is currently defined only perturbatively in general settings, so the prescriptions are not equivalent. Turning to the Python's lunch, an important task will be to extend the formula for the exponential reconstruction complexity~\\cite{Brown:2019rox,Engelhardt:2023bpv} to general spacetimes. This will require a definition of the relevant ``bulge'' wedges in arbitrary spacetimes. Moreover, at the fully general level of max- and min-reconstruction~\\cite{Akers:2019wxj}, no ``Python's Lunch'' prescription for computing the complexity of reconstruction beyond the simple wedge is available even in the original AdS/CFT setting, let alone in our general setting."}
{"input": "task. It will also be interesting to explore the implications of our construction for tensor networks~\\cite{Swingle:2009bg}. So far, tensor networks have mostly been viewed as discretized versions of the classical geometry of a time-reflection symmetric Cauchy slice. The zigzag slices we construct always leave the time-reflection symmetric slice even if one exists, and this appears to be essential (see Fig.~\\ref{fig:horse}). This suggests that perhaps tensor networks should be more broadly viewed as models of broken null hypersurfaces. It will be interesting to investigate whether mean curvature flow~\\cite{Nomura:2018kji,Nomura:2018kji} can be used to construct spacelike Cauchy slices that render the simple wedge accessible.\\footnote{We would like to thank Guanda Lin and Pratik Rath for an initial exploration of this question."}
{"input": "\\section{Heuristic Summary and Examples} \\label{heuristic} In this section, we provide a slightly simplified version of the zigzag construction of the simple wedge, and we describe the key properties of the simple wedge. We also provide several instructive examples. We will not provide proofs or overly rigorous definitions; the goal is to offer a first introduction. The reader interested in more rigor is encouraged to study the remaining sections. \\subsection{Classical Simple Wedge in Arbitrary Spacetimes} Recall that a \\emph{lightsheet} is a null hypersurface generated by nonexpanding null geodesics orthogonal to a surface~\\cite{Bousso:1999cb}. A \\emph{wedge} is the full causal development of an open spatial region. A wedge is called \\emph{antinormal} on some portion of its edge if both the past and future outgoing orthogonal null congruences are nonexpanding. For example, the spatial exterior of a round sphere in Minkowski space is an antinormal wedge. (Intuitively, the antinormal property indicates that a wedge holographically encodes at least the infinitesimally nearby regions in the antinormal direction.) We consider a spacetime $M$ that satisfies the classical Einstein equations with matter obeying the Null Energy Condition~\\cite{Wald:1984rg}. The starting point of our construction is an arbitrary ``input wedge'' $a\\subset M$. The role of $a$ is analogous to a boundary subregion $B$ in AdS, in that we will construct a simple wedge pertaining to $a$. \\begin{figure}[h] \\centering \\includegraphics[width=15cm]{./omaxImages/closedz.pdf} \\vspace{-3.4cm} \\caption{Simple wedges in spatially closed universes. The input wedge $a$ is shaded grey, the simple wedge $z(a)$ red, and the max-hologram $\\emax(a)$ green."}
{"input": "as in Fig.~\\ref{fig:adszigzag}, $Z_1$ stops at the first\\footnote{This requires a rigorous definition. Roughly, $Z_1$ must not contain any future lightsheet of $a$ that is everywhere past-expanding. See Def.~\\ref{def:zig} for details.} surface where the past-directed null congruence has vanishing expansion. \\begin{figure}[h] \\centering \\includegraphics[width=16cm]{./omaxImages/horseshoe.pdf} \\vspace{-3.8cm} \\caption{Simple wedge $z(a)$ (red edge) and max-entanglement wedge $\\emax(a)$ (green edge) of a horseshoe-shaped input wedge $a$ in 2+1 dimensional Minkowski space. Left: zigzag construction of $z(a)$. $Z_1$ is a portion of a light cone; it ends on the red dashed line, which is PNC. $Z_2$ is a portion of a null plane that connects this to the extremal red line. No lightsheets begin at the convex (expanding) portions of the horseshoe. Right: Although the horseshoe lies on a time-reflection symmetric Cauchy slice $\\Sigma$, we have not found a viable construction of $z(a)$ purely on $\\Sigma$. In particular, the presence of a lunch is obscured by the existence of a spacelike foliation on $\\Sigma$ (grey lines) with monotonically decreasing area, which interpolates from the red to the green edge.} \\label{fig:horse} \\end{figure} Having constructed the zig, we now iterate, switching future and past at every step. That is, we follow $Z_2$, the past-directed lightsheet of $z_1$, until it fails to be antinormal. See Figures~\\ref{fig:adszigzag} and \\ref{fig:horse} for nontrivial examples. The resulting region (including $z_1$) is called the \\emph{zag} of $z_1$ and is labeled $z_2$. Continuing onward, alternating between zigs and zags, we construct ever growing regions, $z_3, z_4, \\ldots$. We call $z_n$ the $n$-th \\emph{zigzag} of $a$."}
{"input": "construction equips $z_n$ (really, just the portion exterior to $a$) with a preferred Cauchy slice, $Z_1\\cup Z_2 \\cup \\ldots \\cup Z_n$. The \\emph{simple wedge} $z$ is the $n\\to\\infty$ limit of the zigzag. Our Corollary~\\ref{cor:zprop} reduces, in the classical setting, to the proof that $z$ is a stationary surface for the area functional\\footnote{In a Lorentzian geometry, no surface has locally minimal or maximal area, so none is extremal. Nevertheless we will use ``extremal'' instead of ``stationary'' below, since this terminology is widely used.} (i.e., has vanishing past and future expansions) where its edge differs from that of $a$. Theorem~\\ref{zthm:zigoutermost} reduces to the proof that $z$ is contained inside all other wedges with this property. These two results establish that $z$ is properly called the simple or outermost wedge. \\subsection{Quantum Extensions} The Quantum Extremal Surface (QES) prescription~\\cite{Engelhardt:2014gca} replaces the area by the generalized entropy $\\S$ (the Bekenstein-Hawking entropy of the edge plus the von Neumann entropy of the matter fields in the wedge). This evades the need for assuming the Null Energy Condition (which is false in nature), and it is vital for the correct treatment of semiclassical phenomena such as black hole evaporation. The improvement is analogous to replacing Hawking's area theorem~\\cite{Hawking:1971vc} by the Generalized Second Law~\\cite{Bekenstein:1972tm}, or the classical focusing of lightrays by the Quantum Focusing Conjecture~\\cite{Bousso:2015mna}. To implement this substitution, our definition of the simple wedge follows exactly the same steps as outlined above, with the classical expansion replaced by the quantum expansion (a functional derivative of $\\S$)."}
{"input": "this is the case shown here. After the Page time, $\\emax(a)=z(a)$.} \\label{fig:island} \\end{figure} Full generality also requires us to allow regions bounded by corners or folds. Corners arise generically at caustics of null congruences, and when considering unions or intersections of regions.\\footnote{To see the importance of this level of generality, consider the smooth ($C^\\infty$) horseshoe-shaped input region in Fig.~\\ref{fig:horse}. Its simple wedge has corners: the null normal vectors are discontinuous. Its entanglement wedge has no corners but discontinuous null expansions.} This leads to a minimal, more robust structure, in which quantitative null expansions are abandoned. Nonexpansion and noncontraction become qualitative properties, and the Quantum Focusing Conjecture~\\cite{Bousso:2015mna} is replaced by a pared-down version, Discrete Max Focusing~\\cite{Bousso:2024iry}. Discrete nonexpansion still allows for key definitions, such as the concept of antinormal; and Discrete Max Focusing suffices in proofs. Working at this level of generality, the appropriate generalization of the notion of quantum extremal surface is the statement that the max-hologram $\\emax(a)$ is a ``throat accessible from $a$.'' A throat is an antinormal wedge that cannot be enlarged while staying antinormal. Accessibility from $a$ relaxes the antinormal requirement to apply only to edge portions of the throat that differ from the edge of $a$. But accessibility adds a new condition: that the portion of the throat wedge outside of $a$ admits a partial Cauchy slice such that the generalized max entropy of the throat conditioned on any intermediate wedge whose edge lies on the slice is negative."}
{"input": "all intermediate surfaces on the Cauchy slice have larger area than the throat.) Our definition of the simple wedge $z$ is fully general in that it incorporates all of the above refinements and generalizations. This requires the apparatus of definitions and lemmas developed in the following section. %We must first prove Theorem~\\ref{zthm:accessible} that $z$ is itself a throat accessible from $a$, before proving the ``outermost'' property that $z$ is contained in any other throat accessible from $a$. Without the first proof, $z$ would not necessarily be a subset of $\\emax(a)$. We find that $z$ is accessible from $a$ via the zigzag Cauchy surface $Z\\equiv Z_1\\cup Z_2 \\cup \\ldots$ furnished by our construction."}
{"input": "\\section{Preliminary Definitions and Lemmas} \\label{definitions} Here we reproduce a number of standard definitions, fix notation, and derive some Lemmas. \\subsection{Wedges and Causal Structure} \\label{sec:wedges} Let $M$ be a globally hyperbolic Lorentzian spacetime with metric $g$. The chronological and causal future and past, $I^\\pm$ and $J^\\pm$, and the unphysical spacetime $M\\cup\\partial M$ with conformal boundary $\\partial M$ are defined as in Wald~\\cite{Wald:1984rg}. A proper subset will be denoted by $\\subsetneq$; $\\subset$ permits equality. For $s\\subset M$, $\\setint s$, $\\cl s$, and $\\partial s$ denote the interior, the closure, and the boundary of $s$. All operations are performed in $M$ unless explicitly stated otherwise; in those cases we will denote the relevant set by a subscript. \\begin{defn}\\label{zdef:sc} The \\emph{spacelike complement} of a set $s\\subset M$ is \\begin{equation}\\label{zeq:sc} % s'\\equiv \\setint [M\\setminus I(s)]~. s'=M\\setminus \\cl[I(s)]~. \\end{equation} (Thus, $s'$ is necessarily open.) \\end{defn} \\begin{defn}\\label{zdef:covwedge} A {\\em wedge} is a set $a\\subset M$ that satisfies $a=a''$. (Thus $a$ is an open set; $a'$ is a wedge; and the intersection of two wedges $a,b$ is a wedge~\\cite{Bousso:2022hlz, Bousso:2023sya}.) \\end{defn} \\begin{defn}\\label{zdef:wedgeunion} The {\\em wedge union} of two wedges $a,b$ is the wedge \\begin{equation} a\\Cup b\\equiv (a'\\cap b')'~. \\end{equation} \\end{defn} \\begin{defn}\\label{zdef:edgehor} The \\emph{edge} $\\eth a$ and \\emph{Cauchy horizons} $H^\\pm(a)$ of a wedge $a$ are \\begin{align} \\eth a & \\equiv \\partial a \\setminus I(a)~,\\\\ H^+(a) & \\equiv \\partial a\\cap I^+(a)~,\\\\ H^-(a) & \\equiv \\partial a\\cap I^-(a)~,\\\\ H(a) & \\equiv \\partial a\\cap I(a) = H^+(a)\\cup H^-(a)~."}
{"input": "to be \\emph{antinormal} at $p\\in\\eth a$ if $a$ is FNE and PNE at $p$. \\end{defn} See Figure~\\ref{fig:ads} and Appendix~\\ref{app:examples} for an illustration of nonexpansion and of other concepts introduced below. When a numerical outward future quantum expansion is well-defined, FNE implies its nonpositivity; moreover, negative quantum expansion implies FNE~\\cite{Bousso:2024iry}. However, we shall not require numerical values here, and the above definition is superior because it can be applied to non-smooth edges (which arise generically). \\begin{figure}[H] %\\centering \\vspace{-1cm} \\hspace{-1cm} \\includegraphics[width=18cm]{./omaxImages/ads.pdf} \\vspace{-1cm} \\caption{Illustration of the notions of noncontraction, nonexpansion, and accessibility using examples in Schwarzschild-AdS. In the cases where the blue wedge is antinormal (i.e., FNE and PNE), it is accessible from the purple wedge (clockwise from top left: accessible; past-marginally accessible; throat accessible; future-marginally accessible).} \\label{fig:ads} \\end{figure} \\begin{defn}[Accessibility] Given a wedge $a$, the wedge $k\\supset a$ is said to be \\emph{accessible from $a$} if it satisfies the following conditions: \\begin{enumerate}[I.] \\item $a\\subset f\\subset \\tilde a '$, where $\\tilde a$ is the fundamental complement of $a$ (see Def.~\\ref{def:tildea}); \\item $k$ is antinormal at points $p\\in \\eth f\\setminus\\eth a$; \\item $k$ admits a Cauchy slice $\\Sigma$ such that $\\Sigma\\supset \\eth a$ and such that for any wedge $h\\subsetneq k$ with $a\\subset h$, $\\eth h\\subset \\Sigma$, and $\\eth h\\setminus \\eth k$ compact in $M$,\\footnote{Some references require a strict inequality~\\cite{Bousso:2023sya,Akers:2023fqr}, but our choice will be convenient in proofs. The distinction is not meaningful given the smoothing inherent in the definition of the conditional max entropy $\\hmg^\\epsilon$.} \\begin{equation} \\hmg(k|h)\\leq 0~."}
{"input": "the statement that the area of any intermediate surface $\\eth h$ suffices as an entanglement resource for performing quantum state merging from $k$ to $h$~\\cite{Akers:2019wxj}. \\end{defn} \\begin{defn}[Max-Hologram]\\label{def:emax} Given a wedge $a$, its \\emph{max-hologram} (or \\emph{generalized max-entanglement wedge}), $\\emax(a)$, is the wedge union of all wedges that are accessible from $a$~\\cite{Bousso:2023sya}. \\end{defn} \\begin{defn}[Lightsheets] Let \\emph{$\\eth a^+$} be the set of points where the wedge $a$ is FNE. A null hypersurface \\emph{$L^+(a)$} $\\subset H^+(a')$ whose past boundary lies on $\\eth a^+$ is called a \\emph{future lightsheet} of $a$~\\cite{Bousso:1999xy,Bousso:1999cb}. The outward deformation of $a$ along the future lightsheet $L^+(a)$, $a\\Cup L^+(a)$, is called a \\emph{future lightsheet wedge} of $a$. Past lightsheets and past lightsheet wedges are defined analogously. \\end{defn} The following conjecture is a minimal, discrete version~\\cite{Shahbazi-Moghaddam:2022hbw,Bousso:2024iry} of the Quantum Focussing Conjecture~\\cite{Bousso:2015mna,Bousso:2015wca} (which in turn becomes the focusing theorem of General Relativity in the classical limit): \\begin{conj}[Discrete Max-Focusing]\\label{conj:qfc} Let $b\\subset c$ both be future lightsheet wedges, or both be past lightsheet wedges, of $a$. Then \\begin{equation}\\label{eq:qfc} \\hmg(c|b)\\leq 0~. \\end{equation} An immediate consequence is the ``persistence of nonexpansion'': for any future lightsheet wedge $b$ of $a$ \\begin{equation} \\label{eq:persistence} \\eth b \\cap J^+(\\eth a^+) \\subset \\eth b^+~. \\end{equation} \\end{conj} \\subsection{Discrete Noncontraction, Marginal Wedges, and Throats} \\begin{defn}[Noncontracting Wedges] A wedge $a$ is said to be \\emph{future-noncontracting} (FNC) if there exists an open set $O\\supset\\eth a$ such that no proper past-directed outward null deformation of $a$ with compact support within $O$ is FNE on all new edge points."}
{"input": "\\eth a \\cup [H^-(a')\\cap O]$ is FNE on all points in $\\eth b\\setminus\\eth a$. (Heuristically, in sufficiently smooth settings, FNC implies nonnegativity of the outward future quantum expansion of $a$; moreover, positivity of the quantum expansion implies FNC.) \\emph{Past-noncontracting} (PNC) is defined analogously. \\end{defn} \\begin{lem} \\label{zlem:intersectFNC} The intersection of two FNC (PNC) wedges is FNC (PNC). \\end{lem} \\begin{proof} Let $a$ and $b$ be FNC wedges, and suppose for contradiction that $c\\equiv a\\cap b$ is not FNC. Then for any open set $O_c\\supset\\eth c$, there exists a past deformation $d\\supsetneq c$, with $\\eth d \\subset\\eth c\\cup [H^-(c')\\cap O_c]$, such that $d$ is everywhere FNE on $\\eth d\\setminus\\eth c$. Let $f=d\\Cup a$. We may assume that $f\\neq a$; otherwise exchange the names of $a$ and $b$. Since $d$ is everywhere FNE on $\\eth d\\setminus\\eth c$, Discrete Max-Focusing and Discrete Subadditivity imply that $f$ is everywhere FNE on $\\eth f\\setminus\\eth a$. Moreover, we can construct such a deformation in any open neighborhood $O_a$ of $a$ by choosing $O_c\\subset a\\Cup O_a$. This shows that $a$ is not FNC, in conflict with the assumption of the Lemma. Hence $c$ must be FNC. The time-reversed argument shows that $a\\cap b$ is PNC for PNC wedges $a$ and $b$. \\end{proof} % We shall extend the notions of \\emph{FNE, PNE, antinormal, FNC, PNC, future-marginal, past-marginal}, and \\emph{throat} to any set whose double complement satisfies these conditions. For example, a partial Cauchy slice $\\Sigma$ is said to be PNC if $\\Sigma''$ is PNC."}
{"input": "\\section{Zigzag and the Simple Wedge} \\label{full} \\subsection{Simple Wedge in General Spacetimes} \\label{sec:general} \\begin{defn}[Zig and zag]\\label{def:zig} Given a wedge $a$, let $Q(a)$ be the set of future lightsheet wedges $l^+(a)$ that satisfy \\begin{enumerate}[A.] \\item $l^+(a)$ is PNE---and hence antinormal, by Eq.~\\eqref{eq:persistence}---on $\\eth l^+(a)\\setminus \\eth a$; \\item $l^+(a)$ contains no PNC future lightsheet wedge of $a$ as a proper subset; \\item $l^+(a)\\subset\\tilde a'$\\footnote{Property C ``almost'' follows from the rest of the definition, in the sense that it could be eliminated with a weak ``generic'' assumption analogous to an assumption that prevents null congruences from having vanishing expansion over finite affine length in classical General Relativity.}. \\end{enumerate} We define the \\emph{zig} of $a$, $z^+(a)$ as their wedge union: \\begin{equation} z^+(a) = \\Cup_{l^+(a)\\in Q(a)} ~l^+(a)~. \\end{equation} The \\emph{zag} $z^-(a)$ is similarly defined in terms of past lightsheet wedges of $a$ that contain no FNC past lightsheet wedge of $a$ as a proper subwedge. \\end{defn} Next, we establish two important properties of the zig. \\begin{lem} $z^+(a)\\in Q(a)$. \\end{lem} \\begin{proof} Property A: By Corollary 48 of Ref.~\\cite{Bousso:2024iry}, $z^+(a)$, too, is antinormal on $\\eth z^+(a)\\setminus \\eth a$. Property B: Suppose $z^+(a)$ contained a proper PNC subwedge $w$ that was a future lightsheet wedge of $a$. Therefore $Q(a)$ contains an element $\\hat l(a)$ that is not contained in $w$ (or else $w$ would not be a proper subwedge of $z^+(a)$. The union of two light-sheet wedges is obviously itself one: $v\\equiv w\\Cup \\hat l(a)\\in Q(a)$; yet $v$ contains the PNC wedge $w$ as a proper subwedge."}
{"input": "all properties of the finite-$n$ zigzags. It is accessible from $a$; and it is both PNC and FNC: otherwise, it could be enlarged by either another zig or another zag. We summarize these properties in a third corollary of Theorem~\\ref{zthm:accessible}: \\begin{cor}\\label{cor:zprop} The simple wedge satisfies the following properties: \\begin{itemize} \\item $z(a)$ is a throat accessible from $a$ via the Cauchy slice $Z(a)$; \\item in particular, $z(a)$ is antinormal on $\\eth z(a)\\setminus\\eth a$, and $z(a)\\subset \\emax(a)$. \\end{itemize} \\end{cor} Moreover, $z(a)$ shares a defining characteristic of the simple (or outermost) wedge in AdS/CFT: \\begin{thm}\\label{zthm:zigoutermost} $z(a)$ is contained in any other throat accessible from $a$. \\end{thm} \\begin{proof} Suppose for contradiction that $k\\supset a$ is a throat but $z(a)\\not\\subset k$. Then $H(k)\\cap z(a)$ is not empty. Notice $\\eth k\\not\\subset Z(a)$, otherwise we have an immediate contradiction with the definition of the zigzag. Let $N$ be the smallest $i$ such that $Z_i(a)$ intersects $H(k)$ and such that $z_i(a)\\not\\subset k$. For odd (even) $N$, $z_N(a)$ is PNC (FNC). By Lemma~\\ref{zlem:intersectFNC}, for $N$ odd (even) \\begin{equation} j\\equiv k\\cap z_N(a) \\end{equation} defines a PNC (FNC) proper subwedge of $z_N(a)$. This contradicts the definition of the zigzag. \\end{proof} Our definition of the zigzag involved an arbitrary choice: starting from $a$, we first constructed a zig, then a zag, and so on. One could also have defined a ``zagzig'' by starting with a zag instead."}
{"input": "\\section{Further examples of FNE, PNE, FNC, PNC, and marginal wedges} \\label{app:examples} In this appendix, we present further examples that illustrate the various types of discrete expansions in commonly considered spacetimes. This complements a first set of examples given in Fig.~\\ref{fig:ads}. \\begin{figure}[H] \\centering \\vspace{.2cm} \\includegraphics[width=16cm]{./omaxImages/flat.pdf} \\vspace{-.7cm} \\caption{Classical black hole formed from collapse in asymptotically flat space. Again various examples of wedges are shown, with some being accessible from a purple subwedge. (Past-marginally accessible in the middle top figure; throat accessible in the bottom right wedge.)} \\label{fig:flat} \\end{figure} \\begin{figure}[H] \\centering \\vspace{-1.5cm} \\includegraphics[width=14.5cm]{./omaxImages/ds.pdf} \\vspace{-1.5cm} \\caption{Vacuum de Sitter space. Except for the top left example, none of the blue wedges are accessible from any spherical proper subwedge.} \\label{fig:ds} \\end{figure} \\begin{figure}[H] \\centering \\vspace{-.5cm} \\includegraphics[width=14.5cm]{./omaxImages/flrw.pdf} \\vspace{-.3cm} \\caption{Spatially closed FLRW universe dominated by pressureless dust. As in vacuum de Sitter, except for the top left example, none of the blue wedges are accessible from any proper subwedge.} \\label{fig:flrw} \\end{figure} \\subsection*{Acknowledgements} We would like to thank C.~Akers, N.~Engelhardt, G.~Lin, P.~Rath, A.~Shahbazi-Moghaddam, J.~Yeh, and especially S.~Kaya and G.~Penington for discussions. This work was supported by the Department of Energy, Office of Science, Office of High Energy Physics under QuantISED Award DE-SC0019380."}
{"input": "\\section{Introduction} Today, the ``Hierarchy Problem'' is the question in particle physics about which there is the least consensus. There are many conceptions of this problem and the related question of the ``naturalness'' of the parameters of the Standard Model. I doubt that --- absent any illuminating experimental discovery --- a new theoretical paper could meet the stated goal of this special issue of ``clarifying common misconceptions''. Still, I am glad to have this opportunity to state my personal opinions on what the Hierarchy Problem really asks and offer my own route to a solution. As to whether I am dissolving confusion and simply adding to it, I leave to the reader to judge. The approach to the Hierarchy Problem in this paper is intentionally narrow. In particular, I will only discuss ``the'' Hierarchy Problem, the problem of the value of the Higgs mass term $\\mu^2$ in the Standard Model. For those who seek a comprehensive review of the Hierarchy Problem, including discussions of the Cosmological Constant Problem, I strongly recommend the paper written by Nathaniel Craig for the Snowmass 2021 study, which attempts a complete survey of the literature~\\cite{Craig:2022eqo}."}
{"input": "\\section{The Wilsonian point of view} Ken Wilson is often credited with the original statement requiring a natural explanation for the Higgs mass parameter, through his comment: ``It is interesting to note that there are no weakly coupled scalar particles in nature; scalar particles are the only kind of free particles whose mass term does not break either an internal or a gauge symmetry.''~\\cite{Wilson:1970ag}. However, Wilson's ideas run through this problem at a deeper level than this. At a time when particle theorists believed that hadron were fundamental and rejected equations with pointlike couplings and UV divergences, Wilson held to the belief that quantum field theories were not different from ordinary quantum-mechanical systems. He analyzed them by taking the UV cutoff seriously as the largest energy scale and working downward, rather than considering the UV cutoff scale as an artifact to be removed by appropriate incantations~\\cite{Wilson:1965zzb}. This led him to consider quantum field theories as related to models of phase transitions in condensed mater and, eventually, to important discoveries about those systems. These discoveries doubled back to particle physics in the concept of Wilsonian effective field theories, which now provide a basic language for our discussions of particle physics. As a student of Wilson, I am thoroughly infected with that philosophy, for better or worse. Spontaneous breaking of symmetry is found in a great variety of condensed matter systems --- magnets, superconductors, liquid crystals, and more."}
{"input": "different way. Condensed matter systems are made of atoms, and the laws of atomic physics have been fixed since discovery of quantum mechanics. This means that the explanations for these examples of spontaneous symmetry breaking must be derivable from those laws. This makes the study of phase transitions in condensed matter a fascinating subject, with many surprising mechanisms leading to the observed nontrivial ground states~\\cite{ChaikinLubensky}. Thinking in this way get us away from simplifying the hierarchy program to the explanation of a large ratio of mass scales. Then, {\\it the real goal of our pursuit of the origin of $\\mu^2$ should be to find a compelling physical mechanism that explains the origin of EWSB.} This mechanism might be one already known in condensed matter physics, or it might be one that is completely new. In any case, that physics is not present in the SM, a theory that is completely weak-coupling at short distances. {\\it This point of view requires that there must be new fundamental interactions working at shorter distances than the ones that we have probed so far.} That gives an important goal for further exploration in high-energy physics. An example of a physical mechanism leading to spontaneous symmetry breaking in particle physics is the explanation of chiral symmetry breaking in the strong interactions."}
{"input": "many years in the future. In 1961, Nambu and Jona-Lasinio proposed that the chiral symmetry of the strong interactions could be spontaneously broken by same mechanism that causes superconductivity -- fermion pair condensation~\\cite{Nambu:1961tp}. Throughout the 1960's, this idea and the related idea of the quark model were considered to be hopelessly naive. Progress was made through the language of ``current algebra''~\\cite{Gell-Mann:1964hhf} in papers that are very difficult to read today. Finally, these physically transparent ideas found their place within the theory of QCD with asymptotic freedom. My goal for the hierarchy problem is similar. Can we find a cogent explanation for EWSB based on an explicit physical model? This does not need to be an ultimate explanation or one correct all the way to the Planck scale. We can build our model of fundamental physics step by step. This is the step that we need to take now. Different models of EWSB lead to different models of the Higgs boson, for example, as a member of a supersymmetry multiplet, as member of a global symmetry multiplet, as a composite particle. Each hypothesis leads to distinct theories of the most important issues in elementary particle physics -- the origin of the hierarchy of fermion masses and mixings, the origin of the baryon asymmetry in nature, the origin of neutrino masses. Many hypotheses for the problem of the origin of dark matter also depend of this choice."}
{"input": "of the top quark Yukawa coupling and some convenient group theory factors, it is the Higgs doublet scalars for which this effect is largest, leading to the exactly the pattern of EWSB found in the SM~\\cite{Inoue:1982pi,Ellis:1982xz,Alvarez-Gaume:1983drc}. This mechanism is highlighted in my 2006 TASI lectures on supersymmetry~\\cite{Peskin:2008nw}. If we give up the idea that the solution to EWSB must also provide a fundamental theory up to very high energies, then there are many models that use the largeness of the top quark Yukawa coupling to drive other mechanisms for EWSB. In 1984, Kaplan and Georgi introduced the idea that the Higgs boson could be a composite particle bound by a new set of strong interactions~\\cite{Kaplan:1983fs}. The Higgs boson could be light compared to the symmetry-breaking scale of the strong interaction theory if it were a Goldstone boson of a dynamically broken global symmetry of that theory. The Higgs boson mass term would be supplied by radiative corrections induced by terms in the theory that did not respect that global symmetry. Later, this mechanism would be explicitly realized (and some difficulties of the Kaplan-Georgi model solved) in Little Higgs theories~\\cite{Arkani-Hamed:2002ikv, Schmaltz:2005ky}. In these models, a vector-like fermion top quark partner would cancel the quadratic divergences of the top quark loop correction to $\\mu^2$, leaving over a naturally negative contribution. A similar mechanism was found to work in models with an extra space dimension."}
{"input": "fields in the higher dimension. This identification leads to the cancellation of quadratic divergences. The contribution to the Higgs mass term from the top quark is again negative, but the contributions from the Kaluza-Klein resonances of the 5-dimensional top quark field are positive and cut off the ultraviolet divergence. Here also, radiative corrections due to the top quark Yukawa coupling lead to a naturally negative value of $\\mu^2$ ~\\cite{Contino:2003ve}. In all of these cases, new particles are needed to build a complete model of EWSB --- the supersymmetric partners of the top quark in the case of supersymmetry, vectorlike fermion top quark partners in the cases of nonsupersymmetric models. These particles can be heavier than the Higgs boson, but fine-tuning is needed to push their masses above 1~TeV. In the case of supersymmetry, where the entire theory is described by weak-coupling interactions, a rather sophisticated literature on fine-tuning developed, suggesting quite strong upper limits on the masses of these particles~\\cite{Barbieri:1987fn,Feng:2013pwa}. In the non-supersymmetric cases, the limits are weaker but still in the region of 1-3~TeV. So far, none of these new particles has been discovered at the LHC. Especially because of the stringent expectations for supersymmetry, most members of our community have come to the opinion that these particles do not exist. Still, especially for the vectorlike fermions, there is opportunity to extend the searches and discover these particles at the HL-LHC. I hope that LHC experimenters will take this opportunity seriously."}
{"input": "particles of chiral gauge theories. Maybe this is a place that we will find clues to these questions. It would be wonderful to learn the correct explanation from experiment. But, again, our ignorance of the fundamental nature of the Higgs field and our lack of clues about what stands behind the SM is an impediment here. The third problem is the ``Little Hierarchy Problem''. This is the question of why the masses of new particles needed for a dynamical model of EWSB are so much heavier than the mass of Higgs boson. In the context of a search for the dynamical explanation of EWSB, this is a constraint on the possible answers: They must have some feature that leads to an unanticipated mass gap between the $\\mu^2$ parameter and the masses of the lightest new particles. In my opinion, the Little Hierarchy Problem is the one that offers the best chance for a solution now. Only a small hierarchy is needed. In addition, we need the answer to this question to make progress on either of the previous two hierarchy problems, which cannot even be properly posed without new information about the nature of the new particles to be found at energies above those currently probed by the LHC."}
{"input": "\\section{Solutions to the Little Hierarchy Problem} There are solutions to the Little Hierarchy Problem in the literature, but no one is very impressed with them. In this section, I will discuss three of these. In all three cases, the next set of fundamental interactions beyond the SM has two levels, one immediately generating the Higgs potential and another, at a higher mass scale, being the more fundamental cause of the symmetry-breaking. I will refer to these as the intermediate and the fundamental mass scales. In their current state, these models seem artificial and overly complex. However, they provide starting points. The first idea is to include more fields in the region between the intermediate and fundamental scales in the model. There are many examples of models that use this strategy implicitly. The strategy is made more explicit in work of mine with Yoon, where we call it ``competing forces''~\\cite{Yoon:2017cty}. In the example we discuss, the chiral top quark naturally generates a term in the Higgs potential with a negative $\\mu^2$. In the same model, an additional vector-like fermion generates a term in the Higgs potential with a positive $\\mu^2$. Playing these two effects against one another give a Higgs potential that requires only a modest level of tuning to move new particles out of the range explored by the LHC. The new competing fermion might have its own reason for existence, for example, to generate the cosmic dark matter."}
{"input": "\\section{The Hierarchy Problem and future high-energy colliders} Particle theorists often confine their thinking to their own narrow domain. However, our beliefs about the Hierarchy Problem have important implications for experimental particle physics that I feel cannot be ignored. In the development of the SM, the progress of theory relied strongly on surprises from experiment. The $\\tau$-$\\theta$ puzzle of kaon decays opened the path to an understanding of the weak interactions. The SLAC-MIT deep inelastic scattering experiments and the discovery of the $J/\\psi$ shattered the notion that hadrons were fundamental particles and opened the door to the quark model and QCD. Even after the SM was formulated and began to be tested with precision, the heaviness of the top quark was a new discovery that now informs our ideas about beyond-SM physics. We need new experimental surprises to guide further progress. For the next high energy physics collider, there is a general consensus and a well-developed physics case for an $\\ee$ Higgs factory~\\cite{Dawson:2022zbb}. This will make precision measurements that may point to the next energy scale. But in order to actually reach that scale, we will need a collider at energies substantially higher than those of the LHC. There is much interest now in planning for a 100~TeV proton collider, or, more generally, for a ``10~TeV pCM'' (parton CM energy) collider~\\cite{P5:2023wyd}. The cost of any 10~TeV pCM collider will be in the \\$10 B range."}
{"input": "beyond our current knowledge. For particle physicists, the importance of this goal is obvious. But for our colleagues in other fields of science, and for our government sponsors, this is much less clear. Already, we are hearing from many sources that, with the SM complete, particle physics is finished, and that the funding it requires for a next step is better spent in other areas. Especially at this level of cost, we will be asked what, more precisely, we expect to discover, and what energy is actually needed to achieve that goal. Our physics arguments for a higher energy collider need to be much stronger. Because the SM can be extrapolated to the Planck scale, there is no guaranteed discovery at such a machine (as there was, for example, for the LHC). Still, we can make a strong argument for a 10 TeV pCM collider if we can argue that this collider gives us an {\\it opportunity} to discover and characterize a new fundamental interaction of nature. Do you believe that this is so? The mechanistic view of the Hierarchy Problem that I have presented in this paper leads to a very different answer from models that simply motivate a hierarchy of unknown size from the randomness of nature or from the influence of constraints from gravity or cosmology. In those models, higher energy scales in physics may be very far away, or, even, might not be needed at all below the scale of gravity."}
{"input": "useful comments on the manuscript. This work was supported by the US Department of Energy under contract DE--AC02--76SF00515. I am grateful to Nathaniel Craig and David Gross for a visit to the the Kavli Institute for Theoretical Physics that allowed me to preview these ideas before a skeptical and discerning audience. That visit was supported by National Science Foundation grant NSF PHY-2309135 to the KITP. \\begin{thebibliography}{99} \\bibitem{Craig:2022eqo} N.~Craig, ``Naturalness: Past, Present, and Future,'' Eur. Phys. J. C \\textbf{83}, 825 (2023) %doi:10.1140/epjc/s10052-023-11928-7 [arXiv:2205.05708 [hep-ph]]. \\bibitem{Isidori:2001bm} G.~Isidori, G.~Ridolfi and A.~Strumia, ``On the Metastability of the Standard Model Vacuum,'' Nucl. Phys. B \\textbf{609}, 387-409 (2001) %doi:10.1016/S0550-3213(01)00302-9 [arXiv:hep-ph/0104016 [hep-ph]]. \\bibitem{Degrassi:2012ry} G.~Degrassi, S.~Di Vita, J.~Elias-Miro, J.~R.~Espinosa, G.~F.~Giudice, G.~Isidori and A.~Strumia, ``Higgs Mass and Vacuum Stability in the Standard Model at NNLO,'' JHEP \\textbf{08}, 098 (2012) %doi:10.1007/JHEP08(2012)098 [arXiv:1205.6497 [hep-ph]]. \\bibitem{poletwo} A way to see the presence of these fluctuations within dimensional regularization is to note that they create a nonzero pole in the Higgs field propagator at $d=2$. This contrasts with the situation of the electromagnetic vacuum polarization. In that case, the quadratic divergence is required to vanish by QED gauge invariance. Dimensional regularization respects gauge invariance, and so it not only removes the divergence at $d=4$ but also it removes the pole at $d=2$ so that the QED Ward-Takahashi identity is valid for all $d$. See M. E. Peskin and D. V. Schroeder, {\\it An Introduction to Quantum Field Theory} (CRC Press, 1995), Section 7.5. \\bibitem{Inoue:1982pi} K.~Inoue, A.~Kakuto, H.~Komatsu and S."}
{"input": "(2013) %doi:10.1146/annurev-nucl-102010-130447 [arXiv:1302.6587 [hep-ph]]. \\bibitem{Hook:2023yzd} A.~Hook, ``New Solutions to the Gauge Hierarchy Problem,'' Ann. Rev. Nucl. Part. Sci. \\textbf{73}, no.1, 23-39 (2023) doi:10.1146/annurev-nucl-102422-080830 \\bibitem{Giudice:2017pzm} G.~F.~Giudice, ``The Dawn of the Post-Naturalness Era,'' %doi:10.1142/9789813238053\\_0013 [arXiv:1710.07663 [physics.hist-ph]]. \\bibitem{Fritzsch:1977za} H.~Fritzsch, ``Calculating the Cabibbo Angle,'' Phys. Lett. B \\textbf{70}, 436-440 (1977) % doi:10.1016/0370-2693(77)90408-7 \\bibitem{Yoon:2017cty} J.~Yoon and M.~E.~Peskin, ``Competing Forces in Five-Dimensional Fermion Condensation,'' Phys. Rev. D \\textbf{96}, 115030 (2017) %doi:10.1103/PhysRevD.96.115030 [arXiv:1709.07909 [hep-ph]]; ``Dissection of an $SO(5) \\times U(1)$ Gauge-Higgs Unification Model,'' Phys. Rev. D \\textbf{100}, 015001 (2019) %doi:10.1103/PhysRevD.100.015001 [arXiv:1810.12352 [hep-ph]]. \\bibitem{Perelstein:2003wd} M.~Perelstein, M.~E.~Peskin and A.~Pierce, ``Top Quarks and Electroweak Symmetry Breaking in Little Higgs Models,'' Phys. Rev. D \\textbf{69}, 075002 (2004) %doi:10.1103/PhysRevD.69.075002 [arXiv:hep-ph/0310039 [hep-ph]]. \\bibitem{Fox:2002bu} P.~J.~Fox, A.~E.~Nelson and N.~Weiner, ``Dirac Gaugino Masses and Supersoft Supersymmetry Breaking,'' JHEP \\textbf{08}, 035 (2002) %doi:10.1088/1126-6708/2002/08/035 [arXiv:hep-ph/0206096 [hep-ph]]. \\bibitem{Baer:2015rja} H.~Baer, V.~Barger and M.~Savoy, ``Upper Bounds on Sparticle Masses from Naturalness or How to Disprove Weak Scale Supersymmetry,'' Phys. Rev. D \\textbf{93}, 035016 (2016) %doi:10.1103/PhysRevD.93.035016 [arXiv:1509.02929 [hep-ph]]. \\bibitem{Chakraborty:2018izc} S.~Chakraborty, A.~Martin and T.~S.~Roy, ``Charting Generalized Supersoft Supersymmetry,'' JHEP \\textbf{05}, 176 (2018) %doi:10.1007/JHEP05(2018)176 [arXiv:1802.03411 [hep-ph]]. \\bibitem{Cohen:2020ohi} T.~Cohen, N.~Craig, S.~Koren, M.~Mccullough and J.~Tooby-Smith, ``Supersoft Top Squarks,'' Phys. Rev. Lett. \\textbf{125}, 151801 (2020) %doi:10.1103/PhysRevLett.125.151801 [arXiv:2002.12630 [hep-ph]]. \\bibitem{Dawson:2022zbb} S.~Dawson, \\textit{et al.} ``Report of the Topical Group on Higgs Physics for Snowmass 2021: The Case for Precision Higgs Physics,'' [arXiv:2209.07510 [hep-ph]]. \\bibitem{P5:2023wyd} S.~Asai \\textit{et al.} [P5 Panel], ``Exploring the Quantum Universe: Pathways to Innovation and Discovery in Particle Physics,'' %doi:10.2172/2368847 [arXiv:2407.19176 [hep-ex]]."}
{"input": "\\section{Introduction} \\begin{figure}[!ht] \\centering \\includegraphics[scale=0.79]{images/bm-mae_concept.pdf} \\caption{(a) BM-MAE pre-training on all modalities through the reconstruction of masked patches. The trained encoder can handle any modality combination. (b) Fine-tuning the encoder for subtyping on T1 and FLAIR. (c) Fine-tuning the same encoder for segmentation on T1c.} \\label{fig:intro} \\end{figure} Gliomas are the most common type of primary brain tumors, originating from glial cells and often leading to poor patient prognosis due to their potential rapid growth and infiltration into surrounding brain tissue \\cite{schwartzbaum_epidemiology_2006}. These tumors comprise complex subtypes that pose significant treatment challenges, driven by diverse genetic profiles and variable responses to therapy, complicating both surgical and therapeutic interventions \\cite{chen_glioma_2017,louis_2021_2021}. In this critical context, multimodal anatomical MRI plays a central role in the clinical workflow for brain tumor assessment. Hence, standard sequences, including T1-weighted (T1), contrast-enhanced T1-weighted (T1c), T2-weighted (T2), and Fluid Attenuated Inversion Recovery (FLAIR), provide complementary information essential for accurate diagnosis, surgical planning, and treatment monitoring \\cite{henson_mri_2005}. Recent advances in self-supervised learning (SSL) have opened new possibilities for learning rich representations from unlabeled data through the use of pretext tasks. Two prominent self-supervised learning paradigms have gained widespread adoption. The first is contrastive learning, exemplified by SimCLR \\cite{pmlr-v119-chen20j}, which aligns representations of different views of the same input. The second is masked image modeling, as used in Masked Autoencoders (MAE) \\cite{he_masked_2022}, where the model learns to reconstruct missing regions from a visible context. These strategies have been increasingly adopted in medical imaging to alleviate the reliance on large annotated datasets."}
{"input": "high-dimensional 3D multimodal data. A unique Vision Transformer (ViT) encoder \\cite{dosovitskiy_image_2021} is first pre-trained on multimodal data through masked image modeling, where large portions of the input are masked and the model learns to reconstruct them. After the pre-training phase, the encoder can be adapted to any modality combinations and be fine-tuned on diverse downstream tasks, as illustrated in Figure~\\ref{fig:intro}. To the best of our knowledge, this is also the first work proposing to handle missing modalities in multimodal brain MRI using a single shared ViT. To thoroughly assess the effectiveness and flexibility of our pre-training strategy, we conduct a comprehensive set of experiments across multiple clinical tasks and modality configurations. Hence, first an encoder is pre-trained on the BraTS2021 \\cite{baid_rsna-asnr-miccai_2021} dataset using all four anatomical modalities. Then, this same encoder is fine-tuned across every modality combinations for various downstream tasks including glioma segmentation, glioma subtyping and survival prediction. %Finally, to assess the generalization capability of our pre-trained encoder beyond gliomas, we fine-tune it on the BraTS-MEN-RT dataset \\cite{labella2024braintumorsegmentationbrats}, which includes T1ce scans with corresponding Gross Tumor Volume (GTV) segmentations for meningioma patients. %This represents a clinically important task in the context of radiotherapy planning. Across all evaluated tasks and modality configurations, fine-tuning the pre-trained encoder consistently leads to strong and reliable performance, demonstrating both its flexibility and robustness."}
{"input": "\\section{Related Work} Across the following paragraphs, we discuss two main areas of related work. The first focuses on established self-supervised learning methods in medical imaging, which aim to learn transferable representations from unlabeled data. Their final objective is similar to that of BM-MAE. The second covers approaches developed to handle missing modalities in the context of segmentation. Although these methods pursue a different goal, they provide valuable insights into architectural and methodological strategies for managing incomplete multimodal data. \\subsection{Self-Supervised Learning for Medical Image Analysis} Self-supervised learning is gaining momentum in medical imaging, with promising approaches emerging to leverage unlabeled data. For instance, Ouyang et al. \\cite{ouyang2021representationdisentanglementmultimodalbrain} propose a method based on 2D slices of MRI and PET to disentangle shared and modality-specific representations. Following a more conventional contrastive paradigm, Ali et al. \\cite{ali_self-supervised_2021} propose using SimCLR to pre-train an encoder on brain MRI scans. Two views of the same patient are generated through data augmentations. The model is then trained to encourage high cosine similarity between views of the same input while minimizing similarity with other samples in the batch through an InfoNCE loss \\cite{oord}. While this approach yields promising results, its effectiveness heavily depends on the choice of data augmentations \\cite{gowda2024breakfreestrongdata}. In medical imaging, however, standard augmentations such as intensity shifts, random cropping, and rotations are often harder to apply consistently due to anatomical constraints and clinical relevance. Additionally, contrastive methods are sensitive to batch size, as larger batches provide more accurate estimates of mutual information \\cite{oord}."}
{"input": "most closely related to ours is likely mmFormer \\cite{zhang_mmformer_2022}, which uses modality-specific transformer-based encoders followed by an attention module to fuse the resulting tokens and handle missing modalities. The fused representation tokens are then passed to a convolutional decoder to output a semantic segmentation map. Beyond the difference in objectives —segmentation versus self-supervised learning— BM-MAE clearly differs architecturally in key ways: it uses a single transformer encoder shared across all modalities (rather than one per modality), a decoder composed entirely of transformer layers, and a masked input strategy that processes only a subset of image patches, resulting in improved computational efficiency."}
{"input": "\\section{Brain Multimodal Masked Autoencoder} Let \\( M = \\{T_1, T_{1c}, T_2, {FLAIR}\\} \\) be the set of MRI modalities, and let \\(\\mathcal{X} = \\{X_m\\}_{m \\in M}\\) denote the multimodal MRI data for a given patient. Hence, each modality \\( X_m \\in \\mathbb{R}^{H \\times W \\times D} \\) represents a 3D volumetric image, where \\( H \\), \\( W \\), and \\( D \\) denote the height, width, and depth of the volume, respectively. \\newline \\newline \\textbf{Patching and positional embeddings.} Each \\(X_m\\) is reshaped into a sequence of patches \\(X_m^p \\in \\mathbb{R}^{L \\times p^3}\\) with \\(p\\) denoting patch size and \\(L=H\\cdot W\\cdot D/p^3\\) the number of patches. Note that here, since all modalities share the same spatial dimensions, we use a common patch size \\(p\\), resulting in the same number of patches \\(L\\) per modality. Afterwards, each modality \\( m \\) is passed through a modality-specific linear projection \\(l_m: \\mathbb{R}^{p^3} \\mapsto \\mathbb{R}^{d}\\), which maps input patches into a shared embedding space, producing modality-specific tokens. Spatial information is then incorporated by adding a fixed sine-cosine positional encoding \\( \\mathrm{PE} \\in \\mathbb{R}^{L \\times d} \\) to to the embedded patches of each modality. Formally, the resulting token sequence for modality \\( m \\) is given by \\( S_m = l_m(X_m^p) + \\mathrm{PE} \\in \\mathbb{R}^{L \\times d} \\). \\newline \\newline \\textbf{Masking strategy.} As mentionned in MAE \\cite{he_masked_2022}, the mask sampling technique can significantly influence performance on downstream tasks."}
{"input": "\\in [0,1] \\), that controls the number of visible tokens, given by \\( L^v = \\left\\lfloor (1 - r) \\cdot |M| \\cdot L \\right\\rfloor, \\) where \\(|M| \\cdot L\\) is the total number of tokens across all modalities. To distribute visible tokens across modalities, we sample modality-specific proportions \\((w_1, \\ldots, w_{|M|})\\) from a symmetric Dirichlet distribution, \\((w_1, \\ldots, w_{|M|}) \\sim \\text{Dir}(\\boldsymbol{\\alpha})\\), satisfying \\( \\sum_{m=1}^{|M|} w_m = 1 \\). Given the total number of visible tokens \\( L^v \\), each modality \\( m \\) is allocated \\( L_m^v = \\lfloor w_m \\cdot L \\rfloor \\) tokens, selected uniformly at random from its patch sequence. The resulting sequence of visible tokens for modality \\( m \\) is denoted by \\( \\overline{S}_m \\in \\mathbb{R}^{L_m^v \\times d} \\). As discussed in \\cite{bachmann_multimae_2022}, Dirichlet-based sampling induces diverse masking patterns by allowing some modalities to be almost entirely masked, promoting representational diversity. \\newline \\begin{figure*}[t] \\centering \\resizebox{\\textwidth}{!}{ \\includegraphics[scale=0.7]{images/bm-mae_framework.pdf} } \\caption{Overview of the proposed BM-MAE architecture, including a detailed view of the decoding mechanism with modality and mask tokens. For clarity, token notations (e.g \\(\\overline{S_m}, Z_m, H_m\\)) are shown for a single modality, though the same applies to all.} \\label{fig:framework} \\end{figure*} \\newline \\newline \\textbf{Multimodal encoder.} For the encoder input, all \\textit{visible} tokens from each modality are concatenated into a single sequence: \\[\\overline{S} = \\text{Concat}(\\texttt{<cls>}, \\{S_m\\}_{m\\in M}) \\in \\mathbb{R}^{(L^v + 1)\\times d}\\] A learnable global \\texttt{<cls>} token is prepended to the sequence to aggregate a patient-level representation, resulting in a sequence of length \\(L^v + 1 \\)."}
{"input": "sequence is given to an encoder \\(E\\) composed of transformer blocks shared across all modalities, as shown in Figure~\\ref{fig:framework}: \\[\\overline{Z} = E(\\overline{S}) \\in \\mathbb{R}^{(L^{v} + 1) \\times d}\\] This design enables the model to capture both spatial structure within modalities and cross-modal relationships. Since \\(E\\) operates solely on visible (i.e., unmasked) patches, the computational cost during training is significantly reduced, as \\(L^v < |M|\\cdot L\\). \\newline \\newline \\textbf{Mask and modality tokens.} Encoded tokens are projected into a lower-dimensional latent space of size \\( d' \\) using a learnable linear layer. This allows to reduce computational overhead as the decoder is only an auxiliary module used for the pre-training phase. The resulting tokens are then unshuffled, and learnable mask tokens are inserted at the positions corresponding to the originally masked patches. We then regroup the resulting tokens by modality, and denote by \\( Z_m \\in \\mathbb{R}^{L \\times d'} \\) the complete sequence for modality \\( m \\), consisting of both encoded and mask tokens. Each \\( Z_m\\) is augmented by adding a fixed sine-cosine positional encoding \\( \\mathrm{PE'} \\in \\mathbb{R}^{L \\times d'} \\) and a modality-specific learnable embedding \\( C_m \\in \\mathbb{R}^{d'} \\), shared across all tokens of the sequence: \\( H_m = Z_m + C_m + \\mathrm{PE'}\\). This augmentation provides the decoder with both spatial and modality information necessary for accurate patch reconstruction. \\newline \\newline \\textbf{Multimodal decoder.} The multimodal part follows a similar strategy as the encoder."}
{"input": "BM-MAE with SimCLR \\cite{ali_self-supervised_2021} and a standard MAE \\cite{10230477}, all using the same ViT backbone \\cite{dosovitskiy_image_2021}. In SimCLR and standard MAE, a single tokenizer is used, which expects a fixed input shape with modalities stacked as channels. As a result, we repeat pre-training independently for each modality combination. In contrast, BM-MAE employs a modality-specific tokenizer for each input modality, followed by a shared encoder, allowing a single unified pre-training phase across all combinations. In our setup, this results in BM-MAE being pre-trained once, while SimCLR and standard MAE require 15 independent runs, one per modality combination. \\newline \\newline \\textbf{Implementation details.} The encoder consists in 12 transformer blocks, each with 12 attention heads, an embedding dimension \\( d = 768 \\), and a multilayer perceptron (MLP) dimension of 1536. The patch size is set to \\(16\\times 16\\times 16\\) voxels. Each decoder is composed of 3 transformer blocks with 12 attention heads and an embedding dimension \\(d' = 384\\). We set the concentration parameter \\(\\boldsymbol{\\alpha}\\) of the Dirichlet distribution to 1, yielding a uniform prior over masking scenarios. Following common practice \\cite{he_masked_2022}, we use \\(r=0.75\\) as the masking ratio, resulting in \\(N^v=512\\) visible tokens for BM-MAE and \\(128\\) for regular MAE. Both MAE and BM-MAE are pretrained for 1000 epochs with a batch size of 6, while SimCLR is pretrained for 400 epochs with a batch size of 4 and a temperature parameter \\(\\tau = 0.05\\)."}
{"input": "a weight decay of 0.05. Training starts with a 50-epoch warm-up, followed by learning rate decay using a cosine schedule. Experiments are conducted on a single NVIDIA GeForce RTX 4090. \\subsection{Downstream Tumor Segmentation} \\textbf{Glioma Subregion Segmentation.} We evaluate BM-MAE on glioma subregion segmentation using BraTS 2021 \\cite{baid_rsna-asnr-miccai_2021}, which includes labels for necrotic core, enhancing tumor, and peritumoral edema. Following the competitions evaluation rules \\cite{baid_rsna-asnr-miccai_2021}, we report Dice scores on three composite classes: whole tumor (WT), enhancing tumor (ET), and tumor core (TC). Hence, we adopt the self-pretraining paradigm from \\cite{10230477} and evaluate BM-MAE fine-tuning on the same dataset. This is the most challenging setting we confront BM-MAE with. This removes any advantage typically gained from transferring to a smaller target set. It allows us to directly assess whether this leads to improved segmentation performance, as observed in \\cite{10230477}, and whether pre-training on all four modalities enables better generalization when some modalities are missing at fine-tuning, particularly for hard subregions. \\newline \\newline \\textbf{Implementation details.} Pre-trained ViTs are used as the backbone of a UNETR model \\cite{hatamizadeh_unetr_2022}. The formulation \\textit{Scratch} refers to the training of the same UNETR from a random initialization. In BM-MAE, each input modality contributes its own set of tokens after the ViT encoder, leading to a variable number of tokens depending on the number of available modalities. To produce a fixed-length sequence compatible with UNETR, which requires hidden states from all transformer blocks, we average the modality-specific representations at each spatial location and block."}
{"input": "\\mathbb{R}^d\\) a specific hidden state at location \\( j \\) for modality \\( m \\), the aggregated hidden state is defined as \\( \\mathrm{v}^j = \\frac{1}{\\mid M \\mid} \\sum_{m \\in M} \\mathrm{v}_{m}^j \\). This produces a consistent sequence of tokens, independent of the modality configuration. %In the case of BM-MAE, the number of tokens produced by the ViT encoder varies with the number of input modalities, as each modality contributes its own set of tokens. %To obtain a fixed-length representation compatible with the UNETR architecture, we average the hidden states across modalities at each spatial location, resulting in a consistent sequence of 512 tokens regardless of the modality configuration. All models are fine-tuned for 70 epochs using the AdamW optimizer with a batch size of 2. The learning rate is initialized at \\(5 \\times 10^{-4}\\) with a weight decay of 0.05. Training begins with a 10-epoch warm-up phase, followed by a cosine decay schedule. \\begin{table}[ht] \\caption{Segmentation results comparing models trained from scratch, fine-tuned after conventional pre-training (separate pre-training for each modality combination), and fine-tuned after BM-MAE pre-training (one single pre-training). *: \\(p < 0.05\\) by Wilcoxon signed-rank test for pairwise comparison with our method.} \\label{tab:seg} \\resizebox{\\textwidth}{!"}
{"input": "we refer to as BM-MAE (\\textcolor{blue}{\\ding{100}}). All models are trained for 20 epochs using the AdamW optimizer with a batch size of 2. The learning rate is initialized at \\( 1 \\times 10^{-4} \\) with a weight decay of 0.05. Training begins with a 5-epoch warm-up phase, followed by a cosine decay schedule. We conduct a 5-fold stratified cross-validation aggregate results as mean \\( \\pm \\) standard deviation. \\newline \\newline \\textbf{Results.} Table~\\ref{tab:subtype} shows that both variants of BM-MAE consistently outperform training from scratch across all modality combinations in terms of AUC and AP. More specifically, the BM-MAE (\\textcolor{blue}{\\ding{100}}) initialization yields an average improvement of 16.9\\% in AUC and 9.9\\% in AP compared to random initialization. The regular BM-MAE achieves even greater gains, with average improvements of 19.7\\% in AUC and 11.1\\% in AP. We also observe that the T1c modality plays a crucial role in accurately subtyping glioma into LGG and GBM, given the fact that LGG usually do not have contrast enhancement. Training from scratch without T1c leads to a sharp drop in performance (AUC: 0.82 → 0.59, AP: 0.89 → 0.73) compared to combinations that include T1c. While both BM-MAE variants also experience a performance decline without T1c (BM-MAE: AUC 0.92 → 0.78, AP 0.95 → 0.86; BM-MAE (\\textcolor{blue}{\\ding{100}}): AUC 0.86 → 0.78, AP 0.92 → 0.86), performances remain substantially higher, which is clinically relevant given the ability to make accurate predictions even without contrast injection."}
{"input": "all share the exact same candidate set in their ballot prefix makes PSC highly non-robust \\citep{Tide06a}. % This issue has been considered from a purely theoretical point of view \\citep{AzLe20a, BrPe23a}, as well as through experiments on synthetic data \\citep{BrPe23a}. There exist several other properties that seek to guarantee proportional representation in ways similar to that of PSC \\citep{AEF+17a, BrPe23a}, and that, from the aforementioned perspective, are more robust \\citep{BrPe23a}. However, such properties are purely qualitative, and like PSC they usually rely rigidly on a ``threshold of representation,'' making them blind to groups of voters that come close to being ``sufficiently large.'' While proportionality axioms have received much attention in the theoretical literature, empirical investigation of the force of axioms formulated to guarantee proportional representation has been limited due to a scarcity of real-world ballot data. However, \\citet{McGr23a} recently compiled a real-world dataset consisting of 1100 Scottish local council elections from the period 2007--2022. In these elections, STV is used to elect members for local councils in Scotland. We observe that in this data, solid coalitions of sufficient size rarely occur. Consequently, for most elections in the dataset, PSC places no or few restrictions on the winning committee, and most outcomes are therefore proportional according to the property. Since PSC often undergirds the claim that STV is proportional, the toothlessness of PSC in practice raises the question of how proportional the method really is.\\footnote{As demonstrated by \\citet{BrPe23a}, STV fails proportionality axioms that are stronger than PSC."}
{"input": "the issue of how to assess the proportionality of a committee when axioms have little to no effect on the outcome. As a first step towards answering this question, we define quantitative versions of several proportionality axioms suggested in the literature, based on the idea that we should be able to relax size-requirements imposed on groups of voters whenever there are few or no groups of sufficient size. This approach not only allows us to measure the extent to which an axiom is satisfied, but also makes it possible to strengthen axioms in situations where they have little to no effect. We furthermore use the data from the Scottish local council elections to assess experimentally the proportionality of different voting rules according to our measures. In practice, voters tend not to form cohesive groups large enough for standard proportionality axioms to place any significant requirement on the winning committee. The issues of ballot truncation and small cohesive groups have been discussed previously in \\citet{BrPe23a, MaPl16a} and \\citep{HKRW21a}, but such work had access to little to no real-world ballot data. Our contribution is that we investigate how to adapt proportionality axioms to such a real-world setting, and we provide empirical results from a large dataset of real-world STV elections. \\subsection*{Related Work} \\paragraph{Proportionality.} The formal study of proportional representation in multiwinner voting dates to the work of \\citeauthor{Dumm84a}, whose proportionality for solid coalitions (PSC) criterion \\citep{Dumm84a} was designed to formally capture the proportionality of STV."}
{"input": "representation has been studied as a fairness criterion in several different decision-making scenarios. Most prominently, starting with \\citet{ABC+16a} and their notion of \\emph{extended justified representation (EJR)} a wide body of work covers proportionality in approval-based multiwinner voting \\citep{LaSk22a}. This has since been used as a basis to study proportionality in settings such as clustering \\citep{CFL+19a, KePe23a, KKK24a, CMS24a}, sortition \\citep{EbMi23a}, participatory budgeting \\citep{PPS21a, BFL+23a}, or AI-augmented civic participation platforms \\citep{FGP+23a}. \\paragraph{Proportionality with Ordinal Preferences.} As mentioned previously, PSC was initially conceptualized by \\citet{Dumm84a}. Following this \\citet{AzLe20a} introduced a generalization of PSC to weak-ordinal preferences (i.e., ordinal preferences with ties allowed) and introduced the \\emph{expanding approvals rule} as an alternative to STV satisfying this generalization. In a recent work, \\citet{DePe24a} studied generalizations of STV (and its single-winner variant) to weak-ordinal preferences and showed that the natural generalization of STV also satisfies the PSC generalization of \\citeauthor{AzLe20a}. In follow-ups, \\citet{AzLe21a} additionally generalized PSC to the setting of participatory budgeting with weak-ordinal preferences and introduced a second generalization of PSC they termed IPSC and derived a characterization of PSC through what they call a Dummett tree \\citep{AzLe22a}. Deviating from the use of solid coalitions, \\citet{BrPe23a} introduced proportionality axioms for ordinal preferences, based on the proportionality axioms for approval preferences and argued that they are more robust than the original PSC based axioms. These axioms were further generalized to the setting of proportional clustering by \\citet{KePe23a} and \\citet{ALM23a}."}
{"input": "stability, a possible extension of Condorcet consistency to multiwinner voting. Both show that committees satisfying local stability might not exist in a given election. However, \\citet{JMW20a} design an approximation algorithm, showing that a $(16+\\varepsilon)$-approximation to local stability always exists. This approximation factor was recently improved by \\citet{CLP+24a} to 9.8217. Locally stable committees additionally found application in the design of randomized voting rules with low distortion \\citep{EKPS24a}. Improving the approximation constant is an open question and additionally has implications for the existence of so-called Condorcet winning sets \\citep{ELS15a, CLP+24a}. Finally, \\citet{Jans18a} also studies this measure for evaluating the proportionality of voting rules, deriving the threshold values for several voting rules in elections with approval ballots, ordinal ballots, or party lists. Additionally, the concept of the proportionality degree \\citep{Skow21a} is highly related, proving a very similar proportionality guarantee in the approval world. \\paragraph{Experiments for Multiwinner Voting.} Next, we highlight the related work on the experimental evaluation of (proportional) multiwinner voting rules. Closely related to us is the recent work of \\citet{BBC+24a} who experimentally evaluate the use of proportional multiwinner voting rules in the Polkadot blockchain, based on real-world data from this blockchain. Further, in the participatory budgeting literature, several papers have analyzed and compared rules on real-world datasets \\citep{BFJK23a, FFP+23a, BFJ+24b}. Besides this, most experimental papers have so far focused on synthetic data or data not necessarily collected for the purpose of multiwinner voting; see the survey by~\\citet{BFJ+24x}. For instance, \\citet{EFL+17a} evaluate ordinal multiwinner voting rules on randomly selected $2$-dimensional Euclidean instances."}
{"input": "\\paragraph{Ballot truncation} We note that the issue of ballot truncation in real-world elections has received little attention in the multiwinner ranked-choice setting, mainly due to the lack of available data. The only study we are aware of which analyzes the effects of ballot truncation empirically is \\citet{HKRW21a}, who only examine six city council elections from Cambridge, Massachusetts. Ballot truncation has received more attention in the single-winner instant runoff setting. For example, \\citep{KGF20, TUK23a} study the effect on the election winner as we vary the amount of truncation, and \\citep{BK15, GSM23} analyze how ballot exhaustion (when partial ballots run out of preferences before the final round) can cause the election winner not to secure a majority of total votes cast."}
{"input": "pair of candidates $c_j \\in C'$ and $c_r \\in C \\setminus C'$, it holds that $c_j \\succ_i c_r$ for all $i \\in N'$. In other words, $C'$ forms a prefix of the ranking $\\succ_i$ of every voter in $N'$. Using the notion of solid coalitions, we can now define PSC.\\footnote{A related axiom is \\emph{generalized PSC} \\citep{AzLe20a}, which generalizes PSC to the case of weak orders. For top-truncated preferences, PSC and generalized PSC are equivalent.} \\begin{definition}[PSC] A committee $W$ satisfies \\emph{proportionality for solid coalitions (PSC)} if for any $\\ell \\in [k]$ and any $\\ell$-large group $N' \\subseteq N$ of voters forming a solid coalition over \\mbox{$C' \\subseteq C$}, it holds that $\\lvert C'\\cap W\\rvert \\ge \\min(\\lvert C'\\rvert, \\ell)$. \\end{definition} As a potential alternative to PSC and possible generalization of the Condorcet principle to proportional representation, \\citet{AEF+17a} introduced the concept of \\emph{local stability}. Intuitively, local stability postulates that no group of voters of size at least $\\frac{n}{k}$ should find an unselected candidate they all prefer to everyone in the committee.\\footnote{The same concept was independently studied by \\citet{JMW20a} in the more general context of core stability.} Notably, committees satisfying local stability need not exist. \\begin{definition}[LS] % A committee $W$ satisfies \\emph{local stability (LS)} if there is no $1$-large group of voters $N' \\subseteq N$ and candidate $c \\notin W$ with $c \\succ_i c'$ for all ${i \\in N'}$ and $c' \\in W$."}
{"input": "\\citet{BrPe23a} introduced ``rank-based'' axioms (based on axioms in approval-based multiwinner voting) that strengthen to conditions imposed by PSC.\\footnote{Since we are only dealing with ordinal preferences in this paper, we omit the ``rank''-prefix used by \\citet{BrPe23a}.} The strongest such axiom that can always be satisfied is PJR+. Intuitively, if a group of voters deserving $\\ell$ candidates all rank a candidate at most at rank $r$ and this candidate is not selected, then $\\ell$ candidates ranked by someone in this group at rank $r$ or better should be included in the committee. \\begin{definition}[PJR+ \\citep{BrPe23a}] A committee $W$ satisfies \\emph{PJR+} if there is no $\\ell \\in [k]$, $\\ell$-large group $N'\\subseteq N$ of voters, unselected candidate $c \\notin W$, and rank $r \\in [m]$ such that \\begin{enumerate}[(i)] \\item $\\rank(i,c) \\le r $ for all $i \\in N'$ \\item $\\lvert \\{c' \\colon \\rank(i,c') \\le r \\text{ for some } i \\in N'\\} \\cap W \\rvert < \\ell$. \\end{enumerate} \\end{definition} \\citet{BrPe23a} were able to show that PJR+ can be verified in polynomial time via a reduction to submodular function minimization. This, however, is more of a theoretical result, as existing polynomial-time submodular function minimization are both difficult to implement and slow, taking upwards of $\\Omega(n^{4})$ time to run \\citep{Cor07a}. Therefore, we refrain from testing PJR+ in our instances and turn to some simpler to compute alternatives, also based on approval-based multiwinner voting. We start with the axiom EJR+ as defined by \\citet{BrPe23a}. Note that EJR+ is not always be satisfiable."}
{"input": "\\emph{EJR+} if there is no $\\ell \\in [k]$, $\\ell$-large group $N'\\subseteq N$ of voters, unselected candidate $c \\notin W$, and rank $r \\in [m]$ such that \\begin{itemize} \\item[i)] $\\rank(i,c) \\le r $ for all $i \\in N'$ \\item[ii)] $\\lvert \\{c' \\colon \\rank(i,c') \\le r\\} \\cap W \\rvert < \\ell$ for all $i \\in N'$. \\end{itemize} \\end{definition} As an alternative to EJR+ we take \\emph{priceability} as defined by \\citet{BrPe23a} and \\citet{PeSk20a}. Priceability is always satisfiable, for instance by the expanding approvals rule, and in essence tries to compute a fractional matching between voters and candidates. \\begin{definition}[Priceability] A committee $W$ is priceable if for each voter $i \\in N$ there is a price function $p_i \\colon C \\to [0,1]$ and a price $p \\in \\mathbb{R}^+$ such that \\begin{itemize} \\item $\\sum_{c \\in C} p_i(c) \\le 1 $ for all $i \\in N$ \\item $\\sum_{i \\in N} p_i(c) \\le p$ for each $c \\in W$ \\item $\\sum_{i \\in N} p_i(c) = 0$ for each $c \\notin W$ \\item $\\sum_{i \\in N\\colon \\rank(i,c) \\le r} (1 - \\sum_{c' \\in W\\colon \\rank(i,c') > r} p_i(c')) \\le p$ \\\\ \\quad \\quad for all $r \\in [m]$. \\end{itemize} \\end{definition} We minimally deviate here from the original definition of \\citet{PeSk20a} by requiring that at most $p$ can be paid for a candidate (instead of exactly $p$). This allows us to associate a price $p$ with any committee."}
{"input": "possible ranks from first to last. For each such rank and so-far unselected candidate $c$ it checks whether the total weight of the voters giving a rank of at most $r$ to $c$ is at least~$q$. If there is such a candidate, it takes any such candidate into the committee and decreases the collective weight of the corresponding voters by~$q$. In our implementation of the rule we select the candidate with the largest total weight and decrease the weights as in Scottish STV. If there is no such candidate, $r$ is increased.\\footnote{The treatment of unranked candidates in EAR allows for different interpretations, as noted in Remark 3 by \\citet{AzLe20a}. When a voter ranks a strict subset $A_i$ of candidates, the set of unranked candidates $C \\setminus A_i$ (i.e., the last equivalence class) can be included either \\textit{(i)} as soon as the ranked candidates are exhausted, or \\textit{(ii)} only in the final step of the method. We tested both variants in our experimental analysis. The performance differences between the two versions were minor, with variant \\textit{(ii)} performing slightly better. Therefore, we focus on variant \\textit{(ii)} here.} The \\emph{Single Non-Transferable Vote (SNTV)} selects the $k$ candidates with the highest first-place vote count. SNTV is sometimes referred to as \\textit{$k$-plurality}. \\emph{Sequential Ranked-Choice Voting (seq-RCV)}, which is currently used in the US state of Utah \\citep{MMLS24a}, executes the single-winner RCV procedure $k$ times. Single-winner RCV (a.k.a."}
{"input": "Labour candidates \\textbf{DM} and \\textbf{LM} and has size $1624$. This coalition consists of the $1218$ voters who cast a ballot of the form $\\textbf{DM}\\succ \\textbf{LM} \\succ \\dots$ and of the $406$ voters who cast a ballot of the form $\\textbf{LM} \\succ \\textbf{DM} \\succ \\dots$. Interestingly, the size of this coalition is much smaller than the total number of voters who ranked a Labour candidate first. The reasons are that some Labour voters rank only one candidate on their ballots and many voters cast split-ticket ballots. The next largest solid coalition has size $1277$ and is over the two SNP candidates \\textbf{BC} and \\textbf{TM}, again barely more than the first-place votes of \\textbf{BC}. The largest solid coalition over more than two candidates consists of $554$ voters who support the two SNP candidates as well as \\textbf{IB}. Solid coalitions over four or more candidates are extremely small. The threshold for a coalition to be $1$-large is $\\lceil \\frac{n}{k} \\rceil = 1711$, and therefore any of the 35 possible winning committees satisfies PSC. In comparison, $24$ out of $35$ committees satisfy rank-EJR+ and priceability, while $12$ out of $35$ outcomes are locally stable."}
{"input": "$\\alpha$-PSC. (Values $\\alpha^\\ell_{(N',C')}$ with $\\ell > |C'|$ are irrelevant because the group's deservingness is upper bounded by $|C'|$ according to the definition of $\\alpha$-PSC.) \\newcommand{\\soli}{\\mathcal{S}} In our algorithm for computing the PSC value of a committee, we compute these values for all solid coalitions. For each subset $C'\\subseteq C$, there is a unique \\textit{maximal} group $N_{C'}$ of voters that solidly supports $C'$. The group $N_{C'}$ consists of all voters ranking all candidates in $C'$ over all other candidates. Clearly, it is sufficient to consider only maximal solid coalitions. Let $\\soli$ denote the set of all maximal solid coalitions. It is not hard to see that $|\\soli|$ is polynomial in the size of the profile and that we can efficiently enumerate all maximal solid coalitions by iterating over the prefixes of the voters. Given the set $\\soli$ of all maximal solid coalitions, we can now collect all threshold values for $\\alpha$. Define $T$ as the set that contains the relevant values for each solid coalition, i.e., \\[T \\, = \\bigcup_{(N',C') \\in \\soli} \\{\\alpha^1_{(N',C')}, \\alpha^2_{(N',C')}, \\dots, \\alpha^{|C'|}_{(N',C')}\\}\\text.\\] Here, each threshold value $\\alpha^\\ell_{(N',C')}$ is associated with a {``PSC constraint''} of the form $|W \\cap C'| \\ge \\ell$. \\begin{restatable}{thm}{psc-W} Given an instance and a committee $W$, the PSC value of $W$ can be computed in polynomial time. \\label{thm:psc-W} \\end{restatable} \\begin{proof} First calculate the set $\\soli$ of all maximal solid coalitions and the set $T$ of relevant thresholds. Consider the threshold values in $T$ in non-increasing order."}
{"input": "\\in C$ and a constraint $\\sum_{c \\in C} x_c \\le k$ ensuring that at most $k$ candidates are selected. Constraints of the form $|W \\cap C'| \\ge \\ell$ can be encoded as $\\sum_{c \\in C'} x_c \\ge \\ell$. We remark that this algorithm has similarities to the \\textit{D'Hondt apportionment method} \\citep{BaYo82a}. In the full version of this paper, we develop a description of this algorithm which gives rise to the idea of ``{apportionment for non-disjoint parties},'' which might be of independent interest. \\subsection{Quantifying Other Axioms} \\label{sec:qOther} Generalizing the quantification approach to local stability and EJR+ is straightforward. Similarly to PSC, we can replace each $\\ell$-large group by an $\\ell_\\alpha$-large group leading to the following two definitions. \\begin{definition}[$\\alpha$-LS] A committee $W$ satisfies \\emph{$\\alpha$-local stability ($\\alpha$-LS)} if there is no $1_\\alpha$-large group $N' \\subseteq N$ of voters and $c \\notin W$ such that $c \\succ_i c'$ for all $i \\in N'$ and~$c' \\in W$. \\end{definition} While $1$-LS might not be achievable, \\citet{CLP+24a} recently showed that every instance admits a $9.8217$-LS committee. For the definition of $\\alpha$-EJR+, we let $\\rank(i,c) = \\lvert \\{c' \\in A_i: c' \\succ_i c\\}| + 1$ denote the rank that voter $i$ assigns to candidate $c$. If $c \\notin A_i$, we let $\\rank(i,c)= m$."}
{"input": "\\in C \\colon \\rank(i,c') \\le r\\} \\cap W \\rvert < \\ell$ for all $i \\in N'$. \\end{enumerate} \\end{definition} The definition of priceability % is already parameterized, with a price of $p < \\frac{n}{k}$ implying PSC \\citep{BrPe23a}. It is easy to generalize this implication to show that if the lowest possible price is $p$, the corresponding committee satisfies $p \\frac{n}{k}$-PSC. Thus, we say that a committee satisfies \\emph{$\\alpha$-priceability} if the smallest price~$p$ for which the committee is priceable % satisfies $p \\le \\alpha\\frac{n}{k}$. For all three notions, the minimal $\\alpha$-value achieved by a given committee can be computed in polynomial time. For local stability and EJR+, it is sufficient to iterate over the unchosen candidates and compare the size of the coalitions that would want to deviate to these candidates. The optimal price for priceability can be computed via a linear program. We note that it is already NP-complete to decide whether any locally stable committee exists \\cite{AEF+17a}. Further, the construction in the proof of \\Cref{thm:hardness} also applies to both priceability and EJR+, showing that computing the minimal $\\alpha$-value for these two measures is also NP-hard."}
{"input": "For each instance, we computed the optimal $\\alpha$-value for each measure (see \\Cref{fig:min-psc-ejr-hist} for histograms of values for PSC and EJR+). For all measures, the majority of minimal $\\alpha$-values lie roughly in the range $0.4$ to $0.6$, the PSC values overall being somewhat lower than for the other measures (EJR+ and priceability in particular). Interestingly, we observe that while a $1$-LS or $1$-EJR+ committee is not guaranteed to exist in general, they always exist for the instances in our dataset. This observation is similar in spirit to the observation that Condorcet winners almost always exist in real-world elections \\citep{McMc24a}. \\begin{figure}[tb] \\centering \\includegraphics[width=0.9\\linewidth]{new_figs/PSC-EJR-val-hist.pdf} \\caption{Histograms of PSC values and EJR+ values achievable in our elections, rounded to one decimal place.} \\label{fig:min-psc-ejr-hist} \\end{figure} \\paragraph{Distance from Optimality} For each voting rule, we counted \\textit{(i)} how often the rule achieves the optimal $\\alpha$-value and \\textit{(ii)} the average distance between the committees chosen by the voting rule and the committees optimizing the $\\alpha$-value.\\footnote{For \\textit{(ii)}, we define the distance between two committees as half of their symmetric difference.} The results, presented in \\Cref{tab:optimal-ptage-avg-dist}, reveal which voting rules are ``most aligned'' with each of the four measures. In particular, SNTV is most aligned with the PSC and LS measures, and the STV rules are most aligned with the EJR+ and priceability measures. The strong performance of SNTV can be considered surprising insofar as the rule does not satisfy any proportionality guarantees."}
{"input": "measures) can be found in the structure of our data: often, most of the constraints that a quantified proportionality axiom like $\\alpha$-PSC imposes involve top-ranked candidates only, and SNTV\\,---\\,by definition\\,---\\,selects the candidates with the most first-place votes. \\begin{table}[t] \\centering \\begin{tabular}{@{}lcccccccc@{}} \\toprule & \\multicolumn{2}{c}{PSC} & \\multicolumn{2}{c}{EJR+} & \\multicolumn{2}{c}{Priceability} & \\multicolumn{2}{c}{LS}\\\\ \\cmidrule(r){2-3}\\cmidrule(lr){4-5}\\cmidrule(lr){6-7}\\cmidrule(lr){8-9} & opt. & dist. & opt. & dist. & opt. & dist. & opt. & dist. \\\\ \\midrule S-STV & 856 & 0.20 & 826 & 0.23 & \\textbf{870} & \\textbf{0.19} & 829 & 0.23 \\\\ % M-STV & 819 & 0.24 & \\textbf{842} & \\textbf{0.22} & 840 & 0.22 & 754 & 0.30 \\\\ % EAR & 677 & 0.38 & 737 & 0.33 & 709 & 0.35 & 656 & 0.40 \\\\ % SNTV & \\textbf{901} & \\textbf{0.16} & 752 & 0.30 & 832 & 0.23 & \\textbf{935} & \\textbf{0.13} \\\\ seq-RCV & 552 & 0.50 & 646 & 0.40 & 586 & 0.46 & 459 & 0.60 \\\\ \\bottomrule \\end{tabular}% \\caption{For each rule and each axiom, \\textit{(i)} ``opt.'' refers to the number of instances for which the rule achieves the optimal $\\alpha$-value and \\textit{(ii)} ``dist.'' refers to the average distance between the outcome of the rule and the outcome with optimal $\\alpha$-value (measured in terms of number of candidates that need to be exchanged). The best values in each column appear in bold.} \\label{tab:optimal-ptage-avg-dist} \\end{table} \\begin{figure}[tb] \\centering \\includegraphics[width=\\linewidth]{new_figs/PSC-vals-rules.pdf} \\caption{The PSC values achieved by voting rules, together with optimal PSC values (shown in the leftmost column)."}
{"input": "\\end{figure} \\paragraph{Values Achieved by Rules} We furthermore computed the spread of $\\alpha$-values achieved by different voting rules over the set of all instances and compared these values to the spread of optimal $\\alpha$-values. For PSC values, the results are presented as a box plot in \\Cref{fig:alphas-psc-boxplot-rules}. Overall, all proportional rules\\,---\\,and the semi-proportional SNTV\\,---\\,perform similarly in terms of approximating optimal values, with the range of values for each of the rules coming close to those of the optimal values.\\footnote{ The outlier value at $\\alpha \\approx 0.08$ stems from the 2012 election of North Lanarkshire, Ward 9, % where $3$ out of $4$ candidates needed to be elected. In this election, all rules and measures choose the same committee and the only unselected candidate is greatly unpopular. } Somewhat surprisingly, EAR\\,---\\,the rule satisfying the strongest proportionality axioms (see \\Cref{sec:rules})\\,---\\,does slightly worse than the other proportional rules. (This is also apparent in \\Cref{tab:optimal-ptage-avg-dist}.) Furthermore, SNTV does slightly better than the other rules w.r.t. PSC values (and the same is true for LS). A reason for that, as already discussed in the context of \\Cref{tab:optimal-ptage-avg-dist}, is that the measures often require the $k$ most popular candidates to be chosen: on average, 57\\% of the constraints corresponding to the optimal PSC value are over singleton sets of candidates, and thus correspond directly to first-place votes. \\paragraph{Pairwise Comparisons} Finally, we considered pairwise comparisons of voting rules w.r.t. the $\\alpha$-values they achieve."}
{"input": "\\section{The Effect of Ballot Truncation} \\label{sec:truncation} To understand how ballot truncation affects our results, we created ballot data with complete rankings based on the Scottish data. To create this synthetic data, we employed an iterative process that is described in the full version of this paper. Basically, when extending partial ballots of length $r$ to length $r+1$, we consider the frequency of ballots of length at least $r+1$ which agree on the first $r$ entries. We then reran all experiments from \\Cref{sec:exp} for the 1070 synthetic election instances with complete rankings. Overall, the results for completed instances are very similar to the results for the original (truncated) instances. For instance, in $21.8\\%$ of these elections any committee of size $k$ is compatible with PSC (compared to $27.5\\%$ in the truncated case; see \\Cref{tab:complete}). This implies that the effect of ballot truncation is rather limited for the elections we study. This is a bit of a surprise, as these results suggest that the primary reason PSC has little discriminatory power in real-world elections is \\textit{not} that voters truncate their ballots; rather, even if preferences are completed, voters do not form sufficiently large cohesive groups. In general, as expected, the achieved $\\alpha$-values are slightly larger in the completed instances, with, for instance, SNTV also sometimes violating EJR+. Further, we observe that in most instances $\\frac{k}{k+1}$ is a lower bound on the lowest possible priceability value, and that most priceability values achieved by the rules are clustered around that threshold."}
{"input": "\\section{Detailed Analysis of Experiments} \\label{app:experiments} \\subsection{Agreement of Rules} \\label{app:exp-rules} To complement the results on how often rules disagree, we calculated the average distance between each pair of rules. The results are given in \\Cref{tab:rules-avgdist}. The pairwise distance between rules is low overall. As we have seen in \\Cref{tab:rules-num-disagreement}, except for SNTV and seq-RCV, each pair agrees in a majority of cases. Moreover, the committees returned by the different rules often differ by only one candidate when they disagree. \\begin{table} \\centering \\begin{tabular}{@{}lccccc@{}} \\toprule & S-STV & M-STV & EAR & SNTV & seq-RCV \\\\ % \\midrule S-STV & 0 & 0.10 & 0.25 & 0.26 & 0.46 \\\\ % M-STV & - & 0 & 0.22 & 0.31 & 0.39 \\\\ % EAR & - & - & 0 & 0.44 & 0.44 \\\\ % SNTV & - & - & - & 0 & 0.58 \\\\ % seq-RCV & - & - & - & - & 0 \\\\ \\bottomrule \\end{tabular} \\caption{Average distance between pairs of rules.} \\label{tab:rules-avgdist} \\end{table} \\subsection{Alignment of Measures} In order to check how closely aligned our four measures are, we calculated, for each election, the optimal committee w.r.t. each measure (i.e., the committee minimizing $\\alpha$). \\Cref{tab:ax-avgdists} shows how much the committees optimizing different measures differ from each other. The difference between the committees optimizing the LS value and committees optimizing other measures is overall higher than the difference between the other measures."}
{"input": "\\phantom{X}LS\\phantom{X} \\\\ \\midrule S-STV & 763 & 591 & 667 & 743 \\\\ % M-STV & 748 & 637 & 675 & 700 \\\\ % EAR & 584 & \\textbf{{645}} & \\textbf{739} & 579 \\\\ % SNTV & \\textbf{860} & 454 & 543 & \\textbf{906} \\\\ seq-RCV & 409 & 618 & 559 & 336 \\\\ \\bottomrule \\end{tabular}% \\caption{For each rule and each axiom, the number of instances for which the rules achieves the optimal $\\alpha$-value for completed preferences. The best values in each column appear in bold} \\label{tab:optimal-ptage-avg-dist-complete} \\end{table} \\subsection{Values Achieved by Rules} \\label{app:comp-boxplots} We compared the spread of $\\alpha$-values achieved by rules to the optimal $\\alpha$-values for each measure. The results are presented in \\Cref{fig:psc-rules-completed} to \\Cref{fig:price-rules-completed}. We observe that the $\\alpha$-values of all rules become slightly higher in the complete case – this is in line with the minimal possible values increasing somewhat. Furthermore, the number of seq-RCV committees that have a value of $\\alpha > 1$ increases for each measure. In other words, for each axiom, there are more instances where seq-RCV returns a committee that does not satisfy the axiom. We also observe a difference in the spread of priceability values for all rules, compared to that in the truncated case. This has to do with the $\\frac{k}{k+1}$ lower bound on the priceability value: the priceability values achieved by rules cluster around this area, to the point where any priceability value that does not is considered an outlier. \\begin{figure} \\centering \\includegraphics[width=.95\\linewidth]{new_figs/psc_vals_rules_completed."}
{"input": "Noetherian and for every finite $R$-algebra $B$ that is an integral domain, $B$ admits a finite $(S_2)$-ification, in the sense that there is a finite inclusion of integral domains $B\\to C$ so that $C$ is $(S_2)$ and $B_\\fp=C_\\fp$ for all $\\fp\\in\\Spec_1(B)$. We have the following main result, which the author believes to be new (Definition \\ref{def:semi-Nagata} and Theorems \\ref{thm:semi-Nagatacharacterizelocal} and \\ref{thm:semi-Nagatacharacterize}) \\begin{Thm}\\label{thm:semi-Nagatamain} Let $R$ be a Noetherian ring. \\begin{enumerate}[label=$(\\roman*)$] \\item\\label{SagataMain:S1fiber} If $R$ is semilocal, then $R$ is semi-Nagata if and only if $R$ has $(S_1)$ formal fibers. \\item If $R$ is semi-Nagata, then every essentially finitely generated $R$-algebra is semi-Nagata. \\item $R$ is semi-Nagata if and only if $R$ has $(S_1)$ formal fibers and for every $\\fp\\in\\Spec(R)$ there exists $f\\in R,f\\not\\in\\fp$ so that $(R/\\fp)_f$ is $(S_2)$. \\item $R$ is semi-Nagata if and only if for every finite $R$-algebra $B$ that is an integral domain, there is a finite inclusion of integral domains $B\\to C$ so that $C$ is $(S_2)$. \\end{enumerate} \\end{Thm} The corresponding classical result for Nagata rings is \\begin{Thm} Let $R$ be a Noetherian ring. \\begin{enumerate}[label=$(\\roman*)$] \\item\\label{nagata1} If $R$ is semilocal, then $R$ is Nagata if and only if $R$ has geometrically reduced formal fibers. \\item\\label{nagata2} If $R$ is Nagata, then every essentially finitely generated $R$-algebra is Nagata. \\item\\label{nagata3} $R$ is Nagata if and only if $R$ has geometrically reduced formal fibers and for every finite $R$-algebra $B$ that is an integral domain there exists $f\\in B^\\circ$ so that $B_f$ is normal."}
{"input": "\\ref{thm:killFONSI}). After all, FONSIs are pretty rare (Remarks \\ref{Rem:UCnoFONSI} and \\ref{Rem:S2noFONSI} and Lemma \\ref{lem:FONSIfinite}). The idea is inspired by a classical argument of Ratliff \\cite[\\S 31, Lemma 4]{Mat-CRT}. We use local cohomology to make a conceptual argument. We warn the reader that for a semi-Nagata ring $R$, $R^\\sigma$ may not be finite (Example \\ref{exam:semi-NagataRsigmaNotFinite}), resulting in an infinite ascending chain of $(S_2)$-ifications; and $(S_2)$-ifications of modules may not exist (Remark \\ref{rem:NoModuleS2ify}). Again, expectations are met when FONSIs are not present (Corollary \\ref{cor:NOFONSISagataNS2finite} and Theorem \\ref{thm:naiveS2ofmodulesfiniteforSagataNOFONSI}).\\\\ Unfortunately, the author was not able to show lifting of the semi-Nagata property. However, when restricted to semilocal rings, advances in the lifting problem are made in the second part (\\S\\S\\ref{sec:Local-Lifting-Nishimura}--\\ref{sec:Local-lifting-result}) of this article. We repeat the problem in this setting. \\begin{Ques}[local lifting problem]\\label{ques:localformallift} Let $\\bR$ be a property of Noetherian rings. Let $R$ be a semilocal Noetherian ring, $I$ an ideal of $R$. Assume $R$ is $I$-adically complete and $R/I$ is $\\bR$. Is $R$ always $\\bR$? \\end{Ques} We show (\\S\\ref{sec:Local-lifting-result}) \\begin{Thm}\\label{thm:local-lifting-main} Question \\ref{ques:localformallift} admits an affirmative answer when $\\bR$=``has $(S_k)$ formal fibers,'' where $k\\geq 0$ is arbitrary, ``has Cohen--Macaulay formal fibers,'' ``has Gorenstein formal fibers,'' ``has lci formal fibers,'' and ``is a quotient of a Cohen--Macaulay ring.'' \\end{Thm} Note that when $k=1$, this is exactly the semi-Nagata property (Theorem \\ref{thm:semi-Nagatamain}\\ref{SagataMain:S1fiber}). We show the formal fiber properties in Theorem \\ref{thm:local-lifting-main} following the argument of Nishimura \\cite{Nishimura-semilocal-lifting}."}
{"input": "nice rings strictly functorially with respect to nice homomorphisms. This task was effortless for the properties considered in \\cite{Nishimura-semilocal-lifting}. We explain this in \\S\\ref{sec:Local-Lifting-Nishimura}. In our case, such an assignment of ideals can be found with effort. We do a basic reduction in \\S\\ref{sec:ExtendAssignment}, saying we just need to assign $\\fm$-primary ideals to complete local rings $(A,\\fm)$ that are $\\bP$ exactly on the punctured spectrum, strictly functorial with respect to flat maps with $\\bP$-fibers and $0$-dimensional special fiber. After the reduction we find a desired assignment for all properties but lci in \\S\\ref{sec:SkCMGorAssign}. The case $\\bR$=``is a quotient of a Cohen--Macaulay ring'' of Theorem \\ref{thm:local-lifting-main} follows from the case $\\bR$=``has Cohen--Macaulay formal fibers'' via the argument already present in \\cite[\\S 8]{Lyu-dual-complex-lift}. Finding the assignment for the lci property is the most difficult. This is because the intrinsic invariant that determines a Noetherian ring $A$ is lci or not, namely the cotangent complex $L_{A/\\bZ}$, does not have finite cohomology modules. When $A$ is complete local, we can find a regular local ring $R$ and a surjective map $R\\to A$, and $L_{A/R}$ does have finite cohomology modules; however, this is still fragile with respect to ring maps. In any case, we need a way to use an ideal to detect the non-flatness of certain non-finite modules, more explicitly the modules $C_n$ appearing in \\cite{Briggs-Iyenger-Cotangent-Complex}. We investigate their structures in \\S\\ref{sec:lciAssign}, and successfully define their Fitting invariant (for $n\\geq\\dim A+2$), which does the trick."}
{"input": "\\section{Finite inclusions of Noetherian integral domains} \\label{sec:FiniteInclusion} \\begin{Lem}\\label{lem:finiteInclMinoverMinAssoverAss} Let $R\\subseteq R'$ be a finite inclusion of Noetherian semilocal domains. Then the canonical surjective map $\\Spec(R'^\\wedge)\\to\\Spec(R^\\wedge)$ restricts to surjective maps $\\Min(R'^\\wedge)\\to\\Min(R^\\wedge)$ and $\\Ass(R'^\\wedge)\\to\\Ass(R^\\wedge)$. \\end{Lem} \\begin{proof} There exists an $f\\in R^\\circ$ such that $R'_f$ is flat over $R_f$, so $(R'^\\wedge)_f$ is flat over $(R^\\wedge)_f$. As $f$ is a nonzerodivisor in both $R^\\wedge$ and $R'^\\wedge$ we get the desired result, cf. \\citetwostacks{00ON}{0337}. \\end{proof} \\begin{Lem}\\label{lem:FactorFlatBir} Let $R\\subseteq R'$ be a finite inclusion of Noetherian integral domains. Then there exists a factorization $R\\subseteq R''\\subseteq R'$ so that $R\\to R''$ is flat and $R''\\to R'$ is birational. \\end{Lem} \\begin{proof} Let $x'\\in R'^\\circ$ be not in the fraction field $K$ of $R$. Let $f_{x'}(T)=\\sum a_i T^i$ be the monic minimal polynomial of $x'$ over $K$, and let $d=\\deg f_{x'}$. Then for $x\\in R^\\circ$ the monic minimal polynomial of $xx'$ over $K$ is $f_{xx'}(T)=\\sum x^{d-i}a_i T^i.$ Therefore we may choose $x$ so that $f_{xx'}\\in R[T]$, so $R[xx']\\cong R[T]/f_{xx'}(T)$ is flat over $R$. Inductively we can find our $R''$. \\end{proof} \\begin{Lem}[cf. {\\cite[(33.11)]{Nagata-local}}]\\label{lem:MinToAssNOETH} Let $R\\subseteq R'$ be a finite inclusion of Noetherian integral domains. Let $x\\in R^\\circ$. Then every minimal prime divisor of $xR'$ lies above a prime divisor of $xR$. \\end{Lem} \\begin{proof} By Lemma \\ref{lem:FactorFlatBir} we may assume $R$ and $R'$ has the same fraction field. Let $\\fp'$ be a minimal prime divisor of $xR'$ and let $\\fp=\\fp'\\cap R$. Let $b\\in R^\\circ$ be such that $bR'\\subseteq R$ and that $b\\in\\fp$. As $\\HT(\\fp')=1$, $\\fp'^nR'_{\\fp'}\\subseteq bR'_{\\fp'}$ for some $n$."}
{"input": "\\section{Subalgebras of normalization} See \\cite[\\S 33]{Nagata-local} for relevant materials. \\begin{Thm}\\label{thm:Krull} Let $R$ be a Noetherian integral domain, %$R^\\nu$ its normalization, $S$ a sub-$R$-algebra of $R^\\nu$. Then for every $\\fp\\in\\Spec(R)$, there are only finitely many $\\fq\\in\\Spec(S)$ above $\\fp$, and $\\kappa(\\fq)$ is finite over $\\kappa(\\fp)$ for all $\\fq$. \\end{Thm} \\begin{proof} If $S=R^\\nu$, then this is part of \\cite[Theorem 33.10]{Nagata-local}. The general case follows from the fact $\\Spec(R^\\nu)\\to \\Spec(S)$ is surjective. %\\citestacks{00GQ}. \\end{proof} %A \\emph{minimal prime divisor} of an ideal $I$ in a ring $R$ is a \\begin{Lem}[cf. {\\cite[(33.11)]{Nagata-local}}]\\label{lem:MinToAss} Let $R$ be a Noetherian integral domain, %R^\\nu$ its normalization, $S$ a sub-$R$-algebra of $R^\\nu$. Let $a\\in S^\\circ$, and let $\\fq$ be a minimal prime divisor of $aS$. Then the following hold. \\begin{enumerate}[label=$(\\roman*)$] \\item\\label{MtoA:ht} There exists a finite $R$-subalgebra $R'$ of $S$ such that $\\HT(\\fq\\cap R')=1$. \\item\\label{MtoA:Ass} If $a\\in R$, then $\\fp:=\\fq\\cap R$ is a prime divisor of $aR$. %; that is, $\\fp\\in\\Ass_R(R/aR)$. \\end{enumerate} \\end{Lem} \\begin{proof} By Theorem \\ref{thm:Krull}, we can take a finite subalgebra $R'\\subseteq S$ so that $a\\in R'$ and $\\fq$ is the only prime ideal of $S$ above $\\fp'=\\fq\\cap R'$. If $\\fp'$ were not a minimal prime divisor of $aR'$, then we can find primes $\\fp_0'\\subsetneq\\fp'$ in $R'$ containing $a$. We can then find primes $\\fq_0\\subsetneq\\fq_1$ of $S$ lying above $\\fp_0'\\subsetneq\\fp'$; they automatically contain $a$. By uniqueness, $\\fq_1=\\fq$, contradicting the minimality of $\\fq$. Therefore $\\fp'$ is a minimal prime divisor of $aR'$, so $\\HT(\\fp')=1$ as $R'$ is Noetherian. This is \\ref{MtoA:ht}, and we get \\ref{MtoA:Ass} by Lemma \\ref{lem:MinToAssNOETH}."}
{"input": "$R$ be a Noetherian integral domain, %$R^\\nu$ its normalization, $S$ a sub-$R$-algebra of $R^\\nu$. Then for every $a\\in S^\\circ$, the set of minimal prime divisors $\\fq$ of $aS$ is finite, and for every $\\fq$, $S_\\fq$ is a $1$-dimensional Noetherian ring. \\end{Thm} \\begin{proof} We may assume $a\\in R$. Finiteness follows from Lemma \\ref{lem:MinToAss}\\ref{MtoA:Ass} and Theorem \\ref{thm:Krull}. For each $\\fq$, Lemma \\ref{lem:MinToAss}\\ref{MtoA:ht} gives a map $R'_{\\fq\\cap R'}\\to S_\\fq$, so $S_\\fq$ is a $1$-dimensional Noetherian ring by the theorem of Krull-Akizuki \\cite[Theorem 33.2]{Nagata-local}. \\end{proof} \\begin{Def} Let $R$ be a Noetherian integral domain, $S$ a sub-$R$-algebra of $R^\\nu$. We say $S$ is $(S_2)$ if for all $a\\in S^\\circ$, $aS$ is a finite intersection of primary ideals of height $1$. This is the same as Serre's condition $(S_2)$ if $S$ is Noetherian. \\end{Def} \\begin{Lem}\\label{lem:S2assPrincipal} Let $R$ be a Noetherian integral domain, $S$ a sub-$R$-algebra of $R^\\nu$. Then the following are equivalent. \\begin{enumerate}[label=$(\\roman*)$] \\item\\label{S2ass:S2} $S$ is $(S_2)$. \\item\\label{S2ass:ass} For all $a\\in S^\\circ$, the set of zero divisors on $S/aS$ is the union of minimal prime divisors of $aS$. \\item\\label{S2ass:intersect} $S=\\bigcap_{\\fq\\in\\Spec_1(S)}S_\\fq$. \\end{enumerate} \\end{Lem} \\begin{proof} That \\ref{S2ass:S2} implies \\ref{S2ass:ass} is clear as minimal prime divisors of and the set of zero divisors modulo $aS$ can be read off of a primary decomposition, cf. \\cite[Proposition 4.7]{AMcommalg}. Assume \\ref{S2ass:ass}. Let $z=x/y\\in \\bigcap_{\\fq\\in\\Spec_1(S)}S_\\fq$ where $x,y\\in S^\\circ$. Then $(xS:_S yS)$ is not contained in any minimal prime of $yS$, as they are of height $1$ (Theorem \\ref{thm:SisKrull}). Therefore $(xS:_S yS)$ contains a nonzerodivisor on $S/yS$ by prime avoidance, so $x\\in yS,z\\in S$."}
{"input": "rings defining the intersection of $S'^{n\\sigma}$ contains that of $S^{n\\sigma}$, giving the $(-)^{n\\sigma}$ case. If $S'\\subseteq S^{\\sigma}$, then $S'^{\\nu}=S^{\\nu}$, which gives the $(-)^{\\sigma}$ case. \\end{proof} \\begin{Exam}\\label{exam:LechGOONS} By a theorem of Lech \\cite{Lech-completion-domain}, a complete Noetherian local ring containing a field is the completion of a Noetherian local domain if and only if its depth is at least $1$. Therefore, there exist a Noetherian local domain $R$ of dimension $2$ whose completion is $k[[x,y,z]]/(x,y)\\cap(z)$, where $k$ is a field. In this case, $R^{n\\sigma}=\\cO(U)$, where $U$ is the punctured spectrum of $R$. Therefore $R^{n\\sigma}\\otimes_R R^\\wedge=\\cO(U^\\wedge)$, where $U^\\wedge$ is the punctured spectrum of $R^\\wedge$. By the specific form of $R^\\wedge$ we see $\\cO(U^\\wedge)=k((z))\\times k[[x,y]]$, so $\\cO(U^\\wedge)$ is not integral over $R^\\wedge$, and $R^{n\\sigma}$ is not integral over $R$. If the field $k$ has characteristic zero, then we can even make $R$ quasi-excellent; in fact, a Noetherian complete local ring containing a field of characteristic zero is the completion of a quasi-excellent local domain if and only if it is reduced. See \\cite[Proof of Theorem 9]{Loepp-03-complete-excellent-domains}. \\end{Exam} \\begin{Def}\\label{def:GOONS} Let $R$ be a semilocal Noetherian domain. A \\emph{formal obstruction to naive $(S_2)$-ification for $R$}, or \\emph{FONSI for $R$}, is a $P\\in \\Spec(R^\\wedge)$ such that $\\HT(P\\cap R)>1$ and that there exists $P_0\\in\\Min(R^\\wedge)$ contained in $P$ such that $\\HT(P/P_0)=1$. Note that in particular $\\HT(P)>1$. The set of such $P$ is denoted $\\OS{R}.$ There is a canonical identification $\\OS{R}=\\bigsqcup_{\\fm\\in\\Max(R)}\\OS{R_\\fm}$. For a non-semi-local $R$, we abuse notations and write $\\OS{R}$ for $\\bigsqcup_{\\fm\\in\\Max(R)}\\OS{R_\\fm}$."}
{"input": "proves the lemma. \\end{proof} \\begin{Cor}\\label{cor:NS2ofGOONSfreeS} Let $R$ be a Noetherian integral domain, $S$ a sub-$R$-algebra of $R^\\nu$. Assume that $\\OS{R}=\\emptyset$. % for all $\\fm\\in\\Spec(R)$. Then the following are true. \\begin{enumerate}[label=$(\\roman*)$] \\item\\label{NS2c:integral} $S^{n\\sigma}$ is integral over $S$, so $S^{n\\sigma}=S^\\sigma$. \\item\\label{NS2c:S2} $S^\\sigma$ is $(S_2)$. \\item\\label{NS2c:S2unique} If a sub-$S$-algebra $S'$ of $S^\\sigma$ is $(S_2)$, then $S'=S^\\sigma$. \\end{enumerate} \\end{Cor} \\begin{proof} Let $S'$ be a sub-$S$-algebra of $R^\\nu$. Let $\\fq'\\in \\Spec_1(S')$. Then $\\HT(\\fq'\\cap R)=1$ by Lemma \\ref{lem:NoFonsiht1Preim}, so $\\HT(\\fq'\\cap S)=1$. Therefore $S^{n\\sigma}\\subseteq S'^{n\\sigma}$. Apply this to $S'=R^\\nu$, noting that $(R^\\nu)^{n\\sigma}=R^\\nu$ as $R^\\nu$ is a Krull domain \\cite[Theorem 33.10]{Nagata-local}, we see \\ref{NS2c:integral} holds. Now apply the inclusion $S^{n\\sigma}\\subseteq S'^{n\\sigma}$ and Lemma \\ref{lem:reverseIncl} to a sub-$S$-algebra $S'$ of $S^{n\\sigma}=S^\\sigma$. We get $S^{n\\sigma}= S'^{n\\sigma}$, giving \\ref{NS2c:S2}\\ref{NS2c:S2unique}."}
{"input": "\\section{Extension rings via first local cohomology} \\label{sec:ExtendbyLocCoh} This is an auxiliary section that provides a conceptual variant of Ratliff's construction \\cite[\\S 31, Lemma 4]{Mat-CRT}. \\begin{Discu}\\label{discu:DefR+M} Let $R$ be a Noetherian ring and let $I$ be an ideal of $R$. Then there exists a canonical exact sequence \\[\\begin{tikzcd} 0\\arrow[r] & H^0_I(R)\\arrow[r] & R \\arrow[r] & \\cO(D(I)) \\arrow[r] & H^1_I(R)\\arrow[r] & 0. \\end{tikzcd}\\] For a submodule $M$ of $H^1_I(R)$, we denote by $R+_I M$ the unique submodule of $\\cO(D(I))$ that contains the image of $R$ and has image $M$ of $H^1_I(R)$. Denote by $R[I;M]$ the subalgebra of $\\cO(D(I))$ generated by $R+_I M$, and denote by $M^a$ the unique submodule of $H^1_I(R)$ such that $R+_I M^a=R[I;M]$. We thus have a commutative diagram with exact rows \\[\\begin{tikzcd} 0\\arrow[r] & H^0_I(R)\\arrow[r] \\arrow[equal,d] & R \\arrow[r] \\arrow[equal,d] & R+_I M \\arrow[r] \\arrow[hookrightarrow,d] & M\\arrow[r]\\arrow[hookrightarrow,d] & 0\\\\ 0\\arrow[r] & H^0_I(R)\\arrow[r] & R \\arrow[r] & R[I;M] \\arrow[r] & M^a\\arrow[r] & 0. \\end{tikzcd}\\] There is an obvious functoriality with respect to ring maps, giving compatibility with flat base change; and an obvious functoriality with inclusion of submodules. The module $R+_I M$ is finite if and only if $M$ is finite; in which case $R[I,M]$ is a finitely generated $R$-algebra. \\end{Discu} \\begin{Discu}\\label{discu:Rmoda+M} If an ideal $\\fa\\subseteq R$ is $I^\\infty$-torsion, then for $\\overline{R}=R/\\fa$ and $\\overline{I}=(I+\\fa)/\\fa$ we have $H^1_I(R)=H^1_{\\overline{I}}(\\overline{R})$, and $R+_I M=\\overline{R} +_{\\overline{I}} M$. In particular, taking $\\fa=H^0_I(R)$, we reduce to the case $H^0_I(R)=0$, or equivalently, $I$ contains a nonzerodivisor on $R$. \\end{Discu} \\begin{Discu}\\label{discu:R+MTwoSteps} As $R+_IM\\subseteq\\cO(D(I))$ we have $H^0_I(R+_I M)=0$."}
{"input": "then finite over $R$ by Lemma \\ref{lem:onestepExtend}. As $\\fm\\in \\Spec(R)$ is p-unibranch, $R_1$ is a local ring, and we denote its maximal ideal by $\\fm_1$. We know $\\dim R_1=\\dim R\\geq 2$, and we know $\\depth R_1\\geq 1$ as $R_1$ is a subring of $\\cO(D(\\fm))$. Therefore the same applies to the ring $R_1$, and from Discussion \\ref{discu:R+MTwoSteps} we see $R[\\fm;M_2]$ is finite over $R$ for $M_2=(M_1^a:\\fm)$. Inductively we see $R[\\fm;H^1_\\fm(R)]$ is integral over $R$, and so is its subring $R[\\fm;M]$. \\end{proof} \\begin{Rem} Instead of p-unibranchness, the conclusion of Lemma \\ref{lem:punibrLocalCohomWholeExtend} holds with the weaker assumption that all maximal ideals of all finite $D(\\fm)$-modifications of $R$ have height at least $2$. This is true, for instance, when $R$ is universally catenary, by the dimension formula \\citestacks{02IJ}."}
{"input": "$P$. Fix a $P\\in\\OS{R}$ and let $\\fp=P\\cap R\\in\\Spec(R)$, so we have $\\HT(\\fp)>1$. By Theorem \\ref{thm:Krull}, there exists a finite $D(\\fp)$-modification $R_1$ of $R$ such that all preimages %$\\fp_{11},\\ldots,\\fp_{1n}$ of $\\fp$ in $\\Spec(R_1)$ are p-unibranch. We know $R_1\\subseteq R^\\sigma$ as $\\HT(\\fp)>1$. Next, we take a finite $D(\\fp R_1)$-modification $R_2$ of $R_1$, so that the number of maximal ideals of $(R_2^\\wedge)_P$ (\\emph{i.e.} the number of preimages of $P$ in $\\Spec(R_2^\\wedge)$) is maximal among all possible $R_2$; to see this is achievable, note that $(R_2^\\wedge)_{\\operatorname{red}}$ is finite birational over $(R_1^\\wedge)_{\\operatorname{red}}$, so $((R_2^\\wedge)_{\\operatorname{red}})_P$ is contained in the normalization of $((R_1^\\wedge)_{\\operatorname{red}})_P$, which is finite as a complete Noetherian local ring is Nagata \\citestacks{0335}, so the number of maximal ideals of $(R_2^\\wedge)_P$ is bounded. Note that we still have $R_2\\subseteq R^\\sigma$ and that all preimages of $\\fp$ in $\\Spec(R_2)$ are p-unibranch. We will show no FONSIs of $R_2$ are above $P$. Assume that there exists a $P_2\\in \\OS{R_2}$ above $P$. Let $\\fp_2=P_2\\cap R_2$, so $\\fp_2$ lies above $\\fp$, therefore is p-unibranch; and $\\HT(\\fp_2)>1$ as $P_2\\in \\OS{R_2}$. In particular $T:=(R_2)_{\\fp_2}$ and $A:=(R_2^\\wedge)_{P_2}$ have depth at least $1$. The punctured spectrum $U$ of the ring $A$ is disconnected (Remark \\ref{Rem:S2noFONSI}), %, as it has an isolated point given by a minimal prime $P_{02}\\subsetneq P_2$ with $\\HT(P_2/P_{02})=1$. therefore $\\cO(U)$ has a nontrivial idempotent $e$. In the notations of Discussion \\ref{discu:DefR+M}, there exists a finite submodule $N\\subseteq H^1_{P_2A}(A)$ such that $A+_{P_2A}N=A[P_2A;N]=A[e]$. As $T\\to A$ is flat and as $H^0_{\\fp_2T}(T)=0$, we have $H^1_{P_2A}(A)=H^0_{P_2A}(H^1_{\\fp_2T}(T)\\otimes_T A)$ (Discussion \\ref{discu:R+MdifferentIdeals})."}
{"input": "submodule $M\\subseteq H^1_{\\fp_2T}(T)$ such that $N\\subseteq M\\otimes_T A$. Note that $\\fp_2T\\in\\Spec(T)$ is p-unibranch (Lemma \\ref{lem:extendUmodif}), so the ring $T[\\fp_2T;M]$ is finite over $T$ (Lemma \\ref{lem:punibrLocalCohomWholeExtend}), and therefore a finite $D(\\fp_2T)$-modification. We can then find a finite $D(\\fp_2)$-modification $R_3$ of $R_2$, which is automatically a finite $D(\\fp R_1)$-modification of $R_1$, such that $(R_3)_{\\fp_2}=T[\\fp_2T;M]$ (Lemma \\ref{lem:extendUmodif}). Then $(R_3^\\wedge)_{P_2}=T[\\fp_2T;M]\\otimes_T A=A[\\fp_2A;M\\otimes_T A]\\supseteq A[\\fp_2A;N]=A[P_2A;N]=A[e]$, where we used compatibilities in Discussions \\ref{discu:DefR+M} and \\ref{discu:R+MdifferentIdeals}. In particular, $(R_3^\\wedge)_{P_2}$ is not local, so $P_2$ has more than $1$ preimages in $\\Spec(R_3^\\wedge)$, so $P$ has more preimages in $\\Spec(R_3^\\wedge)$ than in $\\Spec(R_2^\\wedge)$, contradicting maximality. \\end{proof} The following result could have been established along the way; we derive it formally from the theorem. \\begin{Cor}\\label{cor:noFONSIisNS2integral} Let $R$ be a Noetherian integral domain. Then $\\OS{R}=\\emptyset$ if and only if $R^{n\\sigma}$ is integral over $R$. In particular, if $\\OS{R}=\\emptyset$, then $\\OS{W^{-1}R}=\\emptyset$ for all multiplicative subsets $W$ of $R$. \\end{Cor} \\begin{proof} The ``in particular'' statement follows from Lemma \\ref{lem:NS2localization}. That $\\OS{R}=\\emptyset$ implies $R^{n\\sigma}$ being integral over $R$ is Theorem \\ref{thm:NS2ofGOONSfree}\\ref{NS2:integral}. Assume $\\OS{R}\\neq\\emptyset$. We show $R^{n\\sigma}$ is not integral over $R$. By Lemma \\ref{lem:NS2localization} we may assume $R$ is local. Let $R'$ be as in Theorem \\ref{thm:killFONSI}. Let $P\\in \\OS{R}$ and let $P_0\\in\\Min(R^\\wedge)$ be contained in $P$ so that $\\HT(P/P_0)=1$. Write $\\fp=P\\cap R$, so $\\HT(\\fp)>1$. Note that $R^\\wedge\\to R'^\\wedge$ is finite injective. Let $P_0'\\in\\Min(R'^\\wedge)$ be above $P_0$. Let $P'\\in V(P_0')$ be above $P$. As $R^\\wedge$ is universally catenary, we have $\\HT(P'/P_0')=\\HT(P/P_0)=1$. As $\\OS{R'}=\\emptyset$, we have $\\HT(\\fp')\\leq 1$, where $\\fp'=P'\\cap R'$."}
{"input": "be a Noetherian integral domain, $S$ a sub-$R$-algebra of $R^\\nu$. Then $S^\\sigma$ is $(S_2)$. \\end{Thm} \\begin{proof} By Lemmas \\ref{lem:NS2localization} and \\ref{lem:S2localization}, we may assume $R$ is local. Let $\\Sigma=\\{\\fq\\in\\Spec_1(S)\\mid \\HT(\\fq\\cap R)>1\\}$. Then by Lemmas \\ref{lem:NoFonsiht1Preim} and \\ref{lem:FONSIfinite} and Theorem \\ref{thm:Krull}, $\\Sigma$ is finite. We may therefore find a finite $R$-subalgebra $R'$ of $S$ such that $\\HT(\\fq\\cap R')=1$ for all $\\fq\\in\\Sigma$, Lemma \\ref{lem:MinToAss}. For all $\\fq\\in\\Spec_1(S)\\setminus\\Sigma$, $\\HT(\\fq\\cap R)=1$, so $\\HT(\\fq\\cap R')=1$ as $R\\to R'$ is integral. Therefore $\\HT(\\fq\\cap R')=1$ for all $\\fq\\in\\Spec_1(S)$, consequently $\\HT(\\fq\\cap R'')=1$ for all $\\fq\\in\\Spec_1(S)$ and all $R''\\in \\cR:=$ the set of all finite sub-$R'$-algebras of $S$. As in the proof of Corollary \\ref{cor:NS2ofGOONSfreeS}, we see $S^{n\\sigma}=\\bigcup_{R''\\in\\cR}R''^{n\\sigma}$, so $S^{\\sigma}=\\bigcup_{R''\\in\\cR}R''^{\\sigma}$ as $S^\\nu=R''^\\nu=R^\\nu$ for all $R''\\in\\cR$. Again, as in the proof of Corollary \\ref{cor:NS2ofGOONSfreeS}, this union is filtered, and we conclude by Lemma \\ref{lem:S2filteredunion} and Theorem \\ref{thm:RsIsS2}."}
{"input": "\\section{Semi-Nagata rings} \\label{sec:SagataRings} \\begin{Def}\\label{def:semi-Nagata} A ring $R$ is \\emph{semi-Nagata} if $R$ is Noetherian and, for all finite ring maps $R\\to B$ where $B$ is an integral domain, there exists a finite inclusion $B\\subseteq C$ of integral domains such that $C$ is $(S_2)$. \\end{Def} We will show that we can take $C$ inside $B^\\sigma$, Theorem \\ref{thm:semi-Nagatacharacterize}. Therefore Definition \\ref{def:semi-Nagata} is equivalent to the definition given in the introduction. \\begin{Rem}\\label{Rem:1dimsemi-Nagata} A one-dimensional Noetherian ring is semi-Nagata, as we can take $C=B$. % This fact is nontrivial only in view of the result that \\end{Rem} \\begin{Rem}\\label{Rem:Nagatasemi-Nagata} A Nagata ring is semi-Nagata, as we can take $C$ to be the normalization of $B$. \\end{Rem} \\begin{Rem}[cf. {\\cite{Greco-excellent-finite}}]\\label{Rem:finiteInclsemi-Nagata} Let $R\\to R'$ be a finite map of Noetherian rings. If $R$ is semi-Nagata, so is $R'$; if $\\Spec(R')\\to\\Spec(R)$ is surjective and $R'$ is semi-Nagata, so is $R$. %then $R$ is semi-Nagata if and only if $R'$ is. This is trivial from our definition. \\end{Rem} \\begin{Rem}\\label{Rem:localizesemi-Nagata} For a Noetherian ring $R$, a multiplicative subset $W$ of $R$, and a finite ring map $W^{-1}R\\to C$ where $C$ is an integral domain, there exists a finite ring map $R\\to B$ where $B$ is an integral domain and $W^{-1}B=C$. Indeed, let $B_0$ be the integral closure of $R$ in $C$. Then $W^{-1}B_0=C$, so $W^{-1}B=C$ for some finite subalgebra $B$. This tells us a localization of a semi-Nagata ring is semi-Nagata. \\end{Rem} Following Grothendieck \\citestacks{0BIR}, we say a Noetherian ring $R$ is an \\emph{$(S_1)$-ring} if all formal fibers of $R$ are $(S_1)$."}
{"input": "We use the fact that the property $(S_1)$ satisfies the axiomatic properties \\citestacks{0BIY}. An essentially finitely generated algebra over an $(S_1)$-ring is an $(S_1)$-ring, \\citestacks{0BIV}. \\begin{Lem}\\label{lem:semi-NagataIsS1ring} A semi-Nagata ring is an $(S_1)$-ring. \\end{Lem} \\begin{proof} % Remark \\ref{Rem:localizesemi-Nagata} Let $R$ be a semi-Nagata local ring. We need to show the fibers of $R\\to R^\\wedge$ are $(S_1)$. This is enough by Remark \\ref{Rem:localizesemi-Nagata}. By Noetherian induction we may assume this is true for all proper quotients of $R$. If $R$ were not an integral domain we are done, so we may assume $R$ is an integral domain. Then there exist a finite inclusion $R\\subseteq R'$ of integral domains so that $R'$ is $(S_2)$. Let $x\\in R^\\circ$ be a noninvertible element. Then $R'/xR'$ is $(S_1)$. %it is also an $(S_1)$-ring, As the fibers of $R/xR\\to (R/xR)^\\wedge$ are $(S_1)$ by the induction hypothesis, so are the fibers of $R'/xR'\\to (R'/xR')^\\wedge$. Therefore $(R'/xR')^\\wedge$ is $(S_1)$ \\citestacks{0339}, so $R'^\\wedge$ is $(S_1)$ \\cite[Proposition 3.4.4]{EGA4_2}. As $R\\subseteq R'$ is a finite inclusion of Noetherian semilocal domains we see $R^\\wedge$ is $(S_1)$ (Lemma \\ref{lem:finiteInclMinoverMinAssoverAss}). \\end{proof} \\begin{Lem}\\label{lem:completeS1finiteS2ify} Let $R$ be a Noetherian semilocal domain such that $R^\\wedge$ is $(S_1)$. Then there exists a finite sub-$R$-algebra of $R^\\sigma$ that is $(S_2)$. \\end{Lem} \\begin{proof} By Theorem \\ref{thm:killFONSI} we can find a finite sub-$R$-algebra $R'$ of $R^\\sigma$ so that $\\OS{R'}=\\emptyset$. $R'^\\wedge$ is $(S_1)$ by Lemma \\ref{lem:finiteInclMinoverMinAssoverAss}. Therefore $R'^\\sigma$ is a finite $R'$-algebra inside $R^\\sigma$ (Lemma \\ref{lem:reverseIncl}) that is $(S_2)$ (Theorem \\ref{thm:NS2ofGOONSfree}). \\end{proof} \\begin{Thm}\\label{thm:semi-Nagatacharacterizelocal} Let $R$ be a Noetherian semilocal ring."}
{"input": "are equivalent. \\begin{enumerate}[label=$(\\roman*)$] \\item\\label{semi-Nagatalocal:semi-Nagata} $R$ is semi-Nagata. \\item\\label{semi-Nagatalocal:finiteS2ify} For every finite ring map $R\\to B$ where $B$ is an integral domain, there exists a finite sub-$B$-algebra $C$ of $B^\\sigma$ that is $(S_2)$. \\item\\label{semi-Nagatalocal:S1ring} $R$ is an $(S_1)$-ring. \\end{enumerate} \\end{Thm} \\begin{proof} Lemma \\ref{lem:semi-NagataIsS1ring} gives \\ref{semi-Nagatalocal:semi-Nagata} implies \\ref{semi-Nagatalocal:S1ring}, whereas \\ref{semi-Nagatalocal:finiteS2ify} trivially implies \\ref{semi-Nagatalocal:semi-Nagata}. Finally, to see \\ref{semi-Nagatalocal:S1ring} implies \\ref{semi-Nagatalocal:finiteS2ify}, we may replace $R$ by $B$ and assume $R=B$ is an integral domain. Then $R^\\wedge$ is $(S_1)$ by \\citestacks{0339}, so \\ref{semi-Nagatalocal:finiteS2ify} follows from Lemma \\ref{lem:completeS1finiteS2ify}. \\end{proof} \\begin{Exam}\\label{exam:semi-NagataRsigmaNotFinite} It is possible that $R^\\sigma$ is not finite over $R$, even when $R$ is semi-Nagata. As in Example \\ref{exam:LechGOONS}, there is a Noetherian local domain $(R,\\fm)$ of dimension $2$ such that $R^\\wedge\\cong k[[x,y,z]]/(x^2,y^2)\\cap (z)$, where $k$ is a field. Then $k[[x,y,z]]/(x^2,y^2)\\times k[[x,y]]$ is a finite $D(\\fm R^\\wedge)$-modification of $R^\\wedge$, thus isomorphic to $R'^\\wedge$ where $R'$ is a finite $D(\\fm)$-modification of $R$ \\citetwostacks{0ALK}{05EU}. By \\cite[Proposition 6.3.8]{EGA4_2} the Cohen--Macaulay ring $R'$ is an $(S_1)$-ring, so $R'$ is semi-Nagata, thus so is $R$. On the other hand, for the maximal ideal $\\fm'$ of $R'$ of height $1$, the normalization $T$ of $R'_{\\fm'}$ is contained in a localization of $R^\\sigma$, as $\\HT(\\fm'\\cap R)=2$. Since $T$ is not finite over $R'_{\\fm'}$ (otherwise the nonreduced ring $R'^\\wedge_{\\fm'}\\cong k[[x,y,z]]/(x^2,y^2)$ will be a subring of a finite product of DVRs), we see $R^\\sigma$ is not finite over $R$. We remark that the ring $R$ in Example \\ref{exam:LechGOONS} is also semi-Nagata for the same reason."}
{"input": "the $(S_2)$ locus of $R$, which is open by Lemma \\ref{lem:S2-2rings}, and let $\\fp\\not\\in U$, so $\\HT(\\fp)>1$. %Let $\\fm\\in\\Max(R)$. The local ring $R_\\fp$ is semi-Nagata by Theorem \\ref{thm:semi-Nagatacharacterizelocal}, so by Lemma \\ref{lem:NS2localization} there exists a finite sub-$R$-algebra $R_1$ of $R^\\sigma$ such that $(R_1)_\\fp$ is $(S_2)$. As $R$ is $(S_2)$-2, the $(S_2)$ locus of the ring $R_1$ is open by Lemma \\ref{lem:S2-2rings}. As the constructible topology of $\\Spec(R_1)$ is compact \\citestacks{0901}, there exists $f_1\\in R\\setminus\\fp$ such that $(R_1)_{f_1}$ is $(S_2)$. Since $R_1\\subseteq R^\\sigma$ we know $R=R_1$ over $U$, so the image $Z_1$ of the non-$(S_2)$ locus of $R_1$ in $\\Spec(R)$ is disjoint from $U\\cup D(f_1)$. If $Z_1\\neq\\emptyset$, take $\\fq\\in Z_1$, then similarly the semilocal ring $(R_1)_\\fq$ is semi-Nagata, and we can find $R_2\\subseteq R_1^\\sigma\\subseteq R^\\sigma$ (Lemma \\ref{lem:reverseIncl}) so that the image $Z_2$ of the non-$(S_2)$ locus of $R_2$ in $\\Spec(R)$ is disjoint from $U\\cup D(f_1)\\cup D(f_2)$ and $f_2\\in R\\setminus\\fq$. As the topological space $\\Spec(R)$ is Noetherian, we get our desired $R'$ after finitely many steps. \\end{proof} We include the following argument for a different perspective. \\begin{proof}[Alternative proof of \\ref{semi-Nagata:S1ring} implies \\ref{semi-Nagata:finiteS2ify}] Let $R$ be a Noetherian integral domain that satisfies \\ref{semi-Nagata:S1ring}. We want to show there exists a finite sub-$R$-algebra of $R^\\sigma$ that is $(S_2)$. Let $\\fm\\in\\Max(R)$. Then $R_\\fm$ is semi-Nagata by Theorem \\ref{thm:semi-Nagatacharacterizelocal}, so by Lemma \\ref{lem:NS2localization} there exists a finite sub-$R$-algebra $R(\\fm)$ of $R^\\sigma$ such that $(R(\\fm))_\\fm$ is $(S_2)$. As $R$ is $(S_2)$-2, the $(S_2)$ locus of the ring $R(\\fm)$ is open (Lemma \\ref{lem:S2-2rings})."}
{"input": "that if $\\varphi:A\\to B$ is a $\\bP$-map of Noetherian rings, then $A\\in\\cD^\\bQ_{\\bP-1}$ implies $B\\in\\cD^\\bQ_{\\bP-1}$; if, further, $\\varphi$ is faithfully flat, then $B\\in\\cD^\\bQ_{\\bP-1}$ implies $A\\in\\cD^\\bQ_{\\bP-1}$, cf. \\citestacks{02JY}. For a subcategory $\\cC$ of $\\cD^\\bQ_{\\bP-1}$, a \\emph{strictly functorial $\\bP$-assignment on $\\cC$} is an assignment $A\\mapsto \\fc(A)$ for all $A\\in \\cC$ where $\\fc(A)$ is a nonzero ideal of $A$ satisfying $V(\\fc(A))=\\Spec(A)\\setminus U_\\bP(A)$, such that $\\varphi(\\fc(A))B=\\fc(B)$ for all $\\varphi:A\\to B$ in $\\cC$. When $\\bQ$ is the trivial property, that is, every Noetherian ring satisfies $\\bQ$, we write $\\cD_{\\bP-1}$ and $\\cD_{\\bP}$ instead. \\end{Def} \\begin{Rem}\\label{rem:BI-Q-is-reduced} In \\cite{Nishimura-semilocal-lifting,BI-semilocal-lifting}, $\\bQ$=``reduced,'' and $\\fc(A)$ is the unique radical ideal that satisfies $V(\\fc(A))=\\Spec(A)\\setminus U_{\\bP}(A)$. The lifting of $\\bQ$-rings is \\cite{Marot-Nagata-Lift}. As a reduced ring is $(R_0)$ we see $\\fc(A)_\\fq=A_\\fq$ for all $\\fq\\in\\Min(A)$, in particular $\\fc(A)\\neq 0$. % for all $\\fp\\in\\Spec(A)$. We have $\\varphi(\\fc(A))B=\\fc(B)$ for all $\\varphi:A\\to B$ in $\\cD^\\bQ_{\\bP-1}$ as $\\Spec(\\varphi)^{-1}(U_\\bP(A))=U_\\bP(B)$ and as the fibers of $\\varphi$ are reduced, so $\\varphi(\\fc(A))B$ is radical. In our case, we do not have such luxury, and it is necessary to find the assignments case-by-case. We will find assignments on $\\cD^\\bQ_{\\bP}$ for $\\bQ$ trivial and $\\bP$=``$(S_1)$,'' $\\bQ$=``$(S_1)$'' and $\\bP$=``$(S_2)$,'' and $\\bQ$=```$(S_2)$'' and $\\bP$=``$(S_k)$'' $(k\\geq 3)$, ``Gorenstein,'' and ``lci.'' \\end{Rem} \\begin{Thm}\\label{thm:Nishimura-local-lifting-argument} Let $\\bP$, $\\bQ$ be two properties of Noetherian rings so that $\\bP$ implies $\\bQ$ and that both $\\bP$ and $\\bQ$ satisfy \\ref{PisSing}--\\ref{PGroLocalizes}. Assume that there exists a strictly functorial $(\\bP,\\bQ)$-assignment on $\\cD^\\bQ_\\bP$. Let $R$ be a Noetherian semilocal ring, $I$ an ideal of $R$. Assume \\begin{enumerate} \\item $R$ is $I$-adically complete. \\item\\label{Nsmr:quotPring} $R/I$ is a $\\bP$-ring."}
{"input": "it suffices to show for a primary ideal $Q$ containing $C_n$ we have $C_n\\subseteq (Q\\cap R)R^\\wedge$. If $\\sqrt{Q}$ is maximal then this is trivial as $Q=(Q\\cap R)R^\\wedge$. Therefore we may assume $\\sqrt{Q}$ is not maximal. Let $\\fp=\\sqrt{Q}\\cap R=\\sqrt{Q\\cap R}\\in\\Spec(R)\\setminus\\Max(R)$. As $R\\to R^\\wedge$ is flat and as $Q\\cap R$ is $\\fp$-primary, every prime divisor of $(Q\\cap R)R^\\wedge$ is above $\\fp$. Therefore it suffices to show $C_n (R^\\wedge)_\\fp\\subseteq (Q\\cap R)(R^\\wedge)_\\fp$. In the remainder of the proof we show $C_n (R^\\wedge)_\\fp=\\fa_n (R^\\wedge)_\\fp$ for all $\\fp\\in\\Spec(R)\\setminus\\Max(R)$, which is enough as $Q\\cap R\\supseteq C_n\\cap R=\\fa_n$. Consider the commutative diagram of rings \\[\\begin{CD} R@>>> R^\\wedge\\\\ @VVV @VVV\\\\ R_\\fp @>{f_\\fp}>> (R^\\wedge)_\\fp\\\\ @V{g_\\fp}VV @V{g^\\wedge_\\fp}VV\\\\ (R_\\fp)^* @>{f^*_\\fp}>> ((R^\\wedge)_\\fp)^*.\\\\ \\end{CD}\\] We know $(R^\\wedge)_\\fp$ is (quasi-)excellent as $R^\\wedge$ is complete, therefore $((R^\\wedge)_\\fp)^*$ is quasi-excellent \\cite{formal-lifting-excellence-Gabber}. In particular, both $(R^\\wedge)_\\fp$ and $((R^\\wedge)_\\fp)^*$ are $\\bP$-rings. By \\eqref{Nsmr:locPring} $(R_\\fp)^*$ is also. We know $f_\\fp$ and $g_\\fp$ are $\\bQ$-maps and $g^\\wedge_\\fp$ is a $\\bP$-map by \\eqref{Nsmr:Qring}\\eqref{Nsmr:locPring} and \\citestacks{0BK9}. The map $f^*_\\fp$ is faithfully flat \\citestacks{0AGW}, and for every $\\fQ\\in V(I(R_\\fp)^*)$, the fiber of $f^*_\\fp$ over $\\fQ$ is the same as the formal fiber of $R$ over $\\fQ\\cap R\\in V(I)$, which is geometrically $\\bP$ by \\eqref{Nsmr:quotPring}. As every maximal ideal of $((R^\\wedge)_\\fp)^*$ contains $I((R^\\wedge)_\\fp)^*$, we see from \\ref{PGroLocalizes} that $f^*_\\fp$ is a $\\bP$-map. Consequently, if we remove $R$ and $R_\\fp$, then the diagram above is a diagram in $\\cD^\\bQ_\\bP$ (cf. \\eqref{Nsmr:RhatisQ}). Therefore $C((R^\\wedge)_\\fp)^*=\\fc(((R^\\wedge)_\\fp)^*)=\\fc((R_\\fp)^*)((R^\\wedge)_\\fp)^*$. Now, let $\\fb=(\\fc((R_\\fp)^*)+I^n(R_\\fp)^*)\\cap R_\\fp$, so $\\fb(R_\\fp)^*=\\fc((R_\\fp)^*)+I^n(R_\\fp)^*$ as $(R_\\fp)^*$ is the $I$-adic completion of $R_\\fp$."}
{"input": "both sides contain $I^n (R^\\wedge)_\\fp$. Contract to $R$ we see $\\fb=\\fa_n R_\\fp$, so $\\fa_n (R^\\wedge)_\\fp=C_n(R^\\wedge)_\\fp$, as desired. \\end{proof} \\begin{Rem} We used \\cite{formal-lifting-excellence-Gabber} to ensure the ring $((R^\\wedge)_\\fp)^*$ is a $\\bP$-ring. A weaker result may be enough. If lci implies $\\bP$, then every lci ring is a $\\bP$-ring (cf. \\cite[(5.4)]{avramov-ci}). The ring $((R^\\wedge)_\\fp)^*$ is a quotient of a regular ring as $R^\\wedge$ is, so it is a $\\bP$-ring, avoiding \\cite{formal-lifting-excellence-Gabber}. This is the case in our applications. We needed to do this because our $\\fc(-)$ is only defined on $\\cD^\\bQ_{\\bP}$. In \\cite{Nishimura-semilocal-lifting,BI-semilocal-lifting}, $\\fc(-)$ is defined on the whole of $\\cD^\\bQ_{\\bP-1}$ (Remark \\ref{rem:BI-Q-is-reduced}), so this is unnecessary."}
{"input": "\\section{Extending $\\bP$-assignments} \\label{sec:ExtendAssignment} \\begin{Def}\\label{def:CategoryCompleteQP} Let $\\bP,\\bQ$, $\\cD^\\bQ_\\bP$ be as in Definition \\ref{def:CategoryDQP}. For an integer $d\\geq 0$ let $^d\\cA^\\bQ_\\bP$ be the full subcategory of $\\cD^\\bQ_\\bP$ of rings $A\\in\\cD^\\bQ_\\bP$ that are complete local of dimension $d$ whose $\\bP$-locus is the punctured spectrum. % (\\emph{i.e.} $U_\\bP(A)=\\Spec(A)\\setminus\\Max(A)$). Let $\\cA^\\bQ_\\bP$ be the disjoint union of all $^d\\cA^\\bQ_\\bP$. In other words, the objects are $\\cA^\\bQ_\\bP$ are complete local rings in $\\cD^\\bQ_\\bP$ whose $\\bP$-locus is the punctured spectrum, and the morphisms are local $\\bP$-maps whose closed fiber has dimension $0$. For every $A\\in \\cA^\\bQ_\\bP$, denote by $\\fm_A$ the maximal ideal of $A$. We know $\\Spec(A)\\setminus U_\\bP(A)=\\{\\fm_A\\}$. Therefore an ideal $\\fc$ satisfying $V(\\fc)=\\Spec(A)\\setminus U_\\bP(A)$ is the same as $\\fc$ being $\\fm_A$-primary. When $\\bQ$ is trivial we write $^d\\cA_\\bP$ and $\\cA_\\bP$ instead. \\end{Def} \\begin{Lem}\\label{lem:extendPassignment} Let $\\bP,\\bQ$, $\\cD^\\bQ_\\bP,\\cA^\\bQ_\\bP$ be as in Definitions \\ref{def:CategoryDQP} and \\ref{def:CategoryCompleteQP}. Assume that $\\bP$ implies $(S_1)$. Then every strictly functorial $\\bP$-assignment $\\fc(-)$ on $\\cA^\\bQ_\\bP$ extends uniquely to a strictly functorial $\\bP$-assignment $\\fc(-)$ on $\\cD^\\bQ_\\bP$ in a way that $\\fc(A)$ has no embedded prime divisors for all $A\\in\\cD^\\bQ_\\bP$. \\end{Lem} \\begin{proof} Let $\\fc(-)$ on $\\cA^\\bQ_\\bP$ be a given strictly functorial $\\bP$-assignment. For $A\\in \\cD^\\bQ_{\\bP}$, let $\\fp_1,\\ldots,\\fp_n\\ (n\\geq 0)$ be the generic points of $\\Spec(A)\\setminus U_\\bP(A)$. If a desired extension exists, then it must satisfy $\\fc(A)=\\bigcap_i(\\fc(A_{\\fp_i})\\cap A)$ as $\\fc(A)$ has no embedded prime divisors. Furthermore, we must have $\\fc(A_{\\fp_i})=\\fc(A^\\wedge_{\\fp_i})\\cap A_{\\fp_i}$, as the completion map $A_{\\fp_i}\\to A^\\wedge_{\\fp_i}$ is in $\\cD^\\bQ_\\bP$ (\\emph{i.e.} a $\\bP$-map) by condition \\eqref{DisPring} in Definition \\ref{def:CategoryDQP}."}
{"input": "see $U_\\bP(A_{\\fp_i})=D(\\fp_iA_{\\fp_i})$, therefore $U_\\bP(A^\\wedge_{\\fp_i})=D(\\fp_iA^\\wedge_{\\fp_i})$, in other words $A^\\wedge_{\\fp_i}\\in \\cA^\\bQ_\\bP$. This shows the uniqueness of the extension; we must have $\\fc(A)=\\bigcap_i\\fc(A^\\wedge_{\\fp_i})\\cap A$. It remains to verify that $\\fc(A):=\\bigcap_i\\fc(A^\\wedge_{\\fp_i})\\cap A$ is indeed a strictly functorial $\\bP$-assignment; by construction it has no embedded prime divisors as each $\\fc(A^\\wedge_{\\fp_i})\\cap A$ is $\\fp_i$-primary. It is clear that $V(\\fc(A))=\\Spec(A)\\setminus U_\\bP(A)$. We have $\\fc(A)\\neq 0$ as $\\fc(A)=A$ when $n=0$, and $\\fc(A)_{\\fp_1}=\\fc(A^\\wedge_{\\fp_1})\\cap A_{\\fp_1}\\neq 0$ when $n>0$, as $\\fc(A^\\wedge_{\\fp_1})$ is nonzero and $(\\fp_1A^\\wedge_{\\fp_1})$-primary. It remains to show for $\\varphi:A\\to B$ in $\\cD^\\bQ_\\bP$, we have $\\fc(A)B=\\fc(B)$, where $\\varphi$ is omitted in the notation. Let $\\fq_{ij}\\ (1\\leq j\\leq m_i)$ be the minimal prime divisors of $\\fp_i B$, where $m_i\\geq 0$. As $\\varphi$ is a $\\bP$-map, $\\Spec(\\varphi)^{-1}(U_\\bP(A))=U_\\bP(B)$, so $\\fq_{ij}\\ (1\\leq j\\leq m_i,1\\leq i\\leq n)$ are exactly the generic points of $\\Spec(B)\\setminus U_\\bP(B)$. Moreover, as $\\bP$ implies $(S_1)$, we see for $\\fc_i:=\\fc(A^\\wedge_{\\fp_i})\\cap A$, $\\Ass_{B}(B/\\fc_iB)=\\{\\fq_{ij}\\mid 1\\leq j\\leq m_i\\}$. Therefore it suffices to show $\\fc_iB_{\\fq_{ij}}=\\fc(B^\\wedge_{\\fq_{ij}})\\cap B_{\\fq_{ij}}$, and as both sides are $\\fq_{ij}$-primary, passing to the completion we see it suffices to show $\\fc(A^\\wedge_{\\fp_i})B^\\wedge_{\\fq_{ij}}=\\fc(B^\\wedge_{\\fq_{ij}})$. We know $A^\\wedge_{\\fp_i},B^\\wedge_{\\fq_{ij}}\\in {^d\\cA^\\bQ_\\bP}$ for $d:=\\HT(\\fp_i)=\\HT(\\fq_{ij})$, therefore, as our $\\fc(-)$ is strictly functorial on $\\cA^\\bQ_\\bP$, it suffices to show $A^\\wedge_{\\fp_i}\\to B^\\wedge_{\\fq_{ij}}$ is a $\\bP$-map. By \\ref{PGroLocalizes} it suffices to show $\\kappa(\\fp_i)\\to (B/\\fp_iB)^\\wedge_{\\fq_{ij}}$ is a $\\bP$-map. This follows from the fact $\\varphi:A\\to B$ is a $\\bP$-map and the fact $B/\\fp_iB$, a quotient of $B\\in\\cD^\\bQ_\\bP$, is a $\\bP$-ring."}
{"input": "\\section{$(S_k)$-, Cohen--Macaulay-, and Gorenstein-assignments} \\label{sec:SkCMGorAssign} \\begin{Lem}\\label{lem:S1assignment} Let $\\bP$=``$(S_1)$.'' Then $\\fc(A)=\\Ann_A(H^0_{\\fm_A}(A))$ is a strictly functorial $\\bP$-assignment on $\\cA_\\bP$. \\end{Lem} \\begin{proof} It is clear that $\\fc(A)$ is $\\fm_A$-primary and $\\fc(-)$ is strictly functorial, as the closed fiber of all maps in $\\cA_{\\bP}$ have dimension $0$. As an Artinian ring is $(S_1)$ we see $^0\\cA^\\bQ_\\bP=\\emptyset$, so $\\dim A>0$ and any $\\fm_A$-primary ideal is nonzero. \\end{proof} \\begin{Discu}\\label{discu:cofS2} Let $\\bQ$=``$(S_1)$'' and $\\bP$=``$(S_2)$.'' For $A\\in \\cA^\\bQ_\\bP$, let $\\Sigma_1$ be the set of primary components $Q$ of $0$ so that $\\dim(A/Q)=1$, and let $\\Sigma_2$ be the set of primary components $Q$ of $0$ so that $\\dim(A/Q)>1$. Note that $A$ is $(S_1)$ and not $(S_2)$, so $0$ has no embedded primes and $\\dim A>1$, therefore primary components of $0$ are uniquely determined and $\\Sigma_2\\neq\\emptyset$. Let $A_1=A/\\bigcap_{Q\\in\\Sigma_1}Q,A_2=A/\\bigcap_{Q\\in\\Sigma_2}Q$. Let $U_2$ be the punctured spectrum of $A_2$ and let $A_2'=\\cO(U_2)$. As $\\dim(A/Q)>1$ for all $Q\\in\\Sigma_2$, similar to Lemma \\ref{lem:naiveS2finiteforcomplete} (cf. \\cite[Lemma 2.11]{Macaulay-Cesnavi}) we have $A_2'$ is finite over $A_2$. This gives a finite birational ring map $A\\to A_1\\times A'_2$. Let $\\fc(A)$ be the conductor of this map. Let $U$ (resp. $U_1$) be the punctured spectrum of $A$ (resp. $A_1$). Then we have $U=U_1\\sqcup U_2$. Therefore $\\fc(A)$ is either $\\fm_A$-primary or $A$. As $U$ is $(S_2)$ we have $U_2$ is $(S_2)$, so $A_2'$ is $(S_2)$ by \\cite[Th\\'eor\\`eme 5.10.5]{EGA4_2}. In particular $A\\neq A_1\\times A_2'$ as $A$ is local and not $(S_2)$, so $\\fc(A)$ is $\\fm_A$-primary. A maximal ideal (resp. minimal prime) of $A_2'$ lies above $\\fm_A$ (resp."}
{"input": "\\section{A lci-assignment} \\label{sec:lciAssign} Let $\\bQ$=``$(S_2)$'' and $\\bP$=``lci.'' Let $A\\in \\cA^\\bQ_\\bP$. Intuitively, we want to define $\\fc(A)$ to be the Fitting invariant of modules $C_n(A/R)$ showing up in \\cite{Briggs-Iyenger-Cotangent-Complex}, where $R$ is a regular local ring mapping surjectively to $A$. The flatness of $C_n(A/R)$ characterizes lci. However, these modules depend on the choice of $R$ and a projective resolution (in a way that does not change the Fitting invariant, however), and are fragile along ascent (\\emph{i.e.} still involve non-finite modules). We will work with $C_n(A/\\bZ)$ instead, which gives the same Fitting invariant. We use standard notations for derived categories, and cohomological conventions for cotangent complexes, as in \\cite{stacks}. \\begin{Discu}\\label{discu:modulesC} Let $A$ be a ring, and let $L\\in D^{-}(A)$. For every bounded above complex of projectives $P^\\bullet$ that represents $L$, we consider the module $C^a(P^\\bullet)=H^a(\\sigma_{\\leq a}P^\\bullet)$, where $\\sigma_{\\leq a}$ is the stupid truncation \\citestacks{0118}. In other words, $C^a(P^\\bullet)$ is the cokernel of the map $P^{a-1}\\to P^a$, which is the module appearing at degree $a$ in $\\tau_{\\geq a}(P^\\bullet)$. There is an obvious compatibility with shift and base change. The collection of all such $C^a(P^\\bullet)$ is denoted $\\cC^a(L)$. For $X,Y\\in\\cC^a(L)$, there exist projective modules $P,Q$ so that $X\\oplus P\\cong Y\\oplus Q$, see \\cite[(7.2)]{Briggs-Iyenger-Cotangent-Complex}. We write $C^a(L)$ for an unspecified element in $\\cC^a(L)$. The flat and projective dimensions of $C^a(L)$ are well-defined. If $L$ has tor-amplitude in $[a,b]$, then $C^a(L)$ is flat. Indeed, let $P^\\bullet$ represent $L$, %with $P^{>b}=0$, then $\\tau_{\\geq a}(P^\\bullet\\otimes_A M)$ represents $L\\otimes_A^L M$ for all $A$-modules $M$, as $L\\otimes_A^L M\\in D^{[a,b]}(A)$."}
{"input": "up to projective summands, as $L_{A/R}\\in D_{Coh}(A)$ \\citestacks{08PZ}. \\end{proof} \\begin{Rem}\\label{rem:ImproveD+1to2} If $A$ is countable, then we can improve $-\\dim A-1$ to $-2$, see \\cite[Seconde partie, Corollaire 3.3.2]{Raynaud}. We could, if necessary, work extensively with countable rings, via a L\\\"owenheim--Skolem type argument, cf. \\cite{Lyu-elementary-subring}. \\end{Rem} We would like to define the Fitting invariant of $C^a(L_{A/\\bZ})$ to be that of $M$; we will show this is well-defined. Before that, note the following variant of the main theorem of \\cite{Briggs-Iyenger-Cotangent-Complex}. \\begin{Thm}\\label{thm:lciandCotangentModule} Let $R$ be a Noetherian lci ring and let $A$ be a finitely generated $R$-algebra of finite tor dimension as an $R$-module. %with $\\dim A<\\infty$. Let $a\\in\\bZ,a<-1$. Then $A$ is lci if and only if $C^a(L_{A/\\bZ})$ is flat. \\end{Thm} \\begin{proof} Take the same exact sequence as in the proof of Lemma \\ref{lem:StructureofCotangentModule}. We have $H=0$ and $P$ is flat. Thus $C^a(L_{A/\\bZ})$ is flat if and only $C^a(L_{A/R})$ is flat, if and only if $R\\to A$ is lci (\\cite[Theorem B]{Briggs-Iyenger-Cotangent-Complex} and \\cite[(1.2)]{avramov-ci}), if and only if $A$ is lci \\cite[(5.4) and (5.9)]{avramov-ci}. \\end{proof} \\begin{Def}\\label{def:Fitting} Let $A$ be a Noetherian local ring. We say an $A$-module $X$ is \\emph{finite-by-flat} if there exists a finite submodule $M$ of $X$ such that $X/M$ is flat. The \\emph{Fitting invariant of $X$} is defined to be the Fitting invariant of $M$. We say $X$ is \\emph{pseudo-finite-by-flat} if there exists a flat $A$-module $C$ so that $X\\oplus C$ is finite-by-flat. The \\emph{Fitting invariant of $X$} is defined to be the Fitting invariant of $X\\oplus C$."}
{"input": "On the other hand $X_\\alpha\\cong M\\oplus L_\\alpha$ as $A$-modules as $L_\\alpha$ is free. We conclude that $M'$ is isomorphic to a direct summand of $M\\oplus F$ where $F$ is a finite free $A$-module, and by symmetry $M$ is isomorphic to a direct summand of $M'\\oplus F'$ where $F'$ is a finite free $A$-module. If $A$ is complete, it follows from Krull-Schmidt \\cite[Corollary 1.10]{Leuschke-Wiegand-Cohen-Macaulay-Modules} that $M\\oplus P\\cong M'\\oplus P'$ for some finite free $A$-modules $P,P'$. For a general $A$ the same is true by \\cite[Corollary 1.15]{Leuschke-Wiegand-Cohen-Macaulay-Modules}. Therefore the Fitting invariants of $M$ and $M'$ are the same \\citestacks{07ZA}. Given an inclusion of modules $X\\subseteq Y$ with flat quotient, if $M$ is a finite submodule of $X$ so that $X/M$ is flat, then $M$ is a finite submodule of $Y$ so that $Y/M$ is flat, as $Y/M$ is an extension of $X/M$ by $Y/X$. This gives \\ref{Fit:cokerFlatSameFit} in the finite-by-flat case. Next, let $X$ be a pseudo-finite-by-flat module and $C,D$ be flat modules so that $X\\oplus C$ and $X\\oplus D$ are both finite-by-flat. Then $X\\oplus C\\oplus D$ is finite-by-flat and has the same Fitting invariant as $X\\oplus C$ and $X\\oplus D$ by \\ref{Fit:cokerFlatSameFit} for finite-by-flat modules. Therefore the Fitting invariants of $X\\oplus C$ and $X\\oplus D$ are the same, which is \\ref{Fit:wdf} for $X$. Finally, given an inclusion of modules $X\\subseteq Y$ with flat quotient, if $C$ is a flat module, then we have an inclusion of modules $X\\oplus C\\subseteq Y\\oplus C$ with flat quotient. This gives \\ref{Fit:cokerFlatSameFit} in the pseudo-finite-by-flat case."}
{"input": "gives $\\fc(A)B=\\fc(B)$. It remains to show $\\fc(A)$ is $\\fm_A$-primary. When $d=0$ this is trivial, so we assume $d>0$. Let $P^\\bullet$ be a complex of finite free modules that satisfies $P^{>-1}=0$ and represents $L_{A/R}$ \\citetwostacks{08PZ}{08QF}. As seen in Lemma \\ref{lem:StructureofCotangentModule} $\\fc(A)$ is the Fitting invariant of $C^a(P^\\bullet)$. We will show $M:=C^a(P^\\bullet)$ is finite flat of constant rank, say $r$, on the punctured spectrum of $A$. Then by \\citestacks{07ZD}, for $\\fa=\\Fit_j(M)\\ (j<r)$, we have $\\fa_\\fp=0$ for all $\\fp\\in\\Spec(A)\\setminus \\{\\fm_A\\}$, so $\\fa=0$ as $\\depth A\\geq 1$; therefore $\\fc(A)=\\Fit_r(M)$, and $\\fc(A)_\\fp=A_\\fp$. We know $M$ is finite flat on the punctured spectrum of $A$ which is lci. If $d\\geq 2$, then $\\depth A\\geq 2$, as $A$ is $(S_2)$. Therefore the punctured spectrum of $A$ is connected \\citestacks{0BLR}, so the rank is constant. We may now assume $d=1$. Let $I=\\ker(R\\to A)$, $\\fp\\in V(I)\\setminus\\Max(R)$. The complex $(\\tau_{\\geq a} P^\\bullet)_\\fp$ represents $(I/I^2)_\\fp[1]$ \\citestacks{08SJ}, as $A_\\fp$ is lci. Computing Euler characteristic, we see $(-1)^a\\rank M_\\fp + \\sum_{i=a+1}^{-1} (-1)^i\\rank P^i=-\\rank (I/I^2)_\\fp$. We know $\\rank (I/I^2)_\\fp=\\dim R_\\fp-\\dim A_\\fp$, as $I_\\fp$ is generated by a regular sequence. As $d=1$ and as $R$ is a catenary domain, we have $\\dim R_\\fp=\\dim R-1,\\dim A_\\fp=0$, independent of the choice of $\\fp$. Therefore, $\\rank M_\\fp$ is independent of the choice of $\\fp$, as desired."}
{"input": "\\section{Local lifting} \\label{sec:Local-lifting-result} \\begin{Thm}\\label{thm:local-lift-P} Let $\\bP$ be the property ``$(S_k)$'' $(k\\geq 0)$, ``Cohen--Macaulay,'' ``Gorenstein,'' or ``lci.'' Let $R$ be a Noetherian semilocal ring, $I$ an ideal of $R$. Assume \\begin{enumerate} \\item $R$ is $I$-adically complete. \\item\\label{locallift:quotPring} $R/I$ is a $\\bP$-ring. \\end{enumerate} Then $R$ is a $\\bP$-ring. \\end{Thm} \\begin{proof} First consider the case $\\bP$=``$(S_1)$,'' as every Noetherian ring is $(S_0)$. Let $\\bQ$ be the trivial property. A strictly functorial $\\bP$-assignment on $\\cD_\\bP$ exists, Lemmas \\ref{lem:extendPassignment} and \\ref{lem:S1assignment}. The assumptions \\eqref{Nsmr:Qring} and \\eqref{Nsmr:Q-ify} in Theorem \\ref{thm:Nishimura-local-lifting-argument} are trivial, and we conclude. Next, consider the case $\\bP$=``$(S_2)$.'' Let $\\bQ$=``$(S_1)$.'' A strictly functorial $\\bP$-assignment on $\\cD^\\bQ_\\bP$ exists, Lemmas \\ref{lem:extendPassignment} and \\ref{lem:S2assignment}. The assumption %\\eqref{Qring} and \\eqref{Nsmr:Q-ify} in Theorem \\ref{thm:Nishimura-local-lifting-argument} is trivial as a domain in $(S_1)$, whereas \\eqref{Nsmr:Qring} follows from the case $\\bP$=``$(S_1)$,'' and we conclude. Finally, consider the case $\\bP$=``$(S_k)$'' $(k\\geq 3)$, ``Cohen--Macaulay,'' ``Gorenstein,'' or ``lci.'' Let $\\bQ$=``$(S_2)$.'' A strictly functorial $\\bP$-assignment on $\\cD^\\bQ_\\bP$ exists, Lemma \\ref{lem:extendPassignment} and Lemma \\ref{lem:Skassignment}, Remark \\ref{rem:CMassignment}, Lemma \\ref{lem:Gorassignment}, and Theorem \\ref{thm:lci-assignment}. The assumption \\eqref{Nsmr:Qring} in Theorem \\ref{thm:Nishimura-local-lifting-argument} follows from the case $\\bP$=``$(S_2)$,'' and \\eqref{Nsmr:Q-ify} follows from Theorem \\ref{thm:semi-Nagatacharacterizelocal} (or \\cite[Corollary 2.14]{Macaulay-Cesnavi}), and we conclude. \\end{proof} \\begin{Rem}\\label{rem:PisGeometric} All properties $\\bP$ in Theorem \\ref{thm:local-lift-P} are preserved by finite field extensions, so being a $\\bP$-ring is the same as having $\\bP$ formal fibers. This is because a finite extension of fields is a syntomic ring map, cf. \\citestacks{00SK}."}
{"input": "such methods achieve moderate improvements in realism, they still struggle to model complex trajectory interactions accurately and lack guarantees of physical feasibility \\cite{advsim, RN247}. Moreover, their iterative search procedures require substantial computational resources, resulting in low generation efficiency \\cite{advsim, strive}. Recently, diffusion models have shown remarkable performance in generating realistic sequential data, including interactive traffic scenarios that closely mirror real-world driving behaviors \\cite{mao2023leapfrog, niedoba2024diffusion}. The iterative noise-to-data generation paradigm inherent in diffusion models captures complex multi-agent dependencies and enables flexible guidance during inference, thereby facilitating controllable generation \\cite{zhong2023guided,jiang2023motiondiffuser}. Furthermore, recent advancements in latent diffusion models (LDMs) have demonstrated that learning compact and expressive latent representations of driving trajectories facilitates the modeling of their joint distributions, leading to improved realism and diversity in generated traffic scenarios \\cite{xie2024advdiffuser, pronovost2023scenario}, while simultaneously enhancing computational efficiency \\cite{rombach2022high}. Collectively, these works highlight the strong potential of diffusion models in generating realistic and controllable safety-critical scenarios. Inspired by these works, we propose a guided LDM framework for simulating physically realistic and adversarial safety-critical traffic scenarios, as illustrated in Fig~\\ref{fig:overview}. Specifically, we utilize a GNN-based encoder to transform both past and future trajectories into compact latent representations that capture intricate multi-agent interactions. Conditioned on the latent representation of past scene information, our model progressively denoises a noisy future latent to reconstruct realistic driving trajectories. To effectively guide the generation toward adversarial safety-critical scenarios, we introduce novel guidance objectives that progressively perturb the sampling process during inference."}
{"input": "\\section{Related Work} \\label{sec:related work} This section reviews related works in two areas: safety-critical traffic simulation and diffusion-based traffic scenario generation. \\subsection{Safety-Critical Traffic Simulation} Safety-critical traffic simulation is crucial for evaluating autonomous driving systems under rare and high-risk scenarios~\\cite{ding2023survey}. Traditional methods typically depend on simulators such as CARLA~\\cite{dosovitskiy2017carla} and SUMO~\\cite{lopez2018microscopic} to manually design such scenarios by modifying agent states. However, these approaches require significant domain expertise and often yield unrealistic or unscalable designs~\\cite{waymo, metadrive}. To improve realism, recent research has leveraged large-scale driving datasets to learn realistic traffic patterns and generate safety-critical scenarios through optimization-based techniques at test time~\\cite{advsim, strive, mixsim}. In particular, AdvSim~\\cite{advsim} directly perturbs the standard trajectory space to induce adversarial trajectories, while Strive~\\cite{strive} performs adversarial optimization in latent spaces using graph-based representations~\\cite{scarselli2008graph}. MixSim~\\cite{mixsim} learns route-conditioned policies to enable reactive and controllable re-simulation, supporting realistic variations and safety-critical interactions in mixed reality traffic scenarios. Although these approaches achieve moderate improvements in realism, they remain limited by inefficiency and continue to struggle with generating both plausible and controllable driving behaviors, thereby restricting their practicality for large-scale scenario generation. \\begin{figure*}[!h] \\centerline{\\includegraphics[width=\\linewidth, trim= 0 0 0 0, clip]{Fig/framework_v4.pdf}} \\caption{Overall framework of our proposed guided LDM. Graph-based VAEs encode posterior and prior scene input into latent representations, which are denoised by a U-Net conditioned on the prior latent. During the sampling stage, we introduce our proposed guidance objectives to guide the diffusion process toward generating adversarial safety-critical driving scenarios."}
{"input": "to their ability to model complex traffic patterns and generate high-fidelity simulations that closely resemble real-world scenarios. Moreover, they provide enhanced controllability, enabling the customization of traffic scenarios based on specific conditions or guidance \\cite{peng2024diffusion}. For example, DJINN \\cite{niedoba2024diffusion} utilized a classifier-free diffusion model to generate joint interactive trajectories for all agents in a traffic scene, conditioned on a flexible set of agent states. Additionally, several studies have further improved controllability by incorporating guidance mechanisms into the diffusion models during inference time \\cite{zhong2023guided, jiang2023motiondiffuser, xu2023diffscene, chang2024safe, xie2024advdiffuser}. Specifically, CTG \\cite{zhong2023guided} employs Signal Temporal Logic (STL) formulas as guidance for diffusion models to generate rule-compliant trajectories, while MotionDiffuser \\cite{jiang2023motiondiffuser} proposes several differentiable cost functions as guidance, enabling the physical constraints in the generated trajectories. DiffScene \\cite{xu2023diffscene} and Safe-Sim \\cite{chang2024safe} proposed safety-based objective functions to simulate the safety-critical driving scenarios. Other studies have leveraged LDMs to learn more effective representations of driving trajectories and to model the joint distribution over these trajectories \\cite{xie2024advdiffuser, pronovost2023scenario, chen2023executing}. However, these methods still face limitations in terms of controllability and flexibility. For example, AdvDiffuser \\cite{xie2024advdiffuser} cannot be easily applied to generate scenarios for specific adversarial vehicles and requires the training of different classifiers for various adversarial strategies."}
{"input": "denoted as $\\boldsymbol{\\tau} = \\{s^{i}_{t:t+T}\\}^{N-1}_{i=1}$. The model includes an encoder $\\mathcal{E}$, which encodes historical trajectory data and map information into a compact latent representation, and a decoder $\\mathcal{D}$, which decodes the denoised latent into the predicted future trajectories of the non-ego agents. During training, the model learns realistic traffic behaviors from real-world data, and during inference, adversarial objective functions guide the generation of safety-critical scenarios. \\subsection{Latent Diffusion Models for Traffic Simulation} We propose an LDM to generate realistic and controllable adversarial safety-critical driving scenarios through an iterative denoising process. In contrast to conventional diffusion methods that operate directly in trajectory space \\cite{zhong2023guided, zhong2023language, chang2024safe}, our approach performs the denoising process in a latent space, thereby reducing computational overhead while enhancing feature expressiveness \\cite{strive, xie2024advdiffuser, chen2023executing, rombach2022high}. Our model builds upon a pretrained graph-based VAE following Strive \\cite{strive}, where the encoder captures complex multi-agent interactions and the decoder autoregressively generates future trajectories using a kinematic bicycle model to ensure plausibility of the generated trajectories. \\textbf{Architecture.} As illustrated in Fig~\\ref{fig:overall_framework}, our LDM consists of three components: two frozen GNN-based encoders, a learnable U-Net denoising network, and a frozen GNN-based decoder. The prior encoder $\\mathcal{E}_\\theta(\\textit{\\textbf{x}}, \\textit{\\textbf{m}})$ encodes past agent trajectories and local map features to produce the conditioning input $\\textit{\\textbf{c}}$, while the posterior encoder additionally incorporates future trajectories to yield the latent variable $\\mathbf{z}$."}
{"input": "\\beta_k} \\mathbf{z}^{k-1}, \\beta_k \\mathbf{I} \\right) \\end{equation} where $\\beta_k$ denotes the predefined variance schedule controlling the noise level at each step. For a sufficiently large number of steps $K$, the distribution of $\\mathbf{z}^K$ converges to an isotropic Gaussian, i.e., $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. To accelerate inference, we adopt the Denoising Diffusion Implicit Models (DDIM) sampling strategy \\cite{song2020denoising}, which enables non-Markovian reverse diffusion and supports efficient sampling by skipping intermediate steps without requiring retraining. The reverse process is defined as: \\begin{equation} \\mathbf{z}^{k-1} = \\sqrt{\\alpha_{k-1}} \\cdot \\tilde{\\mathbf{z}}^0 + \\sqrt{1 - \\alpha_{k-1}} \\cdot \\epsilon_\\theta(\\mathbf{z}^k, k, \\mathbf{c}) \\label{denoising_process} \\end{equation} where $\\epsilon_\\theta(\\mathbf{z}^k, k, \\textit{\\textbf{c}})$ denotes the noise prediction model conditioned on $\\textit{\\textbf{c}}$, and $\\alpha_k = \\prod_{i=1}^{k} (1 - \\beta_i)$ represents the cumulative product of the noise schedule. The predicted clean latent $\\tilde{\\mathbf{z}}^0$ is estimated as: \\begin{equation} \\tilde{\\mathbf{z}}^0 = \\left( \\frac{\\mathbf{z}^k - \\sqrt{1 - \\alpha_k} \\cdot \\epsilon_\\theta(\\mathbf{z}^k, k, \\mathbf{c})}{\\sqrt{\\alpha_k}} \\right) \\end{equation} Iteratively applying this reverse process starting from $\\mathbf{z}^K$ yields the final denoised latent $\\hat{\\mathbf{z}}^0$. Finally, the decoder $\\mathcal{D}_\\theta(\\hat{\\mathbf{z}}^0, \\textit{\\textbf{x}}, \\textit{\\textbf{m}})$ autoregressively generates agent actions based on the denoised latent and the historical context, with the generated actions propagated through a kinematic bicycle model to ensure the physical plausibility of the resulting trajectories. \\textbf{Training."}
{"input": "$i$ and $j$ at time $t$, and $p_{ij} = r_i + r_j + d_{\\text{buffer}}$ represents the collision threshold determined by the sum of the vehicle radii and a predefined safety buffer. Similarly, the map collision penalty is formulated as: \\begin{equation} \\text{env\\_coll\\_pens}_{i}(t) = \\begin{cases} 1 - \\frac{d_{i}(t)}{p_{i}}, & \\text{if } d_{i}(t) \\leq p_{i} \\\\ 0, & \\text{otherwise} \\end{cases} \\end{equation} where $d_{i}(t)$ denotes the distance from the vehicle center to the nearest non-drivable area at time $t$, and $p_{i}$ denotes the maximum allowable displacement before a collision. In practice, $\\mathcal{J}_{br}$ and $\\mathcal{J}_{ar}$ are computed by accumulating the respective vehicle collision and map collision penalties over all relevant agents and timesteps. In contrast, the adversarial objective is defined as: \\begin{equation} \\mathcal{J}_{\\mathrm{adv}}(\\boldsymbol{\\tau}) = \\sum_{t=1}^{T} min(0,\\ d(t)\\;-\\;p) \\end{equation} where \\(d(t)\\) is the current center-to-center distance between the adversarial vehicle and ego vehicle at time \\(t\\), and \\(p\\) denotes the collision threshold between the adversarial vehicle and the ego vehicle. Accordingly, these three guidance objectives are computed separately for non-adversarial and adversarial vehicles, with corresponding latent variables updated independently. Specifically, the gradient of $\\mathcal{J}_{br}$ is applied to the latent representations of non-adversarial vehicles, while the gradient of the combined objective comprising $\\mathcal{J}_{ar}$ and $\\mathcal{J}_{adv}$ is used to update the latent variables corresponding to the adversarial vehicle, thereby ensuring targeted control over its behavior. \\textbf{Sample Selection Module."}
{"input": "FDD values indicating greater scenario variation. Additionally, we employ metrics such as Average Displacement Error (ADE), Final Displacement Error (FDE), and minimum Scenario Final Displacement Error (minSFDE) to measure overall trajectory accuracy, along with inference time to evaluate generation efficiency. \\subsection{Experimental Setup} \\textbf{Baselines.} We compare our method with two representative baselines. AdvSim\\cite{advsim} optimizes the acceleration of a predefined adversarial vehicle to induce collisions, with initial states generated by SimNet\\cite{bergamini2021simnet}. Strive~\\cite{strive}, using its official implementation, conducts adversarial optimization in a learned latent space based on a traffic model. \\textbf{Implementation Details.} Our model is implemented in PyTorch and trained on four NVIDIA RTX 4090 GPUs for six hours. The diffusion model is trained for 200 epochs using Adam with a learning rate of $5 \\times 10^{-4}$, 20 diffusion steps, and 10 test samples. To ensure fair comparison in controllability, the adversarial vehicle is selected as the one closest to the ego vehicle in the initial state and must satisfy feasibility constraints \\cite{strive}. Unlike Strive, where the adversarial vehicle may change dynamically, we fix the selected adversarial agent throughout the scenario. Moreover, in all experiments, the ego vehicle is controlled by a rule-based planner to ensure consistency across different methods. \\subsection{Simulating Real Traffic} To evaluate the effectiveness of our LDM in simulating real-world traffic scenarios, we conduct comparative experiments on the nuScenes dataset. Specifically, we compare our unguided diffusion-based model against two representative baselines: the VAE-based Strive model and the imitation learning-based AdvSim model, both evaluated without adversarial optimization."}
{"input": "and Strive (609.72 s). % Although diffusion-based generation is not lightweight, it provides a more practical solution compared to optimization-heavy baselines. Overall, our approach achieves strong adversarial performance and concurrently maintains high level realism and generation efficiency, making it well-suited for large-scale safety validation of AVs. \\subsection{Ablation Study} \\begin{figure}[t] \\centering \\includegraphics [width=\\linewidth, trim=5 10 0 0, clip]{Fig/ablation_study.png} \\caption{Ablation study on guidance components. \\textit{Reply} refers to simulation using replayed non-ego vehicle trajectories from the dataset. \\textit{W/O Guidance} denotes our diffusion-based model without guidance, and \\textit{W/ Guidance} represents our proposed guidance-based diffusion model.} \\label{fig:ablation_study} \\end{figure} To investigate the effectiveness of the proposed guidance mechanism, we perform an ablation study comparing three settings: \\textit{Reply}, which replays the original non-ego vehicle trajectories from the dataset; \\textit{W/O Guidance}, which runs our diffusion-based model without guidance; and \\textit{W/ Guidance}, which includes the proposed guidance module in the diffusion process. As shown in Fig.~\\ref{fig:ablation_study}, the \\textit{Reply} setting yields minimal adversarial effectiveness, with a near-zero \\textit{Adv-Ego Collision Rate}, as agents simply replay dataset trajectories. The results of \\textit{W/O Guidance} model demonstrate a strong ability to simulate realistic traffic scenarios, achieving plausible vehicle behaviors and moderate off-road rates. With the integration of the guidance module, the \\textit{W/ Guidance} model further improves adversariality, achieving the highest Adv-Ego Collision Rate, demonstrating the effectiveness of adversarial guidance. Moreover, Adv Offroad and Other Offroad are slightly reduced, indicating that the real-based guidance contributes to improving the realism of generated scenarios without degrading adversarial effectiveness."}
{"input": "$X\\rtimes_{r,\\alpha}G$ is isomorphic to the corresponding reduced crossed product $\\O_X \\rtimes_{r,\\alpha} G$ of the Cuntz-Pimsner algebra of $X$. The Hao-Ng isomorphism problem was first considered around 17 years ago by Hao and Ng \\cite{hao2008crossed} where they established the validity of the isomorphism described above for actions by amenable locally compact Hausdorff groups. In the decade since then, the Hao-Ng isomorphism problems for both full and reduced crossed products were shown to be intimately tied to various functoriality and Takai-type duality for crossed products \\cites{abadie2010takai, kaliszewski2013functoriality, kaliszewski2015coactions} and, in the work of B\\'edos, Kaliszewski, Quigg, and Robertson \\cite{bedos2015new}, the Hao-Ng isomorphism for reduced crossed products was established for actions of discrete exact groups. Further applications can be found in other works, including those of Schafhauser on AF-embeddability \\cite{schafhauser2015cuntz}, and of Deaconu on group actions on graph $\\rC^*$-algebras \\cites{deaconu2012group, deaconu2018group}. In recent years, significant progress has been made on the Hao-Ng isomorphism problems by establishing a bridgehead between the structure theories of C*-algebras and non-self-adjoint operator algebra theory \\cites{katsoulis2019crossed, katsoulis2021non}. In these papers, Katsoulis and Ramsey show that to prove the Hao-Ng isomorphism for the reduced crossed product, it is sufficient to prove that the reduced \\emph{non-self-adjoint} operator algebra crossed product functor commutes with the $\\rC^*$-envelope. This strategy was highly successful, and has led to the resolution of the Hao-Ng isomorphism problem for reduced crossed products by discrete groups \\cites{katsoulis2017c} and under the assumption that the underlying C*-correspondence is hyperrigid \\cite{katsoulis2021non}."}
{"input": "witnessed by the first-named author together with Geffen and Eilers in \\cite{dor2020classification}, where it was used to establish a connection between the seemingly distinct classification theories for C*-algebras and non-self-adjoint operator algebras. These types of links between the self-adjoint and non-self-adjoint theories have led to significant structure results in C*-algebra theory, including gauge-coaction co-univeraslity theorems for a variety of C*-algebras \\cites{dor2020tensor, dor2022c, dor2023normal, kakariadis2023couniversality, sehnem2022c}, as well as various generalizations of Hao-Ng isomorphism theorems in the context of product systems \\cites{dor2020tensor, dor2022c, katsoulis2020product, kakariadis2024fock,li2022zappa}. In this paper, we establish the commutation of the reduced crossed product functor with the $\\rC^*$-envelope for general locally compact Hausdorff groups. \\begin{theoremx}\\label{t:a} Let $\\A$ be an operator algebra with a self-adjoint contractive approximate identity, and let $G$ be a locally compact Hausdorff group. If $\\alpha:G\\acts\\A$ is an action, then $\\rC^*_e(\\A)\\rtimes_{\\alpha,r}G\\cong\\rC^*_e(\\A\\rtimes_{\\alpha,r}G)$ via a canonical $*$-isomorphism. \\end{theoremx} Over the last decade, the $\\rC^*$-envelope has proven itself as a robust tool in operator algebra theory. It was first shown to exist in Hamana's work on injective envelopes for operator systems \\cite{hamana1979injective}, leading to far-reaching consequences in group theory \\cites{kalantar2017boundaries, breuillard2017c}, non-commutative convexity theory \\cites{davidson2017d, davidson2022strongly, davidson2019noncommutative, kennedy2021noncommutative}, and approximation theory \\cites{bilich2024arveson, kennedy2015essential, clouatre2024rigidity}. Theorem \\ref{t:a} is another example of this; allowing us to complete the proof strategy of Katsoulis and Ramsey \\cites{katsoulis2019crossed, katsoulis2021non} and resolve the Hao-Ng isomorphism problem for reduced crossed products. \\begin{theoremx}\\label{t:b} Let $X$ be a $\\rC^*$-correspondence, $G$ be a locally compact Hausdorff group, and $\\alpha:G\\acts X$ be a generalized gauge action."}
{"input": "\\O_{X\\rtimes_{\\alpha, r} G}$. \\end{theoremx} The proof of Theorem \\ref{t:a} is inspired by \\cite{katsoulis2021non}, where it was established that the reduced crossed product functor commutes with the $\\rC^*$-envelope for \\emph{hyperrigid} operator algebras (see \\cite[Definition 1.1]{arveson2011noncommutative}). The problem in extending the proof strategy of \\cite{katsoulis2021non} beyond the hyperrigid setting is rooted in the fact that the unique extension property from non-commutative Choquet theory \\cite[Definition 2.1]{arveson2008noncommutative} is not preserved under direct integrals. This was first witnessed in a recent counterexample to Arveson's hyperrigidity conjecture by Bilich and the first-named author \\cite{bilich2024arveson}. To overcome this obstruction, our central idea is prompted by another recent paper by Clou\\^atre with the second-named author \\cite{clouatre2024rigidity}. Therein, a so-called \\emph{tight} variation of the unique extension property is considered as a substitute for the usual one. Unfortunately, the unique tight extension property does not appear to fit the purpose of proving Theorem \\ref{t:a}. Indeed, this is impeded by the general lack of injectivity of the von-Neumann algebra generated by the range of an arbitrary $*$-representation of the $\\rC^*$-envelope. This led us in Proposition \\ref{p:tmax-action-pres} to consider an intermediate type of unique extension property, where the range of appropriate extensions is contained in $\\bB(\\H)\\ol{\\otimes} L^{\\infty}(G)$. However, for our argument to go through beyond the separable setting, we require an operator-valued version of a Maharam-type lifting theorem. A lifting theorem of Ionescu-Tulcea \\cite{tulcea1967existence} guarantees that there is an appropriate lift for the Lebesgue measure space of a group $G$ with respect to left Haar measure on $G$."}
{"input": "on Fubini tensor products \\cite[Section 3]{hamana1982tensor}, we are able to extend such lifting theorems to the operator-valued setting, which then allows us to prove our main results. This paper has four sections, including this introduction. In Section \\ref{s:prelim}, we gather preliminary facts on the $\\rC^*$-envelope, the injective envelope, C*-correspondences, and Cuntz-Pimsner algebras. In Section \\ref{s:maharam}, we prove operator-valued Maharam-type lifting theorems through the use of Hamana's Fubini tensor products. In Section \\ref{s:cross-prod-env}, we establish the commutation of the C*-envelope with the reduced crossed product (Theorem \\ref{t:a}), which leads to the resolution of the Hao-Ng isomorphism theorem for reduced crossed products (Theorem \\ref{t:b})."}
{"input": "\\section{Preliminaries} \\label{s:prelim} \\subsection{The C*-envelope and injective envelope} \\label{ss:C*-env} We recall some of the necessary machinery from operator algebra theory, which may be found in \\cites{arveson2008noncommutative, paulsen2002completely}. An \\emph{operator algebra} is a norm-closed subalgebra of bounded operators on a Hilbert space $\\A\\subseteq \\bB(\\H)$. A \\emph{representation} of $\\A$ is a completely contractive homomorphism $\\rho:\\A\\rightarrow\\bB(\\K)$. The \\emph{$\\rC^*$-envelope} of $\\A$ is a pair $(\\rC^*_e(\\A), \\varepsilon)$ consisting of a $\\rC^*$-algebra $\\rC^*_e(\\A)$ together with a completely isometric representation $\\varepsilon:\\A\\rightarrow\\rC^*_e(\\A)$ such that $\\rC^*(\\varepsilon(\\A)) = \\rC^*_e(\\A)$ has the following co-universal property: whenever $\\iota:\\A\\rightarrow\\B$ is a completely isometric representation with $\\B = \\rC^*(\\iota(\\A))$, then there is a surjective $*$-homomorphism $\\pi:\\B\\rightarrow\\rC^*_e(\\A)$ such that $\\pi\\circ\\iota = \\varepsilon$. Since this property uniquely determines the C*-envelope up to a $*$-isomorphism that preserves an image of $\\A$, we frequently refer to the $\\rC^*$-algebra $\\rC^*_e(\\A)$ as \\emph{the} $\\rC^*$-envelope of $\\A$. In Arveson's original paper on the subject \\cite{arveson1969subalgebras}, a method of constructing the $\\rC^*$-envelope of a \\emph{unital} operator algebra $\\A\\subseteq \\bB(\\H)$ was proposed. This method is different from the one used by Hamana, and is more in-line with classical Choquet theory. For this, suppose that $\\B:=\\rC^*(\\A)$ is the $\\rC^*$-algebra generated by $\\A$ in $\\bB(\\H)$. A unital $*$-representation $\\pi:\\B\\rightarrow\\bB(\\K)$ is said to have the \\emph{unique extension property} with respect to $\\A$ if there is a unique completely positive map $\\psi:\\B\\rightarrow\\bB(\\K)$ such that $\\psi|_\\A = \\pi|_\\A$. This is a non-commutative analogue of having the unique representing measure for a point evaluation be Dirac mass at that point."}
{"input": "respect to $\\A$ and $\\pi|_\\A$ is completely isometric, then it is easy to show that $(\\pi(\\B), \\pi|_\\A)$ coincides with the $\\rC^*$-envelope of $\\A$ (see \\cite{arveson1969subalgebras}). The existence of such $*$-representations was first exhibited by Dritschel and McCullough \\cite{dritschel2005boundary}, and has led to the resolution of Arveson's conjecture on sufficiency of boundary representations \\cites{arveson2008noncommutative, davidson2015choquet}. Throughout this paper, we shall consider non-unital operator algebras. However, in practice, our operator algebras will possess a self-adjoint contractive approximate unit. The $\\rC^*$-envelope of a non-unital operator algebra is also known to exist and behave similarly to the one in the unital setting. Given a non-unital operator algebra $\\A$, we let $\\A^\\sim := \\A \\oplus \\mathbb{C}I_{\\H}$ denote the one-point unitization of $\\A$. By a theorem of Meyer \\cite{meyer2001adjoining}, $\\A^\\sim$ is uniquely determined up to a unital completely isometric isomorphism of $\\A \\subseteq \\bB(\\H)$. Furthermore, if $(\\rC^*_e(\\A^\\sim), \\varepsilon)$ is the $\\rC^*$-envelope of $\\A^\\sim$, the $\\rC^*$-envelope of $\\A$ may be identified with the $\\rC^*$-algebra generated by $\\varepsilon(\\A)$ \\cite[Proposition 4.3.5]{blecher2004operator} inside $\\rC^*_e(\\A^\\sim)$. For further details on the unique extension property of representations of non-unital operator algebras, we refer the reader to \\cite[Section 2.2]{dor2018full}. We will also require some of the theory on injective envelopes of operator systems. We refer the reader to \\cite[Section 15]{paulsen2002completely} for further details. An \\emph{operator system} is a unital self-adjoint subspace $\\E\\subseteq \\bB(\\H)$, and $\\E$ is said to be \\emph{injective} if it is an injective object in the category of operator systems with unital completely positive maps as morphisms."}
{"input": "always exists a minimal $\\S$-projection $\\theta:\\bB(\\H) \\rightarrow \\bB(\\H)$ with $\\theta \\prec \\gamma$. In other words, we can always guarantee that a copy of $I(\\S)$ is contained in any given injective C*-algebra $\\gamma(\\bB(\\H))$. \\subsection{C*-correspondences, Cuntz-Pimsner algebras, and their dynamics}\\label{ss:correspondence} Here, we record some of the details on $\\rC^*$-correspondences that we shall need in this paper, and refer the reader to \\cite[Section 4.6]{brown2008textrm} or \\cite{lance1995hilbert} for more. Let $\\B$ be a $\\rC^*$-algebra. For a right Hilbert $\\B$-module $X$, we denote by $\\L(X)$ the $\\rC^*$-algebra of adjointable operators and by $\\K(X)$ its subalgebra given by the closure of the ``rank-one\" operators. More precisely, for $x,y\\in X$, the rank-one operators are denoted by $\\theta_{x,y}$ and defined by $\\theta_{x,y}(z) := x\\langle y, z\\rangle$. Recall that a \\emph{$\\rC^*$-correspondence} over $\\B$ is a right Hilbert $\\B$-module $X$ together with a $*$-homomorphism $\\varphi_X: \\B\\rightarrow\\L(X)$, which we consider as a left action of $\\B$ on $X$. We will assume throughout this paper that C*-correspondences are \\emph{non-degenerate}, where we say that $X$ is non-degenerate if the $\\varphi_X(\\B)X$ is dense in $X$."}
{"input": "y\\] for every $v,x\\in X, w,y\\in Y$ and $b\\in\\B$. Then, the completion with respect to the $\\B$-valued inner product yields a $\\rC^*$-correspondence $X\\otimes Y$. For a $\\rC^*$-correspondence $X$, one may then construct the Fock correspondence \\[\\F_X = \\B \\oplus \\bigoplus_{n=1}^{\\infty} X^{\\otimes n}\\] over $\\B$. The Fock correspondence gives rise to the so-called left-creation operators $T_x\\in\\L(\\F_X)$ for $x\\in X,$ defined by \\[T_x(b) = xb \\quad \\text{and} \\quad T_x(x_1\\otimes\\ldots\\otimes x_n) = x\\otimes x_1\\otimes\\ldots\\otimes x_n\\] for $b\\in\\B$ and $x, x_1,\\ldots, x_n\\in X$. The $\\rC^*$-algebra $\\T_X$ generated by the left action of $\\B$ on $\\F_X$ and the left-creation operators on $\\F_X$ is called the \\emph{Toeplitz algebra} of the $\\rC^*$-correspondence $X$. The norm-closed subalgebra $\\T_X^+$ generated by the left action of $\\B$ on $\\F_X$ and the left-creation operators is called the \\emph{tensor algebra} of $X$. Let $\\B, \\C$ be $\\rC^*$-algebras and $X$ be a $\\rC^*$-correspondence over $\\B$. A \\emph{representation} of $X$ is a pair $(\\rho, t)$ consisting of a $*$-homomorphism $\\rho:\\B\\rightarrow \\C$ and a completely contractive linear map $t:X\\rightarrow \\C$ with the property that \\[\\rho(a)t(x)\\rho(b) = t(\\varphi_X(a)x\\varphi_X(b)), \\quad a,b\\in \\B, x\\in X.\\] If, in addition, we have $t(x)^*t(y) = \\rho(\\langle x, y\\rangle)$ for each $x,y\\in X$, then we say that $(\\rho, t)$ is \\emph{rigged} (this is sometimes called \\emph{isometric}, for instance in \\cite{muhly1998tensor}). In \\cites{pimsner1996class, muhly1998tensor}, it is shown that the Toeplitz algebra $\\T_X$ is the universal $\\rC^*$-algebra generated by the rigged representations of $X$."}
{"input": "\\section{Operator-valued Maharam-type lifting theorems} \\label{s:maharam} For our arguments, we require generalizations of classical measure-theoretic lifting theorems. To this end, let $(X, \\F, \\mu)$ be a complete measure space and $M^\\infty(\\F)$ denote the space of bounded measurable functions on $X$, which is a C*-subalgebra of all bounded functions $\\ell^{\\infty}(X)$. Then, there is a surjective $*$-homomorphism $q:M^\\infty(\\F)\\rightarrow L^\\infty(X, \\mu)$ defined by identifying functions $\\mu$-a.e. A \\emph{lifting} for $(X, \\F, \\mu)$ is a unital $*$-homomorphism $\\rho: L^\\infty(X, \\mu)\\rightarrow M^\\infty(\\F)$ such that $q\\circ\\rho = \\id$. The lifting theorem of Maharam \\cite{maharam1958theorem} states that $(X, \\F, \\mu)$ always admits a lifting when $\\mu$ is a finite measure. The existence of a lift is known in several other circumstances \\cite{tulcea2012topics}, including the measure space $(G,\\F,\\mu)$ where $G$ is a locally compact Hausdorff group, $\\mu$ is left Haar measure, and $\\F$ is the completion of Borel $\\sigma$-algebra on $G$ with respect to $\\mu$ \\cite{tulcea1967existence}. Our lifting result will apply Hamana's work on Fubini tensor products \\cite{hamana1982tensor}. For this, fix a pair of Hilbert spaces $\\H, \\K$ and let $\\{e_\\alpha : \\alpha\\in A\\}$ be an orthonormal basis for $\\H$. For $\\alpha,\\beta\\in A$, we define rank-one operators on $\\H$ by $E_{\\alpha\\beta}(h) = \\langle h, e_\\beta\\rangle e_\\alpha$. Furthermore, for each $\\alpha\\in A$, let $J_\\alpha: \\K\\rightarrow\\H\\otimes\\K$ be the isometry defined by $J_\\alpha (k) = e_\\alpha\\otimes k$. Then, each $x\\in \\bB(\\H\\otimes\\K)$ can be expressed as \\[x = \\sum_{\\alpha, \\beta\\in A} E_{\\alpha\\beta}\\otimes J_\\alpha^* xJ_\\beta \\] where the sum converges in the strong operator topology. Let $\\S\\subseteq \\bB(\\K)$ be a norm-closed operator system."}
{"input": "\\cite[Lemma 3.4]{hamana1982tensor}, we may form an operator system \\[\\bB(\\H)\\ol{\\otimes}\\S : = \\{x \\in \\bB(\\H\\otimes\\K) ~:~ (J_\\alpha^* xJ_\\beta)\\in\\S \\text{ for all $\\alpha,\\beta\\in A$}\\}.\\] When $\\M$ is a von Neumann algebra, $\\bB(\\H)\\ol{\\otimes}\\M$ agrees with the usual von Neumann algebra tensor product \\cite[Theorem 3.12 (ii)]{hamana1982tensor}. From \\cite[Lemma 3.5]{hamana1982tensor}, we can guarantee that there is a unique unital completely positive map \\[\\id\\ol{\\otimes}q:\\bB(\\H)\\ol{\\otimes} M^\\infty(\\F) \\rightarrow \\bB(\\H)\\ol{\\otimes}L^\\infty(X, \\mu)\\] that extends $\\id\\odot q$. In fact, we will show that whenever a lifting for $(X, \\F, \\mu)$ exists, the map $\\id\\ol{\\otimes}q$ has a natural unital completely positive right inverse. Before proceeding with the proof, we record some relevant information. Recall that a $\\rC^*$-algebra $\\B$ is said to be \\emph{monotone complete} (respectively, \\emph{$\\sigma$-monotone complete}) if every increasing bounded net (sequence) of self-adjoint operators in $\\B$ has a least upper bound in $\\B$. In \\cite[Theorem 4.2]{hamana1982tensor}, Hamana proved that $\\bB(\\H)\\ol{\\otimes} \\B$ is a monotone complete $\\rC^*$-algebra whenever $\\B$ is. Furthermore, in \\cite[Theorem 6.1]{saito2016tensor}, Sait\\^o showed that when $\\H$ is separable, and $\\B$ is $\\sigma$-monotone complete, then $\\bB(\\H)\\ol{\\otimes} \\B$ is also a $\\sigma$-monotone complete C*-algebra. For our case of interest, $M^\\infty(\\F)$ is merely $\\sigma$-monotone complete and we will need $\\H$ to be potentially non-separable. Thus, at least a priori, we only know that $\\bB(\\H)\\ol{\\otimes}M^\\infty(\\F)$ is an operator system. This is an important distinction, as there are examples of commutative $\\rC^*$-algebras acting on non-separable Hilbert space for which $\\bB(\\H)\\ol{\\otimes}\\B$ is not a $\\rC^*$-algebra \\cite[Section 5]{saito2016tensor}. \\begin{proposition}\\label{p:maharam} Let $(X, \\F, \\mu)$ be a complete measure space and $\\H$ be a Hilbert space."}
{"input": "$\\id\\ol{\\otimes}q: \\bB(\\H)\\ol{\\otimes}M^\\infty(\\F)\\rightarrow \\bB(\\H)\\ol{\\otimes} L^\\infty(X, \\mu)$ be the unique unital completely positive map extending $\\id\\odot q$. If there is a lift $\\rho$ for $(X, \\F, \\mu)$, then the unique unital completely positive map $\\id\\ol{\\otimes}\\rho: \\bB(\\H)\\ol{\\otimes}L^\\infty(X, \\mu)\\rightarrow \\bB(\\H)\\ol{\\otimes} M^\\infty(\\F)$ is a unital complete order embedding extending $\\id\\odot\\rho$, and satisfies $(\\id\\ol{\\otimes} q)\\circ(\\id\\ol{\\otimes}\\rho) = \\id$. \\end{proposition} \\begin{proof} As $\\rho$ is a unital complete order embedding, \\cite[Lemma 3.5 (ii)]{hamana1982tensor} implies that $\\id\\odot\\rho$ uniquely extends to a unital complete order embedding \\[\\id\\ol{\\otimes}\\rho: \\bB(\\H)\\ol{\\otimes}L^\\infty(X, \\mu)\\rightarrow \\bB(\\H)\\ol{\\otimes} M^\\infty(\\F).\\]Since $\\id\\ol{\\otimes}q$ and $\\id\\ol{\\otimes}\\rho$ are unital completely positive maps extending $\\id\\odot q$ and $\\id\\odot\\rho$, respectively, we conclude that $(\\id\\ol{\\otimes}q)\\circ(\\id\\ol{\\otimes}\\rho) = \\id$ by uniqueness of \\cite[Lemma 3.5 (i)]{hamana1982tensor}. \\end{proof} Although the operator system structure will be sufficient for our purposes, a finer analysis reveals that $\\id\\ol{\\otimes}\\rho$ still remains multiplicative, at least in some sense. Indeed, $\\C :=\\rho(L^\\infty(X, \\mu))$ is monotone complete as it is an injective $\\rC^*$-algebra \\cite[Lemma 3.1]{hamana1982tensor}. Thus, by \\cite[Theorem 4.2]{hamana1982tensor} we find that $\\bB(\\H)\\ol{\\otimes}\\C$ is a monotone complete $\\rC^*$-algebra and $\\id\\ol{\\otimes}\\rho : \\bB(\\H)\\ol{\\otimes} L^\\infty(X, \\mu)\\rightarrow \\bB(\\H)\\ol{\\otimes} \\C$ is a unital injective $*$-homomorphism, where $\\bB(\\H)\\ol{\\otimes} \\C$ is given the Choi-Effros product. Thus, it must automatically be normal by the monotone completeness of its domain and range. Furthermore, the inclusion mapping $\\C \\subseteq M^\\infty(\\F)$ promotes to a unital completely isometric map $\\id\\ol{\\otimes}\\id: \\bB(\\H)\\ol{\\otimes}\\C \\rightarrow \\bB(\\H)\\ol{\\otimes}M^\\infty(\\F)$ that uniquely extends $\\id\\odot\\id$ by \\cite[Lemma 3.5 (ii)]{hamana1982tensor}. The uniqueness of such a map ensures that this must be the identity map."}
{"input": "\\section{Reduced crossed products and their C*-envelopes} \\label{s:cross-prod-env} Let $G$ be a locally compact Hausdorff group and $m$ denote the left Haar measure on $G$. Consider an approximately unital operator algebra $\\A$ and a point-norm continuous action $\\alpha:G\\curvearrowright\\A$ by completely isometric automorphisms. Throughout, we will refer to the triple $(\\A, G, \\alpha)$ as a \\emph{dynamical system}. If $\\A$ happens to be a C*-algebra, we will refer to the triple $(\\A, G, \\alpha)$ as a C*-dynamical system. A completely contractive representation $\\pi:\\A\\rightarrow\\bB(\\H)$ is said to be \\emph{$\\alpha$-admissible} if there is an action $\\widehat{\\alpha} : G \\curvearrowright \\rC^*(\\pi(\\mathcal{A}))$ such that $\\widehat{\\alpha}_g \\circ \\pi = \\pi \\circ \\alpha_g$. When the $\\alpha$-admissible representation $\\pi$ is completely isometric and considered as an embedding, we will abuse notation and continue to denote the extended action $\\widehat{\\alpha}$ by $\\alpha$. A dynamical system always admits a completely isometric $\\alpha$-admissible embedding. For instance, the embedding $\\varepsilon:\\A\\rightarrow\\rC^*_e(\\A)$ is always $\\alpha$-admissible \\cite[Lemma 3.3]{katsoulis2019crossed}. A \\emph{covariant pair} $(\\pi, \\mu)$ for $(\\A, G, \\alpha)$ consists of a non-degenerate representation $\\pi:\\A\\rightarrow\\bB(\\H)$ and a strongly continuous representation $\\mu: G \\rightarrow\\bB(\\H)$ such that \\[\\pi(\\alpha_g(a)) = \\mu_g \\pi(a)\\mu_g^*, \\quad g\\in G.\\] Note that whenever $(\\pi,\\mu)$ is a covariant pair, it follows that $\\pi$ is automatically $\\alpha$-admissible where the action $\\widehat{\\alpha}$ of $G$ on $C^*(\\pi(A))$ is given by $\\widehat{\\alpha}(T) = \\mu_gT\\mu_g^*$."}
{"input": "$g\\in G$. Moreover, note that since $g \\mapsto \\pi \\circ \\alpha_g$ is point-norm continuous, the image of $\\pi_{\\alpha}$ is actually contained in $C_b(G;\\bB(\\H))$. We let $\\lambda : G \\rightarrow \\bB(L^2(G))$ be the left-regular representation of $G$, so that $\\id \\otimes \\lambda$ is a strongly continuous representation of $G$ on $\\H \\otimes L^2(G)$. Then, the pair $(\\pi_{\\alpha},\\id \\otimes \\lambda)$ is readily verified to be covariant. In particular, $\\pi_{\\alpha}$ is automatically $\\alpha$-admissible. Given a dynamical system $(\\A,G,\\alpha)$ and a covariant pair $(\\pi,\\mu)$, one may form the integrated form $\\pi \\rtimes \\mu$ on $C_c(G,\\A)$ given by $[\\pi \\rtimes \\mu](f) = \\int_G \\pi(f(g))\\mu_g dm(g)$, where the latter integral is understood as in \\cite[Lemma 1.91]{williams2007book}. Since the reduced crossed product for $(\\A, G, \\alpha)$ is independent of the choice of a completely isometric $\\alpha$-admissible $\\pi$ \\cite[Corollary 3.16]{katsoulis2019crossed}, we may then define the reduced crossed product as follows. \\begin{definition} Let $(\\A, G, \\alpha)$ be a dynamical system and $\\pi : \\mathcal{A} \\rightarrow \\mathbb{B}(\\mathcal{H})$ be a completely isometric $\\alpha$-admissible representation. Then, the reduced crossed product $\\mathcal{A} \\rtimes_{\\alpha,r} G$ is the closure of the image of $C_c(G,\\A)$ under $\\pi_{\\alpha} \\rtimes (\\id \\otimes \\lambda)$. \\end{definition} The following proposition is the key observation that allows us to push the strategy of \\cite{katsoulis2021non}, and verify the commutation of the $\\rC^*$-envelope with the reduced crossed product. In what follows, we recall \\cite[Definition 2.2]{dor2018full} where the unique extension property for potentially non-unital operator algebras is defined with respect to completely contractive completely positive extensions to a generated C*-algebra."}
{"input": "\\alpha)$ be a C*-dynamical system, and let $\\A \\subseteq \\B = C^*(\\A)$ be an $\\alpha$-invariant operator subalgebra generating $\\B$. Suppose $\\pi : \\mathcal{B} \\rightarrow \\mathbb{B}(\\mathcal{H})$ is a non-degenerate $*$-representation that has the unique extension property with respect to $\\A$. Then $\\pi_{\\alpha}$ is the unique completely contractive completely positive extension of $\\pi_{\\alpha}|_{\\A}$ to $\\B$ with range contained in $\\bB(\\H)\\ol{\\otimes}L^{\\infty}(G)$. \\end{proposition} \\begin{proof} Let $\\varphi : \\B \\rightarrow \\bB(\\H)\\ol{\\otimes}L^{\\infty}(G)$ be a completely contractive completely positive extension of $\\pi_{\\alpha}|_{\\A}$. We show that $\\varphi = \\pi_{\\alpha}$. To this end, let $(G,\\F,m)$ be the measure space where $\\F$ is the completion of the Borel $\\sigma$-algebra on $G$ with respect to left Haar measure $m$. By \\cite[Theorem 5]{tulcea1967existence}, there exists an equivariant lifting $\\rho : L^{\\infty}(G) \\rightarrow M^{\\infty}(\\F)$ for $(G, \\F, m)$, and by \\cite[Proposition 1]{tulcea1967existence} we may assume that $\\rho$ acts as the identity on $C_b(G)$. Thus, by Proposition \\ref{p:maharam}, the map $\\id\\ol{\\otimes} q$ has a right inverse \\[ \\id\\ol{\\otimes} \\rho: \\bB(\\H)\\ol{\\otimes}L^{\\infty}(G) \\rightarrow \\bB(\\H)\\ol{\\otimes} M^{\\infty}(\\F),\\] and $\\id \\ol{\\otimes}\\rho$ acts as the identity on $\\bB(\\H) \\otimes \\rC_b(G)$ as well. By \\cite[Lemma 3.5 (i)]{hamana1982tensor}, the map $\\ev_g \\ol{\\otimes} \\id : \\bB(\\H)\\ol{\\otimes} M^{\\infty}(\\F) \\rightarrow \\bB(\\H)$ is a well-defined UCP map for each $g\\in G$. Thus, we have a family of completely contractive completely positive maps $\\varphi_g : = (\\id \\ol{\\otimes} \\ev_g) \\circ (\\id \\ol{\\otimes} \\rho) \\circ \\varphi$ which satisfy $\\oplus_g \\varphi_g = (\\id \\ol{\\otimes} \\rho) \\circ \\varphi$. Since the range of $\\pi_{\\alpha}$ is contained in $C_b(G;\\bB(\\H)) \\cong \\bB(\\H) \\otimes \\rC_b(G)$, we have that $(\\id\\ol{\\otimes} \\rho) \\circ \\pi_{\\alpha} = \\pi_{\\alpha}$."}
{"input": "have that $\\theta$ fixes $\\sigma(\\rC^*_e(\\A)\\rtimes_{\\alpha, r}G)$. The $\\rC^*$-algebra generated by $\\sigma(\\A\\rtimes_{\\alpha, r}G)^\\sim$ under the Choi--Effros product induced on the image of $\\theta$ must then be equal to $\\sigma(\\rC^*_e(\\A)\\rtimes_{\\alpha, r}G)^\\sim$. However, since $\\theta$ is a minimal $\\S$-projection and its image is the injective envelope $I(\\sigma(\\A\\rtimes_{\\alpha, r}G)^\\sim)$, we see that $\\sigma(\\rC^*_e(\\A)\\rtimes_{\\alpha, r}G)^\\sim$ is the C*-algebra generated by $\\sigma(\\A\\rtimes_{\\alpha, r}G)^\\sim$ inside the injective envelope. In turn, $\\sigma(\\rC^*_e(\\A)\\rtimes_{\\alpha, r}G)^\\sim$ must then coincide with the C*-envelope $\\rC^*_e(\\sigma(\\A\\rtimes_{\\alpha, r}G)^\\sim)$. Thus, considering the subalgebra $\\sigma(\\A\\rtimes_{\\alpha, r}G)$ and its generated $\\rC^*$-algebra, we deduce that \\[\\rC^*_e(\\sigma(\\A\\rtimes_{\\alpha, r}G)) = \\sigma(\\rC^*_e(\\A)\\rtimes_{\\alpha, r}G).\\]As $\\sigma$ is completely isometric, the conclusion follows. \\end{proof} Now that we are equipped with Theorem \\ref{t:red-cross-iso}, we are at the stage where we can settle the Hao-Ng isomorphism problem for reduced crossed products. \\begin{theorem}\\label{t:hao-ng} Let $G$ be a locally compact Hausdorff group, and let $X$ be a (non-degenerate) $\\rC^*$-correspondence over a C*-algebra $\\B$. Suppose $\\alpha:G\\acts X$ is a generalized gauge action. Then, \\[\\O_X\\rtimes_{\\alpha, r} G\\cong \\O_{X\\rtimes_{\\alpha, r} G}.\\] \\end{theorem} \\begin{proof} By \\cite[Theorem 3.8]{katsoulis2006tensor}, Theorem \\ref{t:red-cross-iso}, and \\cite[Theorem 3.7]{katsoulis2021non}, we have that \\[\\O_X \\rtimes_{\\alpha, r} G \\cong \\rC^*_e(\\T_X^+)\\rtimes_{\\alpha, r}G \\cong \\rC^*_e(\\T_X^+ \\rtimes_{\\alpha, r}G) \\cong \\O_{X\\rtimes_{\\alpha, r} G}.\\] \\end{proof} Although the strategy presented in \\cite{katsoulis2021non} is shown here to be successful in resolving the reduced Hao-Ng isomorphsm in complete generality, the corresponding \\emph{full} Hao-Ng isomorphism is more difficult. Indeed, it is known that the full Hao-Ng isomorphism is equivalent to the commutation of the $\\rC^*$-envelope and the full crossed product for tensor algebras \\cite[Theorem 4.9]{katsoulis2021non}."}
{"input": "to prove an analogue of Theorem \\ref{t:red-cross-iso} for full crossed products. Unfortunately, this fails for arbitrary operator algebras \\cite[Theorem 5.7]{harris2019crossed}, while it is unknown whether it is valid for the class of tensor algebras. So, a more nuanced approach to the full Hao-Ng isomorphism would be necessary. One such approach has been proposed for the subclass of so-called hyperrigid $\\rC^*$-correspondences. We say that a $\\rC^*$-correspondence $X$ is \\emph{hyperrigid} if all non-degenerate representations of $\\O_X$ have the unique extension property when restricted to $\\T_X^+$. Although many $\\rC^*$-correspondences are hyperrigid, as can be witnessed by Kim's criterion \\cite{kim2021hyperrigidity}, there are many interesting examples of C*-correspondences that are not hyperrigid. In fact, in a recent paper by Bilich \\cite{bilich2025maximality}, several examples of C*-correspondences are shown to fail to satisfy Arveson's hyperrigidity conjecture, and are hence not hyperrigid. For hyperrigid $\\rC^*$-correspondences, however, a notable reduction in establishing the full Hao-Ng isomorphism is to establish the coincidence of all full crossed products of $\\T_X^+$ relative to admissible representations \\cite[Theorem 4.9]{katsoulis2021non}. Thus, to establish the Hao-Ng isomorphism for full crossed products arising from hyperrigid $\\rC^*$-correspondences, it suffices to show that the full crossed product construction by a generalized gauge action, considered as an action on $\\T_X^+$, does not depend on the choice of $\\alpha$-admissible representation. Unfortunately, this reformulation has only been verified for row-finite graph $\\rC^*$-correspondences \\cite[Corollary 5.3]{katsoulis2021non}."}
{"input": "\\section{Introduction} In this paper, we will be dealing with structures endorsed with a single binary relation. Such structures are intuitively easier to visualize as directed graphs. A \\emph{directed graph} $(G,E)$ is an ordered pair consisting of a set $G$ of \\emph{vertices} and a set $E\\subset G\\times G$ of \\emph{arrows}. For the sake of being concise, we will drop the word ``directed'' in this definition. This should cause no confusion as we will be dealing only with directed graphs. When talking about some graph $(G,E),$ we might use the binary relation notation to indicate arrows, that is, we might write $x \\mathrel{E} y$ to indicate $(x,y)\\in E.$ Recall that a map $h:G_1\\rightarrow G_2$ is called a \\emph{homomorphism} from the graph $(G_1,E_1)$ to the graph $(G_2,E_2)$ iff $(x,y)\\in E_1 \\implies (h(x),h(y))\\in E_2.$ The fact that $h$ is a homomorphism from $(G_1,E_1)$ to $(G_2,E_2)$ will be indicated by writing $h:(G_1,E_1) \\rightarrow (G_2,E_2).$ An \\emph{isomorphism} is a bijective map $h:G_1\\rightarrow G_2$ such that both $h$ and $h^{-1}$ are homomorphisms. A homomorphism from a graph to itself is called an \\emph{endomorphism} and an isomorphism from a graph to itself is called an \\emph{automorphism}. \\begin{definition} A binary relation $E$ on a set $G$ is said to be \\emph{rigid} iff the graph $(G,E)$ has no nontrivial automorphisms. If, furthermore, $(G,E)$ has no nontrivial endomorphisms, then $E$ is said to be \\emph{strongly rigid.} If $E$ is a (strongly) rigid relation on $E,$ then we say that the graph $(G,E)$ is \\emph{(strongly) rigid}."}
{"input": "$\\RR,$ asserts that every set $G$ has a rigid relation $E.$ The \\emph{strongly rigid relation principle}, $\\SRR,$ asserts that every set $G$ has a strongly rigid relation $E.$ \\end{definition} $\\SRR$ implies $\\RR$ by definition. \\citeauthor{vopenka} \\cite{vopenka} prove, using the Axiom of Choice $(\\AC),$ that every set has a strongly rigid relation.\\footnote{In graph theory literature, what we call ``strongly rigid'' (following \\cite{hamkins}) is called just ``rigid.''} Thus, $\\AC$ implies $\\SRR,$ and hence also $\\RR.$ Joel David Hamkins and Justin Palumbo \\cite{hamkins} prove that $\\RR$ does not imply $\\AC$ and also establish the independence of $\\RR$ from $\\ZF.$ We therefore have the following theorem: \\begin{theorem}[\\cite{vopenka} and \\cite{hamkins}]\\label{thm:rr_principle} $\\RR$ is independent from $\\ZF,$ follows from $\\AC,$ but is not equivalent to $\\AC.$ \\end{theorem} We will prove in this paper that the same is true for $\\SRR$ as well. That is, \\begin{theorem} $\\SRR$ is independent from $\\ZF,$ follows from $\\AC,$ but is not equivalent to $\\AC.$ \\end{theorem} The independence of $\\SRR$ from $\\ZF$ already follows from independence of $\\RR$ and the fact that $\\neg\\RR\\implies \\neg\\SRR.$ Also, \\cite{vopenka} already proves that $\\SRR$ follows from $\\AC,$ as mentioned above. So, we only need to prove that $\\SRR$ does not imply $\\AC.$ We do so by combining ideas from \\cite{hamkins} and a method for producing elementary embeddings from homomorphisms."}
{"input": "embedding $j:V_\\alpha^L\\rightarrow V_\\alpha^L.$ \\end{thm:reals_case} Using that, in section 4, we prove that $\\SRR$ does not imply $\\AC$ by building a model for $\\ZF+\\neg\\AC+\\SRR.$ \\theoremstyle{plain} \\newtheorem*{thm:neg_ac_plus_srr}{Theorem~\\ref{thm:neg_ac_plus_srr}} \\begin{thm:neg_ac_plus_srr} If $\\ZF$ is consistent, then so is $\\ZF+\\neg\\AC+\\SRR.$ \\end{thm:neg_ac_plus_srr} In the last section, we will take a look at Proto Berkeley cardinals, a large cardinal notion inconsistent with $\\AC$ introduced in \\cite{lcbc}. Their definition is given in terms of the existence of elementary embeddings. \\begin{definition}[\\cite{lcbc}] A cardinal $\\delta$ is \\emph{proto Berkeley} iff for every transitive set $M$ such that $\\delta\\in M,$ there is a nontrivial elementary embedding $j:M\\rightarrow M$ with critical point strictly below $\\delta.$ \\end{definition} We will use our method for getting elementary embeddings from homomorphisms to characterize these cardinals in terms of the existence of homomorphisms rather than elementary embeddings. This is useful since the notion of a homomorphism is simpler and more ``elementary'' than the notion of elementary embeddings, and one would naturally want their (large cardinal) axioms to be as simple as possible. Recall that the \\emph{Hartog's number} of a set $X,$ denoted by $\\aleph(X),$ is the least ordinal $\\alpha$ such that there is no injection from $\\alpha$ into $X.$ \\theoremstyle{plain} \\newtheorem*{thm:proto_berkeley_cardinals}{Theorem~\\ref{thm:proto_berkeley_cardinals}} \\begin{thm:proto_berkeley_cardinals} A cardinal $\\delta$ is proto Berkeley iff for any graph $(G,E)$ such that $\\aleph(G)>\\delta$ and any injection $f:\\delta\\rightarrow G,$ there is an endomorphism $h:(G,E)\\rightarrow(G,E)$ such that $h\\vert_{\\range{f}}\\neq \\id."}
{"input": "\\emph{subgraph induced by $H$} is the graph $(H,F)$ where $F=\\{(u,v)\\in E\\mid u,v\\in H\\}.$ \\begin{proposition}\\label{prop:countable_case} Every at most countable set has a strongly rigid relation. \\end{proposition} \\begin{proof} Let $G$ be a set that is at most countable. If $G$ is countable, then let $E$ be such that $(G,E)$ is the graph in Figure~\\ref{fig:countable_case}. Fix an endomorphism $h:(G,E)\\rightarrow (G,E).$ First, notice that $u_0$ has outdegree $2$ due to the two arrows connecting it with $u_1$ and $u_2.$ Since $u_1$ and $u_2$ are connected by an arrow, and since $(G,E)$ is free of loops $h(u_1)\\neq h(u_2).$ Thus, $h(u_0)$ must have outdegree at least $2.$ But, $u_0$ is the only vertex with outdegree at least $2,$ and therefore $h(u_0)=u_0.$ Now, $h$ can either fix both $u_1$ and $u_2$ or swap them. However, the arrow $(u_1,u_2)$ ensures that swapping them is not possible, so that $h$ fixes $u_1$ and $u_2$ too. It is now easy to see that $h$ must fix every other vertex as well, meaning that $h=\\id.$ If $G$ is finite, say $G=\\{u_0,\\ldots,u_n\\},$ then simply take the induced subgraph of the graph in Figure~\\ref{fig:countable_case}. \\end{proof} \\begin{figure}[htb!] \\centering \\includegraphics[page=6]{images/figures_strong_rigidity.pdf} \\caption{A strongly rigid graph on a countable set} \\label{fig:countable_case} \\end{figure} %\\FloatBarrier %Since we will be talking about elementary embeddings in the sense of $V,$ we need to be careful about and able to distinguish between formulas that are metatheoretic and formulas that belong to $V."}
{"input": "the sense of $V,$ due to $V$ having more formulas.\\todo{is this necessary?} The language of set theory, $\\LST,$ is defined in $V$ to be the first order language with the binary relation symbol $\\in.$ Denote by $\\FORM$ the set of all formulas of $\\LST.$ To distinguish metatheoretic formulas from those in $V,$ whenever a formula appears in this paper, we will mention that it belongs to the set $\\FORM$ precisely when the formula is in $V.$ \\begin{theorem}\\label{thm:ordinal_case} If there exists an ordinal $\\beta$ for which there is no strongly rigid relation, then for some ordinal $\\alpha$ there exists a nontrivial elementary embedding $j:V_\\alpha^L\\rightarrow V_\\alpha^L.$ \\end{theorem} \\begin{proof} Let $\\kappa=|\\beta|.$ Since $\\kappa$ is a cardinal in $V,$ it is also a cardinal in $L.$ The fact that $\\GCH$ holds in $L$ implies that for every cardinal $\\lambda$ in $L$ there is some ordinal $\\alpha$ such that $|V_\\alpha^L|^L=\\lambda.$ Fix $\\alpha$ such that $|V_\\alpha^L|^L=\\kappa.$ Our argument would be easier if $\\alpha$ were a limit ordinal so that $V_\\alpha^L$ is closed under taking ordered pairs and finite sequences. But, we do not know this for sure, so let $N\\supset V_\\alpha^L\\cup \\{V_\\alpha^L\\}$ be the smallest set closed under taking finite subsets, i.e., $\\{x_1,\\ldots,x_n\\}\\subset N\\implies \\{x_1,\\ldots,x_n\\}\\in N,$ for all $n\\in\\omega.$ Notice that $N\\in L$ and has cardinality $\\kappa$ in $L,$ and since $\\kappa$ is a cardinal in $V$ too, we have that $|N|=\\kappa.$ Furthermore, $N$ is transitive and closed under the operations: \\begin{enumerate}[label=N\\arabic*] \\item $x,y \\mapsto (x,y).$ \\item $x_1,\\ldots, x_n \\mapsto \\langle x_1,\\ldots, x_n\\rangle,$ for all $n\\in\\omega."}
{"input": "\\end{enumerate} %Let $\\Sat(a)$ be a formula such that, for all natural numbers $n,$ for all $X,x_1,\\ldots,x_n,$ and all formulas $\\varphi(a_1,\\ldots,a_n)$ in the language of set theory, %$$ %(X,\\in)\\satisfies\\varphi[x_1,\\ldots,x_n] \\iff \\Sat(\\langle X, \\ulcorner \\varphi(a_1,\\ldots,a_n)\\urcorner, x_1,\\ldots,x_n\\rangle). %$$ \\todo{reference for such sat to exist, model theoretic satisfaction} %By the Reflection Principle\\todo{reference in jech} in $L,$ fix $N\\in L$ such that $N\\supset M,$ $|N|^L=\\kappa,$ and $N$ is absolute for $\\Sat(a).$ Observe that $|N|=\\kappa$ too, since $\\kappa$ is a cardinal in $V$ as well. Let $N^c$ be a copy of $N.$ Let $N^c$ be a copy of $N.$ Define $<_L^c\\subset N^c\\times N^c$ by $x^c<_L^c y^c \\iff x<_L y.$ Consider the graph $(G,E)$ in Figure~\\ref{fig:L_case}, where \\begin{enumerate}[label=G\\arabic*] \\item $(w_0,x),(x,x^c),(w_1,x^c)\\in E,$ for every $x\\in N.$ \\item $(x,y)\\in E$ iff $x\\in y,$ for every $x,y\\in N.$ \\item $(x^c,y^c)\\in E$ iff $x^c<_L^c y^c,$ for every $x^c,y^c\\in N^c.$ \\item $(p_n,x^c)\\in E$ iff $|x|=n,$ for every $n\\in\\omega$ and every $x^c\\in N^c.$ \\item For every $n\\in \\omega$ and every $x^c\\in N^c,$ $(q_n,x^c)\\in E$ iff one of the followings hold: \\begin{enumerate}[label=(\\alph*)] \\item $x\\in V_\\alpha^L,$ $n=\\ulcorner\\psi(a)\\urcorner$ for some $\\psi(a)\\in\\FORM,$ and $V_\\alpha^L\\satisfies \\psi[x].$ \\item $x=\\langle x_1,\\ldots,x_m\\rangle$ for some $m>1,$ $x_1,\\ldots,x_m\\in V_\\alpha^L,$ $n=\\ulcorner \\psi(a_1,\\ldots,a_m)\\urcorner$ for some $\\psi(a_1,\\ldots,a_m)\\in \\FORM,$ and $V_\\alpha^L\\satisfies \\psi[x_1,\\ldots,x_m].$ \\end{enumerate} \\end{enumerate} \\begin{figure}[htb!] \\centering \\includegraphics[page=3]{images/figures_strong_rigidity.pdf} \\caption{} \\label{fig:L_case} \\end{figure} The cardinality of $G$ is $|A|+|B|+|N|+|N^c|=4+\\omega+\\kappa+\\kappa=\\kappa.$ Thus, the graph $(G,E)$ has a nontrivial endomorphism $h:(G,E)\\rightarrow (G,E).$ We will show that the restriction $h\\vert_{V_\\alpha^L}$ is a nontrivial elementary embedding of $V_\\alpha^L$ into itself. We start by showing that $h$ fixes every vertex in $A."}
{"input": "E\\ \\vee\\ (h(y^c), h(x^c)) \\in E\\\\ &\\implies h(x^c)\\neq h(y^c). \\end{align*} Let us show that, for all $n\\in\\omega$ and all distinct $x_1,\\ldots,x_n\\in N,$ \\begin{equation}\\label{eq:h_respects_finite_sets} h(\\{x_1,\\ldots,x_n\\})=\\{h(x_1),\\ldots,h(x_n)\\}. \\end{equation} Denote by $S$ the set $\\{x_1,\\ldots,x_n\\}.$ We have $(x_i,S)\\in E,$ for all $i,$ by G2. By endomorphism of $h,$ we have $(h(x_i),h(S))\\in E,$ for all $i.$ This means that $h(x_i)\\in h(S),$ for all $i,$ by G2. We already know that $h$ is injective on $N,$ so we deduce that $h(S)$ has at least the $n$ distinct elements $h(x_1),\\ldots, h(x_n).$ We will be done if we can show that the cardinality of $h(S)$ is $n.$ By G4, $(p_n,S^c)\\in E,$ and by applying $h$ to this we get $(h(p_n),h(S^c))\\in E.$ $p_n$ is fixed by $h$ and we saw in the previous paragraph that $h(S^c)=h(S)^c,$ therefore, $(p_n,h(S)^c)\\in E.$ Again by G4, $|h(S)|=n.$ Using \\eqref{eq:h_respects_finite_sets} above, we can easily show that, for all $(x,y)\\in N,$ $h((x,y))=(h(x),h(y)):$ \\begin{align*} h((x,y)) &= h(\\{\\{x\\},\\{x,y\\}\\}) \\\\ &= \\{h(\\{x\\}),h(\\{x,y\\})\\} \\\\ &= \\{\\{h(x)\\},\\{h(x),h(y)\\}\\} \\\\ &= (h(x),h(y)). \\end{align*} Observe that, by G5(a) and the fact that all $m\\in\\omega$ are uniquely definable inside of $V_\\alpha^L,$ we must have $h(m)=m$ for all $m\\in\\omega.$ We can now also show that, for all $\\langle x_1,\\ldots,x_n\\rangle\\in N,$ $h(\\langle x_1,\\ldots,x_n\\rangle)= \\langle h(x_1),\\ldots,h(x_n)\\rangle:$ \\begin{align*} h(\\langle x_1,\\ldots,x_n\\rangle) &= h(\\{(0,x_1),\\ldots,(n-1,x_n)\\}) \\\\ &= \\{h((0,x_1)),\\ldots,h((n-1,x_n))\\} \\\\ &= \\{(h(0),h(x_1)),\\ldots,(h(n-1),h(x_n))\\} \\\\ &= \\{(0,h(x_1)),\\ldots,(n-1,h(x_n))\\} \\\\ &= \\langle h(x_1),\\ldots,h(x_n)\\rangle. \\end{align*} There are two last facts that we will need before we present our final argument. The first one is the fact that $x\\in V_\\alpha^L \\implies h(x)\\in V_\\alpha^L."}
{"input": "that $h$ fixes $V_\\alpha^L,$ so that $x\\in V_\\alpha^L \\implies h(x)\\in h(V_\\alpha^L)=V_\\alpha^L.$ Observe that every set in $N-V_\\alpha^L$ is finite except for $V_\\alpha^L,$ which is infinite because $|V_\\alpha^L|=\\kappa>\\omega$ by Proposition~\\ref{prop:countable_case}. Also, $h(V_\\alpha^L)$ must be an infinite set too by injectivity of $h$ on $(N,\\in),$ so we will be done if we can show that $h(V_\\alpha^L)\\notin V_\\alpha^L.$ But, $h(V_\\alpha^L)\\in V_\\alpha^L$ is clearly not possible because repeated applications of $h$ to this relation can give an infinitely descending chain $V_\\alpha^L\\ni h(V_\\alpha^L)\\ni h^2(V_\\alpha^L)\\ni\\ldots.$ The other fact we need is that indeed $h$ is nontrivial on $V_\\alpha^L.$ Suppose that this is not the case. We have already shown that $h$ fixes $V_\\alpha^L$ as well as everything outside of $N$ and $N^c.$ A simple inductive argument using the fact that $h(\\{x_1,\\ldots,x_n\\})=\\{h(x_1),\\ldots, h(x_n)\\},$ for any $n\\in\\omega,$ will easily demonstrate that $h$ must fix everything in $N-(V_\\alpha^L\\cup \\{V_\\alpha^L\\})$ too. This contradicts the fact that $h:(G,E)\\rightarrow (G,E)$ is nontrivial. We are now ready to conclude our proof. Fix a formula $\\psi(a_1,\\ldots,a_m)\\in \\FORM$ for some $m>1,$ the case where $\\psi$ is a single variable formula is the same. We have, for any $x_1,\\ldots,x_m\\in V_\\alpha^L,$ \\begin{align*} V_\\alpha^L \\satisfies \\psi[x_1,\\ldots,x_m] \\iff& (q_{\\ulcorner \\psi(a_1,\\ldots,a_m)\\urcorner},\\langle x_1,\\ldots, x_m\\rangle^c)\\in E \\\\ \\implies& (h(q_{\\ulcorner \\psi(a_1,\\ldots,a_m)\\urcorner}),h(\\langle x_1,\\ldots, x_m\\rangle^c))\\in E \\\\ \\iff& (q_{\\ulcorner \\psi(a_1,\\ldots,a_m)\\urcorner},\\langle h(x_1),\\ldots, h(x_m)\\rangle^c)\\in E \\\\ \\iff& V_\\alpha^L\\satisfies \\psi(h(x_1),\\ldots, h(x_m)). \\end{align*} By using $\\neg \\psi$ in place of $\\psi$ in the above displayed argument, we can also reverse that single one way implication in the second line."}
{"input": "\\section{Subsets of $\\mathbb{R} \\times\\OR$} In this section, we generalize the theorem from the previous section to sets $G\\subset \\mathbb{R}\\times \\OR.$ We take $\\mathbb{R}$ to be the set of all binary sequences of length $\\omega,$ i.e., $\\mathbb{R}=2^\\omega.$ The relation ${<}\\subset \\mathbb{R}\\times\\mathbb{R}$ is the usual ``less than'' relation of real numbers. We will need the following lemma first: \\begin{lemma}\\label{lem:reals_case_no_countable_subset} Every set $G\\subset \\mathbb{R}\\times \\OR$ with no countable subset has a strongly rigid relation. \\end{lemma} \\begin{proof} Define the relation $E$ on $G$ by setting $(r_1,\\xi_1)\\mathrel{E}(r_2,\\xi_2)$ iff $\\xi_1<\\xi_2$ or $\\xi_1=\\xi_2\\ \\land\\ r_1<r_2.$ Suppose $h:(G,E)\\rightarrow (G,E)$ is a nontrivial endomorphism, and let $(r,\\xi)\\in G$ be such that $h((r,\\xi))\\neq (r,\\xi).$ Without loss of generality, we may assume $(r,\\xi)\\mathrel{E} h((r,\\xi)).$ Repeated applications of $h$ to this relation will give $(r,\\xi)\\mathrel{E} h((r,\\xi))\\mathrel{E}h^2((r,\\xi))\\mathrel{E}\\ldots.$ But, this chain forms a countable subset of $G,$ contrary to the assumption that $G$ has no countable subsets. \\end{proof} Let $\\pi_2:V\\rightarrow V$ be the operation defined by $(x_1,x_2)\\mapsto x_2.$ For any finite sequence $s=\\langle x_0,\\ldots,x_{n-1}\\rangle\\in V^{<\\omega},$ define $\\length(s)=\\domain(s)=n.$ An ordinal is said to be \\emph{odd} iff it can be written in the form $\\alpha+2n+1$ for some limit ordinal $\\alpha$ and some $n\\in\\omega.$ An ordinal is \\emph{even} iff it is not odd. Denote by $\\OR^{\\mathrm{o}}$ and $\\OR^{\\mathrm{e}}$ the classes of odd and even ordinals, respectively. \\begin{theorem}\\label{thm:reals_case} If there exists a set $G\\subset \\mathbb{R}\\times \\OR$ for which there is no strongly rigid relation, then for some ordinal $\\alpha$ there exists a nontrivial elementary embedding $j:V_\\alpha^L\\rightarrow V_\\alpha^L.$ \\end{theorem} \\begin{proof} Take such a set $G."}
{"input": "we will use the following notation: Given any two sets $X,Y$ of vertices, instead of saying, for all $a\\in X$ and all $b\\in Y,$ $(a,b)\\in E,$ we will simply say $(X,Y)\\in E^*.$ In a figure, while a small black node represents a vertex, sets of vertices will be represented by a slightly bigger node that is white with a black circumference (see Figure~\\ref{fig:reals_case_countable_subset_infinite_kappa} for an example). The fact that $(X,Y)\\in E^*$ will be represented by a dotted arrow from the node for $X$ (or $a$ if $X=\\{a\\}$) to the node for $Y$ (or $b$ if $Y=\\{b\\}$). Unlike a dotted arrow, an arrow with dots and dashes only indicates that there are some, but not necessarily all possible, arrows from the vertices at its tail to the vertices at its head. Let $\\kappa^{\\mathrm{e}}=\\kappa\\cap\\OR^{\\mathrm{e}}$ and $\\kappa^{\\mathrm{o}}=\\kappa\\cap\\OR^{\\mathrm{o}},$ and observe that $\\kappa=|\\kappa^{\\mathrm{e}}|=|\\kappa^{\\mathrm{o}}|.$ Partition $K$ into the two sets $K^{\\mathrm{e}}=\\{(r,\\xi)\\in K\\mid \\pi_2(\\xi)\\in \\kappa^{\\mathrm{e}}\\}$ and $K^{\\mathrm{o}}=\\{(r,\\xi)\\in K\\mid \\pi_2(\\xi)\\in \\kappa^{\\mathrm{o}}\\}.$ Similar to the proof of Theorem~\\ref{thm:ordinal_case}, we can fix $V_\\alpha^L$ such that $|V_\\alpha^L|=\\kappa$ and take $N,$ and its copy $N^c,$ such that $N\\supset (V_\\alpha^L\\cup\\{V_\\alpha^L\\})$ is the smallest set closed under taking finite subsets. As $|N|=\\kappa=|\\kappa^{\\mathrm{e}}|,$ we can take a bijection $f:\\kappa^{\\mathrm{e}}\\rightarrow N,$ and use it to replace each $(r,\\xi)\\in K^{\\mathrm{e}}$ with $(r,f(\\xi)).$ Thus, we may assume that $\\pi_2[K^{\\mathrm{e}}]=N.$ Similarly, we may assume that $\\pi_2[K^{\\mathrm{o}}]=N^c.$ We partition each of $K^{\\mathrm{e}}$ and $K^{\\mathrm{o}}$ by the equivalence $(r,x)\\sim (s,y)$ iff $x=y,$ and denote the class of $(r,x)$ by $[x].$ Let $\\subbar{N}=\\{[x]\\mid x\\in N\\}$ and $\\subbar{N^c}=\\{[x^c]\\mid x^c\\in N^c\\}."}
{"input": "$\\subbar{\\in}\\subset \\subbar{N}\\times \\subbar{N}$ by $[x]\\relsubbar{\\in}[y] \\iff x\\in y,$ for all $x,y\\in N.$ Similarly, define $\\subbar{<_L^c}\\subset \\subbar{N^c}\\times \\subbar{N^c}$ and $<_L^c\\subset N^c\\times N^c$ by $[x^c]\\relsubbar{<_L^c} [y^c] \\iff x^c<_L^c y^c \\iff x<_L y,$ for all $x^c,y^c\\in N^c.$ Consider the graph $(G,E)$ in Figure~\\ref{fig:reals_case_countable_subset_infinite_kappa}, where \\begin{enumerate}[label=G\\arabic*] \\item $(\\{w_0\\},[x]),([x],[x^c]),(\\{w_1\\},[x^c])\\in E^*,$ for every $x\\in N.$ \\item $([x],[y])\\in E^*$ iff $[x]\\relsubbar{\\in}[y],$ for every $[x],[y]\\in\\subbar{N}.$ \\item $([x^c],[y^c])\\in E^*$ iff $[x^c]\\relsubbar{<_L^c}[y^c],$ for every $[x^c],[y^c]\\in \\subbar{N^c}.$ \\item $(\\{p_n\\},[x^c])\\in E^*$ iff $|x|=n,$ for every $n\\in\\omega$ and every $[x^c]\\in \\subbar{N^c}.$ \\item For every $n\\in \\omega$ and every $[x^c]\\in \\subbar{N^c},$ $(\\{q_n\\},[x^c])\\in E^*$ iff one of the followings hold: \\begin{enumerate}[label=(\\alph*)] \\item $x\\in V_\\alpha^L,$ $n=\\ulcorner\\psi(a)\\urcorner$ for some $\\psi(a)\\in\\FORM,$ and $V_\\alpha^L\\satisfies \\psi[x].$ \\item $x=\\langle x_1,\\ldots,x_m\\rangle$ for some $m>1,$ $x_1,\\ldots,x_m\\in V_\\alpha^L,$ $n=\\ulcorner \\psi(a_1,\\ldots,a_m)\\urcorner$ for some $\\psi(a_1,\\ldots,a_m)\\in \\FORM,$ and $V_\\alpha^L\\satisfies \\psi[x_1,\\ldots,x_m].$ \\end{enumerate} \\item $(e_s,(r,\\xi))\\in E$ iff $r\\vert_{\\length(s)}=s,$ for all $s\\in 2^{<\\omega}$ and all $(r,\\xi)\\in K.$ \\end{enumerate} Figure~\\ref{fig:dotted_lines_expanded} depicts what the dotted arrows represent by showing the subgraph of $(G,E)$ induced by $\\{w_0\\}\\cup [x] \\cup[x^c].$ \\begin{figure}[htb!] \\centering \\includegraphics[page=1]{images/figures_strong_rigidity.pdf} \\caption{} \\label{fig:reals_case_countable_subset_infinite_kappa} \\end{figure} \\begin{figure}[htb!] \\centering \\includegraphics[page=2]{images/figures_strong_rigidity.pdf} \\caption{} \\label{fig:dotted_lines_expanded} \\end{figure} Since $G$ has no strongly rigid relation, we get a nontrivial endomorphism $h:(G,E)\\rightarrow(G,E).$ Again, $h$ must fix every vertex in $H,$ and the two sets $K^{\\mathrm{e}}$ and $K^{\\mathrm{o}}$ are each closed under $h$ by G1. We want to prove that if a member of some class $[x]$ is sent by $h$ to a member of some class $[y],$ then every member of $[x]$ is sent to a member of $[y]."}
{"input": "the classes in $\\subbar{N^c}$ is done similarly. To do this, we will prove that $h((r_1,x))\\in [y_1]\\ \\land\\ h((r_2,x))\\in[y_2]\\implies y_1=y_2,$ for every $x,y_1,y_2\\in N$ and every $(r_1,x),(r_2,x)\\in[x].$ Suppose, towards a contradiction, that $h((r_1,x))\\in [y_1]$ and $h((r_2,x))\\in [y_2],$ but $y_1\\neq y_2.$ We know that there exists some real number $s$ such that $(s,x^c)\\in K^{\\mathrm{o}},$ because $\\pi_2[K^{\\mathrm{o}}]=N^c\\ni x^c.$ By G1, $((r_1,x),(s,x^c))\\in E.$ Applying $h$ to this, we get $(h((r_1,x)),h((s,x^c)))\\in E.$ By closure of $K^{\\mathrm{e}}$ and $K^{\\mathrm{o}},$ $h((r_1,x))\\in K^{\\mathrm{e}}$ and $h((s,x^c))\\in K^{\\mathrm{o}}.$ Therefore, by G1, $\\pi_2(h((s,x^c)))=\\pi_2(h((r_1,x)))^c=y_1^c.$ But, by symmetry, we also get $\\pi_2(h((s,x^c)))=\\pi_2(h((r_2,x)))^c=y_2^c.$ This is a contradiction since $y_1\\neq y_2.$ Using the above, we can define a function $j:N\\cup N^c\\rightarrow N\\cup N^c,$ by letting $j(x)=y$ iff $h((r,x))=(s,y)$ for some $(r,x)\\in [x]$ and $(s,y)\\in [y].$ Now, by dropping the notations $\\subbar{-}$ and $[-],$ we can argue for elementarity of $j\\vert_{V_\\alpha^L}:V_\\alpha^L\\rightarrow V_\\alpha^L$ just as we argued for elementarity of $h\\vert_{V_\\alpha^L}$ in the proof of Theorem~\\ref{thm:ordinal_case}. There will be just one detail that will be different, and that is, for the injectivity argument, we will need the fact that there are no arrows between vertices belonging to the same class. Using that, we can argue that the members of two distinct $[x^c],[y^c]$ cannot be sent by $h$ to the same class $[z^c].$ Observe that, although we have established the elementarity of $j\\vert_{V_\\alpha^L},$ the argument in Theorem~\\ref{thm:ordinal_case} is not sufficient to prove that $j\\vert_{V_\\alpha^L}$ is nontrivial."}
{"input": "\\section{A Model for $\\neg \\AC+\\SRR$} We are now ready to build a model for $\\ZF+\\neg\\AC+\\SRR,$ thus establishing that $\\SRR$ does not imply $\\AC.$ \\begin{theorem}\\label{thm:neg_ac_plus_srr} If $\\ZF$ is consistent, then so is $\\ZF+\\neg\\AC+\\SRR.$ \\end{theorem} \\begin{proof} Work inside $V.$ We can assume that there are no inaccessible cardinals in $L$ (if there are any, then simply work inside $V_\\kappa^L$ for $\\kappa$ the least inaccessible in $L).$ Given this assumption and Corollary~\\ref{cor:no_inaccessibles_to_srr}, the same model that worked for Hamkins and Palumbo \\cite{hamkins} for the consistency of $\\ZF+\\neg\\AC+\\RR$ will work for us too. The model in question is the symmetric Cohen model $M,$ built as follows: Let $P=\\operatorname{Add}(\\omega,\\omega)$ be the usual forcing notion that adds countably many Cohen reals. Thus, $P=\\{p:\\omega\\times\\omega \\rightarrow\\{0,1\\}\\mid p\\textrm{ is finite}\\}$ and $p\\leq q\\iff p\\supset q,$ for all $p,q\\in P.$ Every permutation $\\pi:\\omega\\rightarrow \\omega$ induces an automorphism $\\bar\\pi:P\\rightarrow P$ by letting $$ \\bar\\pi(p)=\\{(\\pi(n),m)\\mid (n,m)\\in p\\}. $$ This automorphism, in turn, induces an automorphism $\\hat\\pi:V^P\\rightarrow V^P$ of the class of $P$-names by the recursive definition $$ \\hat\\pi(\\tau)=\\{(\\hat\\pi(\\sigma),\\bar\\pi(p))\\mid(\\sigma,p)\\in\\tau\\}. $$ It is easy to prove, using induction on complexity of formula, that $p\\forces_P \\psi[\\tau] \\iff \\bar\\pi(p)\\forces_P \\psi[\\hat\\pi(\\tau)].$ A $P$-name $\\tau$ is said to be \\emph{symmetric} iff there exists a finite set $e\\subset \\omega$ such that whenever $\\pi:\\omega\\rightarrow \\omega$ is a permutation that fixes every member of $e,$ then $\\hat\\pi(\\tau)=\\tau.$ The class of hereditarily symmetric names is denoted by $\\mathrm{HS}."}
{"input": "\\section{Proto Berkeley Cardinals} \\begin{theorem}\\label{thm:proto_berkeley_cardinals} A cardinal $\\delta$ is proto Berkeley iff for any graph $(G,E)$ such that $\\aleph(G)>\\delta$ and any injection $f:\\delta\\rightarrow G,$ there is an endomorphism $h:(G,E)\\rightarrow(G,E)$ such that $h\\vert_{\\range{f}}\\neq \\id.$ \\end{theorem} \\begin{proof} From left to right is easy, so we do that first. Fix any $(G,E)$ and $f:\\delta\\rightarrow G$ as in the statement of the theorem. Let $V_\\alpha$ be such that $\\langle G,E,f\\rangle \\in V_\\alpha,$ and let $M=V_\\alpha\\cup\\{V_\\alpha,\\{\\langle G,E,f\\rangle,V_\\alpha\\}\\}.$ Clearly, $M$ is a transitive set and $\\delta\\in M.$ By proto Berkeleyness, we can find an elementary embedding $j:M\\rightarrow M$ that has critical point $\\kappa$ strictly below $\\delta.$ The pair $\\{\\langle G,E,f\\rangle,V_\\alpha\\}$ is definable in $M$ as the unique set with exactly two members, one of which is a set that has every other set as a member. It easily follows from this that each of $G,E,f$ is definable in $M,$ and we deduce that $j$ must fix each one of them. This shows that $j\\vert_G:(G,E)\\rightarrow(G,E)$ is an endomorphism. Also, $j(f(\\kappa))=j(f)(j(\\kappa))=f(j(\\kappa))\\neq f(\\kappa),$ so that $j\\vert_G$ is not the identity on the range of $f.$ For the other direction, fix a transitive set $M$ such that $\\delta\\in M.$ Our aim is to show that there is a nontrivial elementary embedding $j:M\\rightarrow M$ with critical point strictly below $\\delta.$ The idea of the proof is similar to that of the proof of Theorem~\\ref{thm:ordinal_case}. Thus, let $N\\supset M\\cup\\{M\\}$ be the smallest transitive set closed under taking finite subsets, and let $N^c$ be a copy of it."}
{"input": "$x=\\langle x_1,\\ldots,x_m\\rangle$ for some $m>1,$ $x_1,\\ldots,x_m\\in M,$ $n=\\ulcorner \\psi(a_1,\\ldots,a_m)\\urcorner$ for some $\\psi(a_1,\\ldots,a_m)\\in \\FORM,$ and $M\\satisfies \\psi[x_1,\\ldots,x_m].$ \\end{enumerate} \\item $(\\xi_1,\\xi_2)\\in E$ iff $\\xi_1<\\xi_2,$ for every $\\xi_1,\\xi_2\\in \\alpha+1.$ \\end{enumerate} \\begin{figure}[htb!] \\centering \\includegraphics[page=4]{images/figures_strong_rigidity} \\caption{} \\label{fig:berkeley_case} \\end{figure} We remark that although $A$ and $N$ have some common ordinals, we really mean that they are disjoint and merely use their real names for convenience. Normally one would use a copy of, say, the set $A,$ just as we did with $N.$ We will aim to make it clear from the context which copy of a given ordinal we are talking about. Since $\\aleph(G)>\\delta,$ we can fix an endomorphism $h:(G,E)\\rightarrow (G,E)$ that is nontrivial on $\\delta\\subset N,$ if we take as $f$ the injection $\\id:\\delta\\rightarrow \\delta\\subset N.$ That will take care of the critical point being strictly below $\\delta\\in M,$ and we only need to show that $h\\vert_M:M\\rightarrow M$ is an elementary embedding. Although we cannot argue that every member of $A$ is fixed by $h,$ we can, nonetheless, argue that $\\alpha\\in A$ is fixed by $h.$ Using that, we can proceed as in the proof of Theorem~\\ref{thm:ordinal_case} and show that $h\\vert_M:M\\rightarrow M$ is indeed an elementary embedding, which will finish the proof. So, let us show that $\\alpha$ is fixed by $h.$ Henceforth, every ordinal we name belongs to $A,$ and not $N.$ First, we argue that $h(\\beta)\\in A,$ for all $\\beta<\\alpha.$ Working towards a contradiction, fix $\\beta<\\alpha$ such that $h(\\beta)\\notin A."}
{"input": "by $x^{c_1}\\notin_\\rk^{c_1} y^{c_1}\\iff x\\notin_\\rk y,$ for all $x^{c_1},y^{c_1}\\in N^{c_1}.$ Consider the graph $(G,E)$ in Figure~\\ref{fig:another_solution_for_injectivity}, where \\begin{enumerate}[label=G\\arabic*] \\item $(x,x^{c_1}),(x^{c_1},x^{c_2})\\in E,$ for every $x\\in N$ \\item $(x,y)\\in E$ iff $x\\in y,$ for every $x,y\\in N$ \\item $(x^{c_1},y^{c_1})\\in E$ iff $x^{c_1} \\notin_\\rk^{c_1} y^{c_1},$ for every $x^{c_1},y^{c_1}\\in N^{c_1}$ \\item $(x^{c_2},y^{c_2})\\in E$ iff $x^{c_2}<_\\rk^{c_2} y^{c_2},$ for every $x^{c_2},y^{c_2}\\in N^{c_2}.$ \\end{enumerate} \\begin{figure}[htb!] \\centering \\includegraphics[page=8]{images/figures_strong_rigidity} \\caption{} \\label{fig:another_solution_for_injectivity} \\end{figure} Clearly, this graph does not contain any cycles by wellfoundedness of $\\in$ and $<_\\rk.$ We can use this graph as part of a bigger graph which ensures that each of the copies of $N$ are closed under any endomorphism $h,$ and which also ensures elementarity of the appropriate restrictions of $h.$ Here, we will only show: \\begin{proposition} If $(G,E)$ is the graph in Figure~\\ref{fig:another_solution_for_injectivity} and $h:(G,E)\\rightarrow (G,E)$ is any endomorphism, under which each of the copies of $N$ are closed, then $h$ is injective on $N$ and behaves the same across all copies of $N.$ \\end{proposition} \\begin{proof} Fix such an endomorphism $h.$ By G1, \\begin{enumerate}[label=C\\arabic*] \\item $h(x^{c_1})=h(x)^{c_1},$ for every $x\\in N$ \\item $h(x^{c_2})=h(x)^{c_2},$ for every $x\\in N.$ \\end{enumerate} This shows that $h$ behaves the same across all copies of $N.$ Now, for injectivity, fix any $x,y\\in N$ such that $x\\neq y.$ If $x<_\\rk y$ or $y<_\\rk x,$ then $h(x)<_\\rk h(y)$ or $h(y)<_\\rk h(x)$ by G4, applying $h,$ and C2. Thus, in such cases, $h(x)\\neq h(y).$ Suppose now that $\\rank(x)=\\rank(y).$ Fix $a\\in N$ such that, without loss of generality, $a\\in x$ but $a\\notin y.$ $a\\in x$ implies $\\rank(a)<\\rank(x)=\\rank(y)."}
{"input": "effective predators in insecticide-free environments, when insecticides are applied, they become vulnerable, which increases their mortality. Although there is growing recognition of these interactions, limited research exists on the long-term population dynamics of predator-prey systems and the predation behaviour of resistant natural enemies when insecticides are applied, particularly considering the coexistence with susceptible natural enemies, their increased mortality, and the resulting ecological effects. The use of mathematical models allows simulations and analysis of complex dynamics. When integrating data about foraging behaviour, predation rates, and functional response of predators in different scenarios, models can predict the impacts of different management strategies for predator-prey populations. These models are particularly valuable to test hypotheses and scenarios that could be logistically difficult or unfeasible to realize experimentally \\citep{ferreira2014ecological}. Mathematical modelling is a tool with applications in biological control, providing valuable insights into interactions between insects and agricultural practices, with the resulting ideas and frameworks having broad ecological application \\citep{alexandridis2021models, moral2023modelling, mcevoy2018theoretical}. Furthermore, previous studies have used mathematical models to explore scenarios involving insecticide applications in predator-prey systems, and other studies have incorporated resistance evolution in pests or natural enemies \\citep{janssen2021pesticides, trumper1998modelling, tabashnik1986evolution, liu2021modelling, jana2013mathematical, tang2005integrated} but in different contexts from the one we address here. In this study, we integrate resistance dynamics, considering fitness costs and changes in the predatory efficiency of resistant predators into a specific ecological context."}
{"input": "the same as those defined in the initial conditions. However, when an application occurs, the mathematical function is triggered, resulting in a decrease in the values of $r_1$ and $r_2$ close to the application time, followed by a slow recovery until the new values of $r_1$ and $r_2$ are reached. In our model, we assume that in scenarios where aphid resistance is selected, the final $r$-values are higher than the initial ones. This assumption is based on findings from \\cite{wang2020feeding,ma2019fitness}, which indicates that resistant populations exhibit higher intrinsic growth rates than susceptible ones. This increase in $r$-values is incorporated into the model to reflect the adaptive advantage observed in resistant populations. Conversely, in scenarios where resistance is not selected, we assume that $r$-values decrease following insecticide applications. This assumption is supported by studies such as \\cite{liu2022sublethal, qiu2024intergenerational}, which report negative effects of insecticide exposure on the growth rates of susceptible populations. To capture this dynamic, the model includes a functional decrease in the $r$-values after insecticide application, simulating the costs associated with exposure. In the model, the function is defined as shown in \\autoref{eq:growth_rate}: \\begin{equation} \\label{eq:growth_rate} r_i(t) = \\begin{cases} r_{i0}, & t \\leq t_{\\text{app}},\\ i \\in \\{1,2\\}, \\\\ \\begin{aligned}[t] &-\\alpha + e^{-\\beta \\cdot t} \\\\ &\\quad + S\\left(t, r_{\\mathit{new},i}, t_{\\text{app}} + \\theta, \\frac{1}{\\gamma}\\right), \\end{aligned} & t > t_{\\text{app}},\\ i \\in \\{1,2\\}."}
{"input": "to the new $r$ value for population \\( i \\). The parameter $\\alpha$ represents the growth level, $\\beta$ is the decline speed, $\\theta$ defines the (\\({delay}\\)) between application and the onset of changes, $\\gamma$ determines the growth speed of $r$, and $S(t)$ is a logistic function. In other words, they determine the transition dynamics. The third and fourth equations describe the rate of change of the susceptible ($P_1$) and resistant ($P_2$) \\textit{E. connexa} subpopulations. In both aspects of the equations, converting consumed prey ($N_1$ and $N_2$) into new predators within the same time interval involves the structure of the type II functional response and the conversion rate. However, in the first part of the equations, the effect of the conspecific predator's mortality rate is included, which has a function to vary the parameter's value according to insecticide application. The exponential function models a process where the value does not change when there is no insecticide application. However, after this point, the value begins to grow exponentially until it reaches a new value while simultaneously undergoing an exponential decline that starts with a certain delay. In this way, the function only increases the mortality rate when insecticide applications occur and then returns to values close to the natural mortality rate. The increase in value for susceptible ladybirds is higher than for resistant ones."}
{"input": "d_{i0} &+ (d_{i0} + (d_{\\text{new},i} - d_{i0}) \\cdot e^{-\\gamma \\cdot (t-t_{\\text{app}})} \\\\ &\\quad - \\alpha \\cdot e^{-\\beta \\cdot (t-(t_{\\text{app}}+\\theta))}, \\end{aligned} & t > t_{\\text{app}},\\ i \\in \\{1,2\\} \\end{cases} \\end{equation} where: $d_{i0}$ is the initial mortality rate for population \\( i \\) (before insecticide application), \\( d_{\\text{new},i} \\) is the mortality rate after insecticide application, \\( t_{\\text{app}} \\) is the insecticide application time. After application, the parameters $\\alpha$ (\\({growth\\_level}\\)), $\\beta$ (\\({decline\\_speed}\\)), $\\theta$ ((\\({delay}\\)), and $\\gamma$ (\\({growth\\_speed}\\)) control the transition dynamics. This function ensures that before insecticide application, the mortality rate remains constant, and after application, it dynamically adjusts to reflect the expected changes in mortality rate. \\subsubsection{Initial conditions and parameter values} \\label{subsec:variables} The initial densities in the standard simulations were $30$ individuals for each pest species ($N_1$ and $N_2$), $10$ susceptible ladybirds ($P_1$), and 5 resistant ladybirds ($P_2$). Most parameter values for simulations were taken from a literature survey focusing on the species of aphids as the pest insect and ladybirds, especially \\textit{E. connexa} as the predator insect. Estimated values for intrinsic growth rate ($r_1$ and $r_2$) were available for the studies from \\textit{A. gossypii} and \\textit{M. persicae}. The first focused on cotton being the host plant \\citep{kerns2000sublethal}, and for \\textit{M. persicae} a range of host plants was researched because no study was found in cotton. Therefore, we selected an intermediate value within this range that closely matched and best represented the data found based on the study by \\cite{wang2018laboratory}."}
{"input": "study of a pest–predator system involving \\textit{E. connexa} and mites by \\cite{matos2020eriopis}. While the current model considers aphids as prey, we also reviewed the literature on other Coccinellidae species preying on aphids to ensure that the parameter values closely align with those observed for mite-based systems \\citep{lee2004306, jalali2017effects, bayoumy2018foraging, moradi2020foraging}. This cross-referencing helped validate the applicability of the selected values, maintaining ecological relevance across prey types. To estimate the parameters for resistant \\textit{E. connexa}, we referenced the study carried out by \\cite{lira2019predation}. This study reported that resistant ladybirds exhibited approximately a $46\\%$ reduction in attack rate and an $8\\%$ decrease in handling time compared to susceptible individuals. Accordingly, our model incorporates these same percentage reductions to reflect the behaviour of resistant \\textit{E. connexa}. In this study, we assume that conspecific predation among predators primarily reduces their growth rates. A direct method to model the mortality caused by conspecific killing in predators is through quadratic terms such as $d_2P_1^2$ e $d_3P_2^2$, as suggested by \\cite{lucasintraguild}. The quadratic formulation effectively captures the non-linear increase in mortality as predator density rises. However, it is noteworthy that $d_2$ and $d_3$ represent only the mortality due to conspecific predation and do not account for insecticide-induced mortality. The mortality caused by insecticide application is expressed by an exponential function, which is activated only during insecticide application periods. Therefore, as the literature predominantly provides mortality rate values for predators without explicitly accounting for conspecific predation, we used bifurcation analysis to investigate the sensitivity of this parameter."}
{"input": "reflect density dependence in the absence of empirical estimates, we assigned an arbitrary value to the mortality rate associated with conspecific predation. Various ecological factors, including cannibalism, can influence the density-dependent mortality rate of predators, making this assumption necessary for model exploration. It is well-documented that cannibalism rates among predators increase when aphids are absent or their density is low \\citep{grez2012biotic, cannibalism2017}. Experimental evidence of cannibalism in \\textit{E. connexa} is documented in \\cite{cannibalism2017, rocca2020larval}. To account for this, our model incorporates a baseline mortality rate that includes cannibalism-driven losses, and when insecticides are applied, we increase this value to reflect the compound effects. The collapse of an aphid population due to insecticide application is expected to intensify competition for food in ladybirds, leading to a rise in conspecific predation. This dynamic effectively increases the quadratic mortality rate, emphasizing the interplay between food scarcity and intraspecific interactions in such scenarios. To modulate the induced mortality of ladybirds, we adjusted parameters following a similar approach to that described by \\cite{survival_2013}, which reported survival rates of $3\\%$ for susceptible ladybirds and $84\\%$ for resistant ones after insecticide exposure. Therefore, these survival rates were used as proxies to approximate the effects of insecticide-induced mortality, as they primarily reflect external factors. \\subsection{Bifurcation} \\label{subsec:bifurcation} A numerical bifurcation analysis was conducted by varying selected model parameters while keeping the others constant, with a focus on the density-dependent mortality of the susceptible ladybird and the predation parameters of both subpopulations."}
{"input": "full results are provided in the Appendix. \\subsection{Population Dynamics}\\label{subsec:population} In addition to bifurcation diagrams, we also simulated a range of scenarios to better understand the population dynamics generated by this system. Different from bifurcation, here we simulated fixing the quadratic mortality parameter of the susceptible ladybird ($d_2$). We conducted a simulation with $d_2 = 0.5$, characterized by sustained oscillations in the population dynamics. With sustained oscillations, this scenario offers a dynamic system that is sufficiently stable but still responds clearly to changes in parameters or initial conditions, allowing us to assess how patterns of population persistence and extinction emerge from interactions in the system. Thus, it allows for a more explicit and detailed analysis of changes in population dynamics when we introduce applications or disturbances to the system. By focusing on this oscillatory behaviour, we highlight how different factors directly influence the patterns of interaction in a species and reinforce the importance of dynamic coupling to understand the system's responses as a whole. The simulations were conducted over $220$ time units, which represent days, in software R $4.4.1$ \\citep{softwareR}. We selected the value 220 because, in general, the cotton cycle lasts a maximum of $220$ days. In scenarios involving insecticide applications, these applications began after $30$ days of simulation, in which no immigration or emigration of individuals was assumed. The insecticide was considered to be applied in the field as a pulse application for one day once every twenty days."}
{"input": "0.21$; $T_{h_1}^{(s)} = 0.68$; $T_{h_2}^{(s)} = 0.73$; $a^{(r)}_1 = 0.10$; $a^{(r)}_2 = 0.11$; $T_{h_1}^{(r)} = 0.62$; $T_{h_2}^{(r)} = 0.67$; $d_2 = d_3 = 0.05$; $g^{(s)}_1 = g^{(s)}_2 = 0.2$; $g^{(r)}_1 = g^{(r)}_2 = 0.17$. In scenarios with insecticide application, the values of $r_1$, $r_2$, $d_2$, and $d_3$ are modified. The arrows indicate insecticide applications at 30, 50, and 70 days.} \\label{fig:image10} \\end{figure*} \\FloatBarrier \\subsection{Analysis of Population Dynamics} The analysis of population dynamics, varying the attack rate ($as$) and handling time ($Ths$) parameters for susceptible and resistant \\textit{E. connexa}, reveals distinct patterns in the behaviour of both prey populations (\\textit{A. gossypii} and \\textit{M. persicae}) and the predator across the simulations. \\autoref{fig:image11} shows the dynamic conforming changes in the susceptible ladybird parameters (attack rate and handling time). When the range of attack rate ($a^{(s)}_1$) is between $0 < a^{(s)}_1 < 0.25$, a drastic decline in the density of \\textit{M. persicae} (green line) is observed, while \\textit{A. gossypii} (blue line) exhibits a gradual decline, followed by oscillations without a clear growth pattern. This suggests that the decline in \\textit{M. persicae} allowed a small increase in \\textit{A. gossypii}. However, after $a^{(s)}_1 \\approx 0.25$, the density of \\textit{A. gossypii} entered a continuous decline and stayed with values close to zero. In contrast, the density of \\textit{M. persicae} begins to increase significantly, suggesting that intense predation on \\textit{A. gossypii} creates a competitive release for \\textit{M. persicae}, allowing its uncontrolled growth. The population of susceptible \\textit{E."}
{"input": "then even small environmental changes (such as insecticide exposure or fluctuations in prey availability) could shift population structures in unexpected ways. In this way, our simulations with insecticide scenarios revealed significant ecological implications, where we observed that insecticide application affects aphid and ladybird populations differently. In scenarios with the absence of insecticide application (\\autoref{fig:image10}a), susceptible ladybirds can control aphid populations, exhibiting typical predator-prey oscillations, while resistant ladybirds tend to become extinct because of adaptive costs associated with resistance \\citep{ferreira2013life}. However, with the application of insecticides (\\autoref{fig:image10}b and \\autoref{fig:image10}c), the increase in predator mortality because insecticide generates a different scenario. After an initial decline in aphid populations, they quickly recover, reaching high densities. Also, pest resurgence after insecticide application has been widely documented in studies on aphids \\citep{amad2003high, sial2018evaluation, ullah2020thiamethoxam, wang2023hormesis}. For instance, \\cite{janssen2021pesticides} demonstrated through modelling that pest resurgence is expected even when effective natural enemies are present, even when these predators are less sensitive to insecticides than the pests. Pest resurgence can be caused by several factors, including an increased growth rate due to the hormesis effect, a phenomenon in which exposure to sublethal doses of stressors (e.g., insecticides, heavy metals) triggers a biphasic biological response: low doses stimulate physiological or reproductive processes, while higher doses suppress them \\citep{calabrese2002defining}. Concurrently, a reduction in the population of natural enemies can further amplify resurgence by disrupting top-down biocontrol \\citep{guedes2016pesticide}. The reduction in natural enemies caused by insecticide exposure, allows prey populations to escape predator control, resulting in higher densities."}
{"input": "population control. Furthermore, although there are peaks in aphid populations during applications, after these peaks, the density does not reach the same high levels as in untreated scenarios. \\cite{spindola2013survival} found that resistant ladybirds have a survival rate of $82\\%$ compared to $3\\%$ in susceptible ones. The authors argue that even at low population densities, resistant \\textit{E. connexa} that survive insecticide application are expected to help minimize aphid numbers, highlighting the importance of these findings for integrated pest management strategies. In the scenario with aphid resistance (\\autoref{fig:image10}c), the reduced mortality of aphids allows resistant ladybirds to maintain their presence over time. However, the aphid population remains high during the applications, but the substantial increase in resistant ladybirds observed during the final application causes a significative decline in the aphid population, reducing them to very low values. This suggests that, aphid populations initially grow due to reduced predation, but their population collapses as the resistant predator population expands. Furthermore, the presence of resistant ladybirds at higher densities enables them to remain in the system for longer periods, maintaining more controlled prey densities even after applications cease. These findings suggest that the presence of resistant predators contributes to pest control, especially under conditions where aphid resistance complicates management efforts. Therefore, the model presented here suggests that the coexistence of susceptible and resistant ladybirds provides an \"ecological insurance\", where the presence of resistant individuals compensates for the high mortality rates observed in susceptible populations."}
{"input": "effective strategy for aphid control in environments where insecticide resistance has evolved. The delay in aphid decline suggests that there is a transitional period where resistant predators need time to establish themselves before effectively regulating the aphid population. This dynamic can be a useful strategy in integrated pest management, providing insights that can help in pest control. \\cite{spindola2013survival} proposed the release of resistant \\textit{E. connexa} in cotton crops based on their findings on insecticide selectivity. This approach appears viable, as the resistant phenotype is not entirely lost in the population in the absence of selection pressure, allowing for re-selection of resistance traits following insecticide applications \\citep{nascimento2023heterosis}. Although our model does not account for ladybird releases, the simulations suggest that a gradual increase in resistant populations in the field could significantly enhance aphid control. This topic represents a promising area for future research, with the potential to optimize the use of resistant predators in pest management programs. However, it is essential to consider potential trade-offs associated with resistance in predators. If resistant ladybirds exhibit reduced efficiency in prey consumption or reproductive capacity in the absence of insecticide pressure, their long-term persistence may be compromised. The resurgence of aphid populations in the final times highlights the long-term risk of pest recovery, which is probably because of the low recovery of susceptible ladybirds after insecticide applications, following the decrease in resistant ladybirds creating a competition scenario."}
{"input": "predator populations, even in the absence of direct mortality. Therefore, while the presence of resistant ladybirds is relevant for pest control, it cannot resolve the issue by itself. The reduction in aphid populations also affects ladybirds, particularly because of adaptive costs associated with resistance and the reduction of the aphid population. This highlights the need for a balanced approach that considers both pest and predator dynamics to maintain effective long-term pest management. \\cite{janssen2021pesticides}, in their modelling of insecticide application scenarios with and without predators, emphasized that insecticides negatively impact pest densities when natural enemies are absent, but this effect is mitigated in the presence of predators. In our model, without the presence of resistant ladybirds, a similar pattern may be found because of the significant mortality of susceptible ladybirds. However, the presence of resistant ladybirds prevents the aphid population from growing excessively during and after insecticide applications. The study by \\cite{nascimento2023heterosis} found that, in the absence of selection pressure, the resistance phenotype in \\textit{E. connexa} was reduced but not completely lost. This suggests that maintaining a population of resistant ladybirds for a period after applications, until susceptible ladybirds recover can help to control the pest. Another relevant consideration is that applying insecticides targeting non-aphid pests may indirectly influence aphid population dynamics. For example, the boll weevil, \\textit{Anthonomus grandis} Boheman (Coleoptera: Curculionidae), is one of the most severe cotton pests, particularly in Brazil."}
{"input": "\\citep{razaq2019insect, showler2007subtropical}. However, certain pyrethroids, such as lambda-cyhalothrin (LCT), exhibit ineffective efficacy against aphids, possibly leading to their proliferation or outbreaks \\citep{dong2022heat, zambrano2021side, ohara2022profile}. Therefore, maintaining populations of \\textit{E. connexa} in cotton fields treated with LCT could be a valuable strategy for aphid control. The authors \\cite{spindola2013survival} found that resistant \\textit{E. connexa} populations showed promising potential for survival and persistence under LCT applications, reinforcing their suitability for integrated pest management (IPM) programs targeting boll weevil or other non-target pests of ladybird in cotton fields. The population analysis at the end of the simulations, varying the attack rate and handling time parameters, showed again how these changes impact the system balance. Furthermore, in almost all scenarios, \\textit{M. persicae} remained close to zero due to its lower growth rate compared to \\textit{A. gossypii}. However, the results indicate that scenarios favouring its growth are possible. Indirect interactions between prey mediated by the same predator population can result in apparent mutualism (positive effects) or apparent competition (negative effects) \\citep{van2012prey, messelink2008biological}. Our results suggest that increasing the attack rate on \\textit{A. gossypii} reduces the predation pressure on \\textit{ M. persicae}, favouring its persistence and growth, which resembles an apparent mutualism. In contrast, increasing the attack rate on \\textit{M. persicae} does not relieve \\textit{A. gossypii} from predation, since ladybirds maintain a significant attack rate on \\textit{A. gossypii}, preventing its population growth. As the attack rate on \\textit{M. persicae}, its population is also consumed more intensively."}
{"input": "to the extinction of both populations, a scenario characterized by apparent competition. \\cite{costa2020occurrence}, in their modelling of scenarios involving two prey species and a single predator, found that, depending on the chosen parameter values, interactions could range from apparent mutualism to apparent competition. Their model suggests that increasing the carrying capacity of each prey species could initially lead to negative interactions (apparent competition), followed by positive interactions (apparent mutualism). Our results expand on this perspective by highlighting that the attack rate also plays a crucial role. Depending on the attack rate for one species, the effects on the other may be positive or negative. Lastly, the results of this study highlight the importance of mathematical models as tools to understand predator-prey dynamics under different conditions. Although the data were obtained from literature studies, experimental validation of the model predictions can help to adjust the parameters and increase their practical applicability. Furthermore, future research could incorporate additional variables, such as climatic factors, spatial dynamics, or the release of ladybirds, as previously mentioned. This would expand the range of simulated scenarios and further deepen our understanding of ecological interactions. For these reasons, this study enhances our understanding of the predation dynamics of \\textit{E. connexa}, highlighting how variations in behavioural rates can impact prey control. The simulations emphasize the significance of sustainable agricultural practices, including reduced insecticide applications and product rotations, and showcase the potential of mathematical models to inform integrated pest management strategies in agricultural systems."}
{"input": "\\section*{Appendix} \\label{app:appendix} \\addcontentsline{toc}{section}{Appendix} \\renewcommand{\\thefigure}{A.\\arabic{figure}} % Define o formato Fig. A.1 \\setcounter{figure}{0} \\subsection*{Bifurcation Analysis – Methodology} The bifurcation analysis was conducted by varying the parameter $d_2$, representing the density-dependent mortality of the susceptible ladybird. This analysis was conducted to explore the system's sensitivity to this parameter. The decision to focus on $d_2$ was based on a few considerations. Firstly, there are no well-defined values for the density-dependent mortality parameters ($d_2$ and $d_3$) in the literature, requiring an exploratory approach. With that in mind, the choice to perform the bifurcation on $d_2$ was grounded in the fact that this parameter is directly related to the population dynamics of the susceptible ladybird, which plays a more active role in the chosen system because we selected a scenario with no insecticide disturbances. In this scenario, the resistant ladybird population exhibits limited growth, being present in the system primarily with adaptive costs. Therefore, keeping $d_3$ constant simplifies the analysis and allows a focused examination of $d_2$ effects. By varying $d_2$, it is possible to identify how changes in the density-dependent mortality of the susceptible ladybird affect population persistence and extinction, providing insights into the system's stability boundaries. By identifying critical thresholds or points of bifurcation, we were able to pinpoint transitions among different dynamic regimes, such as stable equilibrium, oscillatory behaviour, or a population collapse. Bifurcation analysis was also performed on the ladybirds' predation parameters, i.e., attack rate and handling time."}
{"input": "Assume that \\cite[Conjecture $\\mathrm{C}_a(\\eps)$]{mx2y3} holds for all $\\eps >0$. Then \\begin{align*} \\sum_{x \\leq X^{1/2}} \\sum_{y \\leq X^{1/3}} \\Lambda(ax^2+by^3) = (1+o(1)) X^{5/6}. \\end{align*} \\end{theorem} \\subsection{Uniform equidistribution estimates} We define \\begin{align*} \\varrho_{a,h}(k):= \\#\\{ \\nu \\in \\Z/k\\Z: a\\nu^2+h \\equiv 0 \\pmod{k} \\} \\end{align*} The main technical results of this paper are the following two theorems that provide highly uniform asymptotics for the distribution of the roots of quadratic congruences. Theorems \\ref{thm:primefactor} and \\ref{thm:roots} are quick consequences of these by the sieve arguments in \\cite{mlargepf} and \\cite{DFIprimes}. It is convenient for us to define $A \\precprec B$ to mean $|A| \\leq X^{o(1)} B$. We let $\\theta$ denote the best exponent towards the Selberg eigenvalue conjecture, so that by the work of Kim and Sarnak \\cite{KimSarnak} $\\theta \\leq 7/64.$ \\begin{theorem} [Type I estimate] \\label{thm:typeI} Let $1 \\leq D \\leq K \\leq X^{2}$ and $D \\leq X^{1/2}$. Let $h$ be square-free with $1 \\leq h \\precprec X^{2}$ and let $1 \\leq a \\precprec 1$ with $\\gcd(a,h)=1$. Let $\\psi_1,\\psi_2$ be smooth functions supported on $[1,2]$ and $[-1,1]$, resp., such that for all $J \\geq 0$ we have $\\psi_i^{(J)} \\precprec_J 1$. Then \\begin{align*} \\sum_{d \\leq D}\\bigg| \\sum_{k \\equiv 0 \\pmod{d}} \\psi_1\\Bigl(\\frac{k}{K}\\Bigr) \\Bigl(&\\sum_{a \\ell^2 +h \\equiv 0 \\pmod{k}} \\psi_2\\Bigl(\\frac{\\ell}{X}\\Bigr) - \\frac{\\varrho_{a,h}(k) }{k} X \\int_\\R \\psi_2(u) \\d u \\Bigr) \\bigg|\\\\ & \\precprec \\, D^{1/2}X^{1/2}(D^{1/2}+h^{1/4}) \\Bigl(1+\\frac{X}{D(D+h^{1/2})}\\Bigr)^\\theta. \\end{align*} \\end{theorem} \\begin{theorem}[Type II estimate] \\label{thm:typeII} Let $M \\geq N$ with $MN \\geq X$ and $M \\leq X$."}
{"input": "of the roots of $\\ell^2+h \\equiv 0 \\pmod{k}$ in a narrow window $\\ell \\sim X$ in $\\Z/k \\Z$ with $X < K$. More precisely, we want to evaluate asymptotically \\begin{align*} &(\\text{Type I sums}) &\\sum_{d \\sim D} \\lambda_d \\sum_{\\substack{k \\sim K \\\\ k \\equiv 0 \\pmod{d}}} \\sum_{\\substack{\\ell \\sim X \\\\ \\ell^2+h \\equiv 0 \\pmod{k}}} 1 \\quad \\text{and} \\\\ &(\\text{Type II sums}) &\\sum_{m \\sim M} \\alpha_m \\sum_{\\substack{n \\sim N}} \\beta_n \\sum_{\\substack{\\ell \\sim X \\\\ \\ell^2+h \\equiv 0 \\pmod{mn}}} 1, \\quad \\quad \\end{align*} for as wide range of $D$ and $N$ as possible, see Theorems \\ref{thm:typeI} and \\ref{thm:typeII}. The case of Type II sums may be reduced to a more complicated version of the Type I sums by an application of Cauchy-Schwarz, and so we shall first focus on Type I sums in this sketch. We note that all previous results only consider bounded $h.$ Hooley \\cite{hooley} estimated the Type I sums by using the Gauss correspondence to connect them to Kloosterman sums and bounding them with Weil bound. Combining this with an upper bound sieve, Hooley obtained the exponent 1.1 in Theorem \\ref{thm:primefactor}. Deshouillers and Iwaniec \\cite{DIprimefactor} improved this to 1.2 by making use of sums of Kloosterman sums \\cite{deshoullieriwaniec}, which was improved to 1.2182 by de la Bretèche and Drappeau \\cite{BD} by sharpening the dependency on exceptional eigenvalue. In the work of the second author \\cite{mlargepf} Harman's sieve and Type II sums were introduced to the proof, improving the exponent to 1.279 unconditionally and 1.312 conditional to Selberg's eigenvalue conjecture."}
{"input": "the dependency on the exceptional eigenvalue was recently improved by Pascadi \\cite{pascadi2024large} who achieved 1.3. The last two results assume that $h=1$. In this article, we will take a different approach to estimating such sums, originating in the work of Duke, Friedlander, and Iwaniec \\cite{DFIprimes}. Following their approach, we consider symmetric matrices of determinant $h$ \\begin{align*} \\gf= \\det \\mqty(m & \\ell \\\\ \\ell & k) = mk -\\ell^2 = h. \\end{align*} The modular group $\\SL_2(\\Z)$ acts on symmertic matrices via $\\gf \\mapsto \\gamma \\gf \\gamma^t$, and the congruence $k \\equiv 0 \\pmod{d}$ is preserved by the Hecke congruence subgroup $\\Gamma_0(d)$. For a fixed modulus $d$ the sum can be parametrized as an average over a Poincaré type series with a smooth weight $F$ encoding $k \\sim K$ and $\\ell \\sim L$, that is, \\begin{align*} \\sum_{\\substack{k \\sim K \\\\ k \\equiv 0 \\pmod{d}}} \\sum_{\\substack{\\ell \\sim X \\\\ \\ell^2+h \\equiv 0 \\pmod{k}}} 1 = \\sum_{z \\in \\Lambda_h} \\sum_{\\tau \\in \\Gamma_0(d) \\backslash \\SL_2(\\Z)} \\alpha_{d,h}(\\tau z)\\sum_{\\gamma \\in \\Gamma_0(d)} F(\\gamma \\tau z). \\end{align*} Here $z$ ranges over the Heegner points $\\Lambda_h$ in the upper half-plane $\\H$ and for each $z$ the weight $\\alpha_{d,h}(\\tau z)$ restricts $\\tau$ to $d^{o(1)}$ cosets. In \\cite{DFIprimes} such sums are estimated by using the spectral theory of automorphic forms for each fixed $\\tau$, $z$, and $d$ separately."}
{"input": "divisor switching symmetry for the $d$ variable is fully preserved throughout the application of spectral methods. This is the reason that ultimately allows us to remove the dependency on exceptional eigenvalues here as well as for the Type II sums. As usual after Cauchy-Schwarz, we need to consider diagonal and off-diagonal contributions when bounding the terms $ \\sum_d \\langle \\alpha_{i} | \\Delta_d k_i | \\alpha_{i} \\rangle$. The new kernels may be written explicitly in terms of a sum over certain pairs of integer matrices $(\\g_1,\\g_2)$ with a congruence condition of the shape \\begin{align}\\label{eq:introcong} F(\\g_1,\\g_2) \\equiv 0 \\pmod{d}, \\end{align} for some polynomial $F$ in the entries of the matrices $\\g_1$ and $\\g_2$. The diagonal is then given by the set of pairs $(\\g_1,\\g_2)$ where $F(\\g_1,\\g_2) = 0$ over $\\Z$. There the congruence \\eqref{eq:introcong} is fulfilled trivially and the sum over $d$ results in a factor of $D$. However, we can make use of the fact that the diagonal is a sparse subset. In the off-diagonal, the congruence condition \\eqref{eq:introcong} is non-trivial and we can use a divisor bound to absorb the $d$ summation. For the Type I sums we are then able to show that essentially (cf. Propositions \\ref{prop:heegner} and \\ref{prop:lowertriang}) \\begin{align*} \\sum_{d \\sim D} \\langle \\alpha_{d,h} | \\Delta_d k_2 | \\alpha_{d,h} \\rangle &\\ll D h^{1/2} + h Z_2 \\\\ \\sum_{d \\sim D} \\langle I | \\Delta_d k_1 | I \\rangle & \\ll D + Z_1 +\\frac{K}{X}."}
{"input": "we applied sums of Kloosterman sums and the Kuznetsov formula, we would have to provide an exceptional spectral large sieve bound for \\begin{align*} \\sum_{d \\leq D} \\sum_{\\lambda_j=1/4-\\nu_j^2}^{\\Gamma_0(d)} Z^{2\\nu_j} \\bigg| \\sum_{n \\leq N} a_{n,d} \\varrho_j (n)\\bigg|^2. \\end{align*} Here the coefficients $a_{n,d}$ are intricately entangled with the level $d$, especially in the case of Type II sums. In \\cite{mlargepf} the results of Deshouillers and Iwaniec \\cite{deshoullieriwaniec} and de la Bretèche and Drappeau \\cite{BD} were used as a black box and the coefficients $a_{n,d}$ are essentially treated as arbitrary in some cases. For coefficients $a_{n,d}$ which do not depend on the level $d$, Deshouillers and Iwaniec \\cite{deshoullieriwaniec} improve the dependency on the exceptional eigenvalue by a divisor switching argument for the sums of Kloosterman sums. However, our divisor switching completely in physical space turns out to be more effective. Recently, Pascadi \\cite{pascadi2024large} improved the spectral large sieve for a fixed modulus when $a_{n,d}$ are exponential phases $e(n \\theta_d)$, or more general dispersion type coefficients. From the perspective of the spectral large sieve, our application of \\cite{GMtechnical} allows us to fully capture the entanglement of $a_{n,d}$ with the level $d$. We have restricted to the case of a positive discriminant, that is, $a,h \\geq 1$. It would be interesting to consider negative discriminants, combining our ideas with the approach in \\cite{toth}. \\subsection{Notations} We follow standard asymptotic Vinogradov and Landau notations."}
{"input": "\\Z \\Bigr\\} \\end{align*} and recall that the set of all integer matrices with determinant $h$ may be parametrised by $\\SL_2(\\Z)$ as a disjoint union \\begin{align*} \\mathrm{M}_{2,h}(\\Z) = \\bigsqcup_{\\sigma \\in H_h}\\SL_2(\\Z) \\sigma = \\bigsqcup_{\\sigma \\in H_h} h \\sigma^{-1}\\SL_2(\\Z). \\end{align*} We then define the Hecke operator acting on $f:\\G \\to \\C$ by \\begin{align} \\label{eq:heckeopdef} \\mathcal{T}_h f(g) := \\frac{1}{\\sqrt{h}} \\sum_{\\sigma \\in H_h} f\\bigl(\\tfrac{1}{\\sqrt{h}}\\sigma g\\bigr). \\end{align} For any function $f:\\G\\times \\G\\to \\C$ we define the two variables Hecke operators \\begin{align*} (\\mathcal{T}_{h_1,h_2} f)(\\g_1,\\g_2)= (\\mathcal{T}_{h_1})_{\\g_1}(\\mathcal{T}_{h_2})_{\\g_2} f(\\g_1,\\g_2), \\end{align*} where $(\\mathcal{T}_{h})_{\\g_i}$ is the Hecke operator $\\mathcal{T}_{h}$ acting on the $\\g_i$ coordinate. We define for any $R > 0$ and $\\g = \\smqty(a &b \\\\ c& d) \\in \\G$ the $R$-skewed hyperbolic size \\begin{align*} u_R(\\g) := \\tfrac{1}{4} (a^2+(b/R)^2+(cR)^2+d^2-2). \\end{align*} We denote $\\theta=\\theta(\\Gamma) := \\max\\{0,\\Re(\\sqrt{1/4-\\lambda_1(\\Gamma)})\\}$ with $\\lambda_1(\\Gamma)$ denoting the smallest positive eigenvalue of the hyperbolic Laplacian for $\\Gamma$. The Selberg eigenvalue conjecture states that we have $\\theta=0$ and the best unconditional bound for congruence subgroups is $\\theta \\leq 7/64$ by Kim and Sarnak \\cite{KimSarnak}. The following is Theorem 8.1 in \\cite{GMtechnical}. \\begin{theorem} \\label{thm:technical} Let $q \\geq 1$ and let $\\mathbf{X},\\mathbf{Y} > 0$ and $\\delta \\in (0,1)$ and suppose that $\\mathbf{X}/\\mathbf{Y} > \\delta$. let $f(x,y)$ be a smooth function, supported on $[-1,1]\\times[1,2]$ and satisfying $\\partial_x^{J_1} \\partial_y^{J_2} f \\ll_{J_1,J_2} \\delta^{-J_1-J_2}$. Denote $F:\\G \\to \\C, \\, F(\\smqty(a & b \\\\ c& d)):= f(\\frac{x}{\\mathbf{X}},\\frac{y}{\\mathbf{Y}})$ with $y=1/(c^2+d^2)$ and $x= y (ac+bd)$. Let $H \\geq 1$ and let $\\beta(h)$ be complex coefficients supported on $h \\leq H$. Let $\\alpha_1,\\alpha_{2,h} \\in \\mathcal{L}_c(\\G)$."}
{"input": "discriminant $h$ by \\begin{align*} \\Lambda_h := \\bigl\\{ z= \\frac{\\bb + i \\sqrt{h}}{\\cc} \\in \\mathcal{F}: \\aa,\\bb,\\cc \\in \\Z, \\, \\bb^2+h = \\mathfrak{a} \\cc \\Bigr\\}. \\end{align*} Then \\begin{align}\\label{eq:Heegnerhull} \\Lambda_h \\subseteq \\Bigl\\{\\frac{\\bb + i \\sqrt{h}}{\\cc} :\\aa,\\bb,\\cc \\in \\Z, \\,\\bb^2+h = \\aa \\cc, \\quad \\cc \\leq 2 \\sqrt{h}, \\quad |\\bb| \\leq \\frac{\\cc}{2}\\Bigr\\}. \\end{align} In particular, this implies that $\\# \\Lambda_h \\leq h^{1/2+o(1)}$. With this, a set of representatives for $\\SL_2(\\Z)$ acting on $\\mathrm{S}_h$ is given by $\\sigma \\sigma^t$ with \\begin{align*} \\sigma \\in \\mathrm{L}_h := \\Bigl\\{ \\sigma = \\smqty(\\ast & \\ast \\\\ 0 &\\ast) \\in \\SL_2(\\R): \\sigma i \\in \\Lambda_h \\Bigr\\}. \\end{align*} For $\\tau = \\smqty(\\ast & \\ast \\\\ c_0 & d_0) \\in \\SL_2(\\Z)$ and $\\g = \\smqty(\\aa & \\bb \\\\ \\bb & \\cc) \\in \\mathrm{S}_h$ we have \\begin{align} \\label{eq:cctransform} \\cc(\\tau \\diamond \\gf) = c_0^2 \\aa + 2 c_0 d_0 \\bb + d_0^2 \\cc. \\end{align} For $z= \\g i$ corresponding to the symmetric matrix $\\gf=\\g\\g^t$, we denote by abuse of notation $\\cc(z) = \\cc(\\gf)$ and similarly for $\\aa$ and $\\bb$. For any $q \\geq 1$ the congruences $\\cc \\equiv 0 \\pmod{aq}$ and $\\bb \\equiv 0 \\pmod{a}$ are fixed by the action of the Hecke congruence subgroup $\\Gamma_0(q)$ of level $q=ad$. Then the above discussion gives the following parametrisation. \\begin{lemma}\\label{lem:para} Let $\\gcd(a,h)=1$. Let $\\Gamma=\\Gamma_0(q)$ with $q=ad$ and let $\\Gamma_z$ denote the stabilizer of a point $z \\in \\H$. Let $T_q= \\Gamma \\backslash \\SL_2(\\Z)$."}
{"input": "\\section{Upper bounds for automorphic kernels} In this section we show how to bound the kernels that arise from an application of Theorem \\ref{thm:technical}. By making use of an average over the level $q$, we obtain the essentially optimal bounds. We need two types of bounds. The first considers functionals defined by averages over Heegner points (cf. Lemma \\ref{lem:para}), and the second functionals defined by averages over lower-triangular matrices (cf. Lemma \\ref{lem:para2}). \\subsection{Heegner points} Recall that we denote $\\mathcal{K}_q = \\mathcal{K}_\\Gamma$ with $\\Gamma=\\Gamma_0(q)$, and that we use $A \\precprec B$ to mean $|A| \\leq X^{o(1)} B$, where $X$ is the largest scale in the given context \\begin{proposition} \\label{prop:heegner} Let $h,Q,Z \\geq 1$. For $\\Gamma= \\Gamma_0(q)$ and $T_q= \\Gamma \\backslash \\SL_2(\\Z)$, let $\\alpha_q = \\alpha_{q,h}$ be the linear functional defined by \\begin{align*} \\langle f \\rangle_{\\alpha_q} := \\sum_{\\sigma \\in \\mathrm{L}_h} \\frac{1}{|\\Gamma_{\\sigma i}|}\\sum_{\\substack{\\tau \\in T_q \\\\ }} \\alpha_q(\\tau \\sigma) f(\\tau \\sigma), \\quad \\alpha_q(\\g):=\\mathbf{1}_{\\cc(\\g)\\equiv 0 \\pmod{q}}. \\end{align*} Then \\begin{align*} \\sum_{\\substack{q \\sim Q \\\\ \\gcd(h,q^2) \\, \\mathrm{ square-free} }} \\langle \\alpha_q|\\mathcal{K}_q k_{Z,1} | \\alpha_q\\rangle \\precprec Q h^{1/2} + h Z^{1/2}. \\end{align*} \\end{proposition} \\begin{proof} Let $k_{Z,1}(u) = \\mathbf{1}_{|u| \\leq Z}(1+u)^{-1/2}$ and $\\tau=\\tau_2$. Then recalling the point pair invariant $ u(w,z) = \\frac{|w-z|^2}{4 \\Im w \\Im z}$ so that $u_1(\\g) = u(\\g i,i)$."}
{"input": "be the linear functional defined by \\begin{align*} \\langle f \\rangle_{\\beta_{s,n_1,n_2}} := \\mathbf{1}_{\\gcd(n_1,n_2) = 1} \\sum_{|v| \\leq V}f(\\n[-sn_1 v \\,\\overline{n_1} ]^t), \\end{align*} where $\\overline{n_1} n_1 \\equiv 1 \\pmod{n_2}$. Then denoting $s=dn_0t^2$ we have \\begin{align*} \\sum_{t \\leq T} &\\sum_{n_0 \\leq N_0} \\sum_{n_1 \\leq N_1} \\sum_{n_2 \\leq N_2} \\sum_{\\substack{d | n_0n_1 n_2 \\\\ d > D}}\\langle \\beta_{s,n_1,n_2}|\\mathcal{K}_{sn_1n_2} k_{Z,R} | \\beta_{s,n_1,n_2} \\rangle \\\\ & \\precprec N_0 T V (1+R) \\Bigl(N_1 N_2 + N_1 V+N_2V\\Bigr)+ V^2 ((DR)^{-1} + Z^{1/2}). \\end{align*} \\end{proposition} The reader may in the first pass want to pretend that $t=n_0=d=1$, which is the generic case as these variables arise from dealing with certain gcd conditions in the proof. In that case for $N_1=N_2$ the bound simplifies to \\begin{align*} V^2(R^{-1} + Z^{1/2}) + (1+R) (V^2 N + V N^2). \\end{align*} \\begin{proof} We want to estimate for $\\gcd(n_1,n_2)=1$ \\begin{align*} K(s,n_1,n_2) :&= \\langle \\beta_{s,n_1,n_2}|\\mathcal{K}_{sn_1n_2} k_{Z,R} | \\beta_{s,n_1,n_2} \\rangle\\\\ &=\\sum_{|v|,|v'| \\leq V}\\sum_{\\gamma\\in \\Gamma }k_{Z,R}(\\n[sn_1 v' \\overline{n_1}]^t\\gamma \\n[-sn_1 v \\overline{n_1}]^t), \\end{align*} where $\\Gamma=\\Gamma_0(sn_1n_2)$ and \\begin{align*} k_{Z,R}(\\g) \\ll \\frac{\\mathbf{1}\\{ |a| + |b|/R + R |c| + |d| \\leq Z^{1/2} \\}}{1+ |a| + |b|/R + R|c| + |d| }. \\end{align*} We now rewrite the congruences coming from the group $\\Gamma$."}
{"input": "\\section{Proof of Theorem \\ref{thm:typeI} } \\label{sec:typeIproof} We may assume that for some small $\\eta > 0$ we have $K >X^{1-\\eta}$, since otherwise the claim is trivial by applying Poisson summation on $\\ell$. Similarly, we may assume that $K \\leq DX^{1+\\eta}$ since otherwise the claim is trivial by switcing to the complementary divisor $m= \\frac{a\\ell^2+h}{k}$ and applying Poisson summation on $\\ell$. Let $X_1=X$ and $X_2 = K^{1/(1-\\eta)} > X_1$. Noting that the claim follows for $X=X_2$ trivially by the Poisson summation formula, it suffices to bound \\begin{align*} \\sum_{d \\leq D}\\bigg| \\sum_{k \\equiv 0 \\pmod{d}} \\psi_1\\Bigl(\\frac{k}{K}\\Bigr) \\Bigl(\\sum_{a \\ell^2 +h \\equiv 0 \\pmod{k}} \\psi_2\\Bigl(\\frac{\\ell}{X}\\Bigr) - \\frac{X}{X_2}\\psi_2\\Bigl(\\frac{\\ell}{X_2}\\Bigr)\\Bigr) \\bigg|. \\end{align*} Normalizing by $\\sqrt{ah}$, we denote for real symmetric $\\gf=\\smqty(\\aa & \\bb \\\\ \\bb & \\cc )$ with determinant 1 \\begin{align*} F_{\\diamond,j}(\\gf) = \\psi_1\\Bigl(\\frac{\\cc \\sqrt{a h} }{ a K}\\Bigr) \\frac{X}{X_j} \\psi_2\\Bigl(\\frac{\\bb \\sqrt{a h}}{a X_j}\\Bigr), \\end{align*} and define $F_j:\\SL_2(\\R) \\to \\C$ by $F_j(\\g) = F_{\\diamond,j}(\\g \\g^t)$, where $\\aa= a_0^2+b_0^2, \\bb= a_0 c_0+ b_0d_0, \\cc=c_0^2+d_0^2$ for $\\g = \\smqty(a_0 & b_0 \\\\ c_0 & d_0)$. Then the Iwasawa coordinates are $y = \\frac{1}{\\cc}$ and $x= \\frac{\\bb}{\\cc}$ and $F_{j}$ is supported on \\begin{align*} \\mathbf{X}_j& = X^{o(1)}\\frac{X_j}{K} \\quad\\quad \\mathbf{Y} = X^{o(1)} \\frac{h^{1/2}}{a^{1/2}}\\frac{1}{K}, \\quad \\quad \\frac{\\mathbf{X}_j}{\\mathbf{Y}} = X^{o(1)} \\frac{X_j a^{1/2}}{ h^{1/2}} = X^{o(1)} \\frac{X_j }{ h^{1/2}} \\end{align*} Define the linear functional $\\alpha=\\alpha_{d,a,h}$ by \\begin{align} \\langle f \\rangle_\\alpha=\\sum_{\\sigma \\in \\mathrm{L}_h} \\frac{1}{|\\Gamma_{\\sigma i}|}\\sum_{\\substack{\\tau \\in T_q \\\\ }} \\alpha_{d,a}(\\tau \\sigma) f(\\tau \\sigma), \\quad \\alpha_{d,a}(\\g):=\\mathbf{1}_{\\substack{\\cc(\\g)\\equiv 0 \\pmod{ad}\\\\ \\bb(\\g)\\equiv 0 \\pmod{a}}}. \\label{eq:alphaTypeIdef} \\end{align} We also let $I$ denote the linear functional $\\langle f \\rangle_I = f(I)$."}
{"input": "By Lemma \\ref{lem:para} and by inserting the corresponding integrals over $\\G$ which match exactly, we have \\begin{align*} \\sum_{k \\equiv 0 \\pmod{d}} \\psi_1\\Bigl(\\frac{k}{K}\\Bigr) \\Bigl(\\sum_{a \\ell^2 +h \\equiv 0 \\pmod{k}} \\psi_2\\Bigl(\\frac{\\ell}{X}\\Bigr) - \\frac{X}{X_2}\\psi_2\\Bigl(\\frac{\\ell}{X_2}\\Bigr)\\Bigr)\\Bigr) = \\langle I | \\Delta_{da} F_1 | \\alpha_{d,a,h} \\rangle - \\langle I | \\Delta_{da} F_2 | \\alpha_{d,a,h} \\rangle. \\end{align*} We can now apply Theorem \\ref{thm:technical} to bound each of the terms. We only do the calculations for $j=1$, for the case $j=2$ we get better bounds thanks to the factor $X/X_2 < 1$. Recalling that $h \\leq X^{2+o(1)}$, we apply Theorem \\ref{thm:technical} with $\\delta^{-1} \\precprec 1$, $\\Gamma=\\Gamma_0(ad)$, $X h^{-1/2} = Z_0 Z_1 Z_2$, and use Cauchy-Schwarz on $d$ to get \\begin{align*} \\sum_{d\\leq D} \\Bigl| \\langle I | \\Delta_{da} F_1 | \\alpha_{d,a,h} \\rangle\\Bigr|\\precprec Z^{1/2} Z_0^\\theta \\sqrt{K_1 K_2}, \\end{align*} where by positivity we have \\begin{align*} K_1&=\\sum_{d\\leq D}\\langle I |\\Delta_{ad} k_{Z_1^2,\\mathbf{X}} |I \\rangle \\leq \\sum_{q\\leq aD} \\langle I |\\mathcal{K}_{q} k_{Z_1^2,\\mathbf{X}}| I \\rangle. \\\\ K_2&=\\sum_{d\\leq D} \\langle \\alpha_{d,a,h}| \\Delta_{ad} k_{Z_2^2,1} | \\alpha_{d,a,h} \\rangle \\leq \\sum_{q\\leq aD} \\langle \\alpha_{q}| \\mathcal{K}_{q} k_{Z_2^2,1} | \\alpha_{q} \\rangle. \\end{align*} By Proposition \\ref{prop:lowertriang} with $q=n_1$ and $D=N_0=N_2=T=V=1$, we get \\begin{align} \\label{eq:K1bound} K_1 \\precprec D(1+\\mathbf{X})+\\mathbf{X}^{-1}+Z_1. \\end{align} By Proposition \\ref{prop:heegner} we get \\begin{align}\\label{eq:K2bound} K_2 \\precprec D h^{1/2} + h Z_2. \\end{align} The result follows by choosing $Z_1 = D $ and $Z_2 = 1+D h^{-1/2}$ so that $Z_0 = 1+X^{o(1)} \\frac{X}{D(h^{1/2}+D)}$. Recall that at the beginning we reduced to the case $K \\leq DX^{1+\\eta}$."}
{"input": "X +\\frac{X N}{M} (N+h^{1/4}) \\bigg( 1+ \\frac{X^2}{MN^2(N^2 + h^{1/2})} \\bigg)^{\\theta}. \\end{align} We write $B= B_{=} + B_{\\neq}$ to separate the diagonal $\\ell_1=\\ell_2$ and the off-diagonal $\\ell_1 \\neq \\ell_2$. \\subsubsection{Diagonal contribution $\\ell_1=\\ell_2$} By absorbing $t,n_0,n_1,n_2,m$ with a divisor bound, we get \\begin{align*} B_{=}(M,N) \\precprec \\sum_{\\ell} \\Psi(\\ell)^2 \\precprec X. \\end{align*} \\subsubsection{Off-diagonal contribution} By expanding the coprimality condition for $m$, we have \\begin{align*} B_{\\neq}(M,N) = &\\sum_{tn_0 \\leq 2N}\\sum_{\\substack{n_1,n_2 \\sim N/tn_0 \\\\ \\gcd(n_1,n_2)=1 }} \\beta_{n_0 n_1 t } \\overline{\\beta_{n_0 n_2 t} }\\varrho_{a,h}(n_0) \\sum_{\\substack{d \\\\ d | n_0 n_1 n_2 } } \\mu(d) \\\\ & \\times\\sum_{m} \\psi(\\frac{mdt}{M }) \\sum_{\\substack{a \\ell_j^2 +h \\equiv 0 \\pmod{m d n_0n_jt^2} \\\\ \\ell_1 \\equiv \\ell_2 \\pmod{ m d n_0 t^2} \\\\ \\ell_1 \\neq \\ell_2 }} \\Psi(\\ell_1)\\Psi(\\ell_2). \\end{align*} Applying Lemma \\ref{lem:para2} we get for $s=d n_0t^2$ \\begin{align*} \\sum_{m} \\psi(\\frac{mdt}{M}) \\sum_{\\substack{a \\ell_j^2 +h \\equiv 0 \\pmod{s mn_j} \\\\ \\ell_1 \\equiv \\ell_2 \\pmod{ m s } }} \\Psi(\\ell_1)\\Psi(\\ell_2) = \\sum_{(u_1,u_2) \\in U} \\sum_{\\gf \\in \\mathrm{S}_{a,h}(sn_1n_2)}f_2(\\n[asn_1 u_1]^t \\diamond \\gf, \\n[asn_2 u_2]^t \\diamond \\gf ), \\end{align*} where \\begin{align*} f_2(\\gf_1,\\gf_2) &= \\Psi(\\bb_1/a) \\Psi(\\bb_2/a) \\psi(\\frac{\\aa_1 dt}{M}) = \\sum_{i,j \\in \\{1,2\\} } (-1)^{i+j} f_{ij}(\\gf_1,\\gf_2)\\\\ f_{ij}(\\gf_1,\\gf_2) &= \\psi_i(\\bb_1/a) \\psi_j(\\bb_2/a) \\psi(\\frac{\\aa_1 dt}{M}) \\end{align*} We now transform this sum for application of Theorem \\ref{thm:technical}. The first step is to use Fourier inversion to decouple the two variables $\\gf_1$ and $\\gf_2$. Before this we need to truncate the sum over $(u_1,u_2)$. We claim that it is supported on \\begin{align*} |n_2 u_2- n_1 u_1| \\leq V_{ij}:= 10 \\frac{\\max\\{X_i,X_j\\}}{Mn_0t}."}
{"input": "\\section{Proof of Theorems \\ref{thm:primefactor} and \\ref{thm:roots}} We now state two corollaries to Theorems \\ref{thm:typeI} and \\ref{thm:typeII} that give explicit Type I and Type II ranges for $h \\leq X^{1+\\eps}$. We get power saving in the full range as soon as there is a spectral gap, that is, if $\\theta < 1/2$. This makes our main theorems independent of progress towards the Selberg eigenvalue conjecture. \\begin{corollary}[Explicit Type I information] Let $\\eta,\\eps > 0$ be small, Let $ K \\leq X^{2}$ and $D \\leq X^{1/2-\\eta}$. Let $h$ be square-free with $h \\leq X^{1+\\eps}$ and let $a \\leq X^{o(1)}$ with $\\gcd(a,h)=1$. Let $\\psi_1,\\psi_2$ be smooth functions supported on $[1,2]$ and $[-1,1]$, resp., such that for all $J \\geq 0$ we have $\\psi_i^{(J)} \\precprec_J 1$. Then \\begin{align*} \\sum_{d \\leq D}\\bigg| \\sum_{k \\equiv 0 \\pmod{d}} \\psi_1\\Bigl(\\frac{k}{K}\\Bigr) \\Bigl(\\sum_{a \\ell^2 +h \\equiv 0 \\pmod{k}} \\psi_2\\Bigl(\\frac{\\ell}{X}\\Bigr) - \\frac{\\varrho_{a,h}(k) X \\widehat{\\psi_2}(0) }{k} \\Bigr) \\bigg| & \\precprec X^{1-(1-2\\theta)\\eta + \\eps/4} \\end{align*} \\end{corollary} \\begin{corollary}[Explicit Type II information] Let $\\eta,\\eps> 0$ be small and let $M \\geq N \\geq 1$ with $MN=X^\\alpha$ satisfy \\begin{align*} X^{\\alpha-1+2\\eta} \\leq N \\leq X^{(2-\\alpha)/3 - \\frac{4}{3}\\eta }. \\end{align*} Let $h$ be square-free with $1 \\leq h \\leq X^{1+\\eps}$ and let $a \\leq X^{o(1)}$ with $\\gcd(a,h)=1$. Let $\\psi$ be a smooth function supported $[-1,1]$, such that for all $J \\geq 0$ we have $\\psi^{(J)} \\precprec_J 1$. Suppose that $\\alpha_m,\\beta_n$ are divisor bounded coefficients with $\\beta_n$ is supported on square-free integers."}
{"input": "X \\widehat{\\psi}(0) }{mn} \\Bigr) \\precprec X^{1-(1-2\\theta) \\eta + \\eps/8}. \\end{align*} \\end{corollary} We obtain precisely the Type I and Type II ranges that were obtained in \\cite{mlargepf} under the assumption of Selberg's eigenvalue conjecture. By the calculations in \\cite[proof of Theorem 2]{mlargepf}, Theorem \\ref{thm:primefactor} follows. The hypothesis $|\\sum_{p \\leq Y} \\frac{\\log p}{p}(\\tfrac{h}{p}) |\\leq \\eps \\log Y$ ensures that in the application of the linear sieve and Harman's sieve in \\cite{mlargepf} the sieve dimension is still $1$. Similarly, by exactly the same sieve argument as in \\cite{DFIprimes}, these imply Theorem \\ref{thm:roots}. The sieve argument in \\cite{DFIprimes} works unchanged as there a two-dimensional sieve is used to bound the small contribution from the discarded ranges."}
{"input": "\\section{A divisor problem for $ax^2+by^3$} \\label{sec:x2y3} We recall the set-up in \\cite{mx2y3}. Let $a,b>0$ be coprime integers and let $\\delta = \\delta(X):= (\\log X)^{-c}$ for some fixed large $c >0$. Let $f,f_1,f_2$ denote non-negative non-zero smooth functions supported in $[1,1+\\delta]$ and satisfying the derivative bounds $ f^{(J)}, f_1^{(J)}, f_2^{(J)}\\ll_J \\delta^{-J}$ for all $J \\geq 0$. For $A \\in (\\delta X^{1/2}, X^{1/2}]$ and $B\\in (\\delta X^{1/3}, X^{1/3}]$ we define the sequences $\\mathcal{A}=(a_n)$, $\\mathcal{B}=(b_n)$, and their difference $\\mathcal{W}=(w_n)$ by \\begin{align*} a_n &:= a_n(a,b,f_1,f_2,A,B) = \\sum_{n=a x^2+b y^3} f_1(\\tfrac{x}{A}) f_2(\\tfrac{y}{B}), \\\\ b_n &:= b_n(a,b,f,f_1,f_2,A,B,W)= f(\\tfrac{n}{X}) \\frac{AB \\widehat{f_1}(0)\\widehat{f_2}(0) }{X\\widehat{f}(0) } \\\\ w_n &:= a_n-b_n \\end{align*} Then we have the following proposition, which essentially counts the values of the divisor function $d(ax^2+by^3)$ along multiples of a modulus $d$ of size up to $X^{1/4-o(1)}$. \\begin{proposition}[Type I$_2$ information up to $1/4$] \\label{prop:typeI2} Let $a,b > 0 $ be coprime integers and let $\\eta > 0$ be sufficiently small. For any $K \\leq X^{3/4}$ we have \\begin{align*} \\sum_{d \\leq X^{1/4-\\eta}} \\bigg| \\sum_{k \\equiv 0\\pmod{d} } \\mu^2(k) f(\\tfrac{k}{K}) \\sum_{n } w_{k n} \\bigg| \\ll X^{5/6-\\eta^3}. \\end{align*} \\end{proposition} We first reduce the proof to the following technical version, which restricts to a good set of $y$ at the cost of slightly increasing $b$. For the good set of $y$ we show a version with stronger power saving. \\begin{proposition}\\label{prop:typeI2tech} Denote $\\varrho_{a,by^3}(d) := \\# \\{ x \\in \\Z/d\\Z: ax^2+by^3 \\equiv 0 \\pmod{d} \\}$."}
{"input": "\\section{Installation} The package is available at author resources page at Elsevier (\\url{http://www.elsevier.com/locate/latex}). It can also be found in Comprehensive \\TeX{} Archive Network (\\textsc{ctan}), \\url{https://ctan.org/pkg/elsarticle}. Please download the \\file{elsarticle.dtx} which is a composite class with documentation and \\file{elsarticle.ins} which is the \\LaTeX{} installer file. When we compile the \\file{elsarticle.ins} with \\LaTeX{} it provides the class file, \\file{elsarticle.cls} by stripping off all the documentation from the \\verb+*.dtx+ file. The class may be moved or copied to a place, usually, \\verb+$TEXMF/tex/latex/+ %$%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\verb+elsevier/+, or a folder which will be read by \\LaTeX{} during document compilation. The \\TeX{} file database needs updation after moving/copying class file. Usually, we use commands like \\verb+mktexlsr+ or \\verb+texhash+ depending upon the distribution and operating system."}
{"input": "\\section{Usage} \\label{sec:usage} The class should be loaded with the command: \\begin{vquote} \\documentclass[<options>]{elsarticle} \\end{vquote} \\noindent where the \\verb+options+ can be the following: \\begin{description} \\item [{\\tt\\color{verbcolor} preprint}] default option which format the document for submission to Elsevier journals. \\item [{\\tt\\color{verbcolor} review}] similar to the \\verb+preprint+ option, but increases the baselineskip to facilitate easier review process. \\item [{\\tt\\color{verbcolor} 1p}] formats the article to the look and feel of the final format of model 1+ journals. This is always single column style. \\item [{\\tt\\color{verbcolor} 3p}] formats the article to the look and feel of the final format of model 3+ journals. If the journal is a two column model, use \\verb+twocolumn+ option in combination. \\item [{\\tt\\color{verbcolor} 5p}] formats for model 5+ journals. This is always of two column style. \\item [{\\tt\\color{verbcolor} authoryear}] author-year citation style of \\file{natbib.sty}. If you want to add extra options of \\file{natbib.sty}, you may use the options as comma delimited strings as arguments to \\verb+\\biboptions+ command. An example would be: \\end{description} \\begin{vquote} \\biboptions{longnamesfirst,angle,semicolon} \\end{vquote} \\begin{description} \\item [{\\tt\\color{verbcolor} number}] numbered citation style. Extra options can be loaded with\\linebreak \\verb+\\biboptions+ command. \\item [{\\tt\\color{verbcolor} sort\\&compress}] sorts and compresses the numbered citations. For example, citation [1,2,3] will become [1--3]. \\item [{\\tt\\color{verbcolor} longtitle}] if front matter is unusually long, use this option to split the title page across pages with the correct placement of title and author footnotes in the first page. \\item [{\\tt\\color{verbcolor} times}] loads \\file{txfonts.sty}, if available in the system to use Times and compatible math fonts."}
{"input": "\\section{Frontmatter} There are two types of frontmatter coding: \\begin{enumerate}[(1)] \\item each author is connected to an affiliation with a footnote marker; hence all authors are grouped together and affiliations follow; %\\pagebreak \\item authors of same affiliations are grouped together and the relevant affiliation follows this group. \\end{enumerate} An example of coding the first type is provided below. \\begin{vquote} \\title{This is a specimen title\\tnoteref{t1,t2}} \\tnotetext[t1]{This document is the results of the research project funded by the National Science Foundation.} \\tnotetext[t2]{The second title footnote which is a longer text matter to fill through the whole text width and overflow into another line in the footnotes area of the first page.} \\end{vquote} \\begin{vquote} \\author[1]{Jos Migchielsen\\corref{cor1}% \\fnref{fn1}} \\ead{J.Migchielsen@elsevier.com} \\author[2]{CV Radhakrishnan\\fnref{fn2}} \\ead{cvr@sayahna.org} \\author[3]{CV Rajagopal\\fnref{fn1,fn3}} \\ead[url]{www.stmdocs.in} \\cortext[cor1]{Corresponding author} \\fntext[fn1]{This is the first author footnote.} \\fntext[fn2]{Another author footnote, this is a very long footnote and it should be a really long footnote. But this footnote is not yet sufficiently long enough to make two lines of footnote text.} \\fntext[fn3]{Yet another author footnote.} \\end{vquote} \\begin{vquote} \\affiliation[1]{organization={Elsevier B.V.}, addressline={Radarweg 29}, postcode={1043 NX}, city={Amsterdam}, country={The Netherlands}} \\affiliation[2]{organization={Sayahna Foundation}, addressline={JWRA 34, Jagathy}, city={Trivandrum}, postcode={695014}, country={India}} \\affiliation[3]{organization={STM Document Engineering Pvt Ltd.}, addressline={Mepukada, Malayinkil}, city={Trivandrum} postcode={695571}, country={India}} \\end{vquote} The output of the above \\TeX{} source is given in Clips~\\ref{clip1} and \\ref{clip2}. The header portion or title area is given in Clip~\\ref{clip1} and the footer area is given in Clip~\\ref{clip2}. \\vspace*{1pc} \\def\\rulecolor{blue!70} \\src{Header of the title page.} \\includeclip{1}{130 612 477 707}{1psingleauthorgroup.pdf}%%{elstest-1p.pdf}%single author group \\def\\rulecolor{orange} \\pagebreak \\def\\rulecolor{blue!70} \\src{Footer of the title page."}
{"input": "255}{1pseperateaug.pdf}%%{elstest-1p.pdf}%single author group \\def\\rulecolor{orange} Most of the commands such as \\verb+\\title+, \\verb+\\author+, \\verb+\\affiliation+ are self explanatory. Various components are linked to each other by a label--reference mechanism; for instance, title footnote is linked to the title with a footnote mark generated by referring to the \\verb+\\label+ string of the \\verb=\\tnotetext=. We have used similar commands such as \\verb=\\tnoteref= (to link title note to title); \\verb=\\corref= (to link corresponding author text to corresponding author); \\verb=\\fnref= (to link footnote text to the relevant author names). \\TeX{} needs two compilations to resolve the footnote marks in the preamble part. Given below are the syntax of various note marks and note texts. \\begin{vquote} \\tnoteref{<label(s)>} \\corref{<label(s)>} \\fnref{<label(s)>} \\tnotetext[<label>]{<title note text>} \\cortext[<label>]{<corresponding author note text>} \\fntext[<label>]{<author footnote text>} \\end{vquote} \\noindent where \\verb=<label(s)>= can be either one or more comma delimited label strings. The optional arguments to the \\verb=\\author= command holds the ref label(s) of the address(es) to which the author is affiliated while each \\verb=\\affiliation= command can have an optional argument of a label. In the same manner, \\verb=\\tnotetext=, \\verb=\\fntext=, \\verb=\\cortext= will have optional arguments as their respective labels and note text as their mandatory argument. The following example code provides the markup of the second type of author-affiliation. \\begin{vquote} \\author{Jos Migchielsen\\corref{cor1}% \\fnref{fn1}} \\ead{J.Migchielsen@elsevier.com} \\affiliation[1]{organization={Elsevier B.V.}, addressline={Radarweg 29}, postcode={1043 NX}, city={Amsterdam}, country={The Netherlands}} \\author{CV Radhakrishnan\\fnref{fn2}} \\ead{cvr@sayahna.org} \\affiliation[2]{organization={Sayahna Foundation}, addressline={JWRA 34, Jagathy}, city={Trivandrum} postcode={695014}, country={India}} \\author{CV Rajagopal\\fnref{fn1,fn3}} \\ead[url]{www.stmdocs.in} \\affiliation[3]{organization={STM Document Engineering Pvt Ltd.}, addressline={Mepukada, Malayinkil}, city={Trivandrum} postcode={695571}, country={India}} \\end{vquote} \\vspace*{-."}
{"input": "goes to waste in such cases. Therefore, authors are requested to check this problem by typesetting their submissions in final format as well just to see if their equations are broken at appropriate places, by changing appropriate options in the document class loading command, which is explained in section~\\ref{sec:usage}, \\nameref{sec:usage}. This allows authors to fix any equation breaking problem before submission for publication. \\file{elsarticle.cls} supports formatting the author submission in different types of final format. This is further discussed in section \\ref{sec:final}, \\nameref{sec:final}. \\enlargethispage*{\\baselineskip} \\subsection*{Displayed equations and double column journals} Many Elsevier journals print their text in two columns. Since the preprint layout uses a larger line width than such columns, the formulae are too wide for the line width in print. Here is an example of an equation (see equation 6) which is perfect in a single column preprint format: In normal course, articles are prepared and submitted in single column format even if the final printed article will come in a double column format journal. Here the problem is that when the article is typeset by the typesetters for paginating and fit within the single column width, they have to break the lengthy equations and align them properly. Even if most of the tasks in preparing your proof is automated, the equation breaking and aligning requires manual judgement, hence this task is manual. When there comes a manual operation that area is error prone. Author needs to check that equation pretty well."}
{"input": "equation to the single column width typesetters need not want to touch these area and the proof authors get will be without any errors. %\\bigskip \\setlength\\Sep{6pt} \\src{See equation (6)} \\def\\rulecolor{blue!70} %\\includeclip{<page>}{l b scale }{file.pdf} \\includeclip{4}{105 500 500 700}{1psingleauthorgroup.pdf} \\def\\rulecolor{orange} \\noindent When this document is typeset for publication in a model 3+ journal with double columns, the equation will overlap the second column text matter if the equation is not broken at the appropriate location. \\vspace*{6pt} \\def\\rulecolor{blue!70} \\src{See equation (6) overprints into second column} \\includeclip{3}{59 421 532 635}{elstest-3pd.pdf} \\def\\rulecolor{orange} \\vspace*{6pt} \\noindent The typesetter will try to break the equation which need not necessarily be to the liking of the author or as it happens, typesetter's break point may be semantically incorrect. Therefore, authors may check their submissions for the incidence of such long equations and break the equations at the correct places so that the final typeset copy will be as they wish."}
{"input": "\\section{Related Work} Our research is situated within a growing body of research on interpretable models by design and sparsity. \\subsection{Codebook Features} A closely related work to ours is that of \\citet{codebook}, who introduce ``Codebook Features'' to enhance interpretability. Their approach adds a bottleneck layer, termed a codebook, after each component in a transformer. These codebooks replace the original component's activations before being added to the residual stream. Each codebook is composed of a large set of learned code vectors. During a forward pass, only the top-k code vectors most similar to the original activation are selected, inducing a high degree of sparsity, as the codebook's dimensionality is significantly larger than the original components. The authors demonstrate that the learned code vectors often exhibit human-interpretable functions, and that intervening on these codes can predictably alter model behaviour. However, their method substantially modifies the standard transformer architecture, leading to reduced inference speed and limiting compatibility with existing frameworks. Our work builds upon this concept of learned sparse representations but introduces a method that preserves the original transformer structure, enhancing interoperability and maintaining inference efficiency. Furthermore, while \\citet{codebook} primarily rely on qualitative assessments of interpretability, we provide quantitative, measurable indicators to rigorously evaluate the impact of our self-ablation mechanism on model interpretability. \\subsection{Gradient Routing} Similar to codebook features, \\citet{gradientrouting} introduce ``Gradient Routing\" for mechanistic supervision of neural networks. This technique modifies backpropagation using data-dependent, weighted gradient masks, allowing specific data points to update designated parts of the network."}
{"input": "method can create partitioned representations, enable robust unlearning through targeted ablation, and achieve scalable oversight by localizing modules responsible for different behaviours. Unlike codebook features, gradient routing maintains the standard transformer architecture and inference efficiency. Our work draws inspiration from their approach to integrating specialization into the model. However, we apply kWTA directly to activations within each transformer layer during the forward pass, enforcing sparsity and inducing modularity within the core computational pathway of the model, a crucial distinction for achieving the degree of interpretability and control that we demonstrate. \\subsection{K - Winner Takes All (kWTA)} Winner-takes-all (WTA) mechanisms, and their generalization to k-winner-takes-all (kWTA), have a long history of study in the context of neural networks \\citep{WTA-Original, kWTA-Original}. These mechanisms, inspired by competitive interactions observed in biological neurons, enforce a significant form of sparsity where only a limited number of neurons are allowed to be active at any given time. Much of the prior research on kWTA in artificial neural networks has focused on its ability to improve efficiency. These efforts primarily focus on performance gains such as reduced memory footprints or increased throughput \\citep{WTA-Original, kWTA-Original, doubleSparsity}. In contrast, our work approaches kWTA primarily from an interpretability standpoint, leveraging its ability to localize information and induce modularity within the network. Additionally, instead of implementing kWTA by modifying the activation function, we introduce additional gating weights which allow us to extend kWTA naturally to attention units and also enable the model to selectively control its activations."}
{"input": "\\ref{eq:weights} to compute gradients, employing the straight-through estimator technique. This allows us to train the model end-to-end despite the non-differentiable nature of the top-k operation. To train the self-ablation mechanism itself, we utilize an ablated loss term in addition to the standard cross-entropy loss on the model's output. The total loss is the sum of the cross-entropy loss on the clean (unablated) model output and the cross-entropy loss computed on the ablated model's output, using the threshold and temperature calculated in Equations \\ref{eq:threshold} and \\ref{eq:temperature}, respectively. This combined loss signal is then used to update all trainable parameters in the model via backpropagation (using the straight-through estimator for the ablation gates), including both the original transformer weights and the newly introduced gating weights. Thus, the base language model learns in conjunction with the self-ablation constraints from the beginning of training. To facilitate the computation of the clean loss, our model incorporates a dual residual stream where we store the states of both the clean (no ablations applied) and ablated residual streams. This structure allows us to obtain both the ablated and unablated outputs in a single pass through the model, which are crucial for calculating the activations of the MLP and attention units for the ablated stream. \\subsection{Model Architecture} \\begin{figure*}[t] \\centering \\begin{subfigure}[t]{0.55\\textwidth} \\centering \\includegraphics[width=\\linewidth]{figures/Global-Revised-cropped.png} \\caption{Global Ablation} \\label{fig:global-revised} \\end{subfigure} \\hfill \\begin{subfigure}[t]{0.43\\textwidth} \\centering \\includegraphics[width=\\linewidth,height=5.9cm,keepaspectratio]{figures/Lbl-Revised.png} \\caption{Local Ablation} \\label{fig:lbl-revised} \\end{subfigure} \\caption{Comparison of global and local ablation mechanisms. Both models use a transformer with 8 blocks and are structurally identical during inference."}
{"input": "reduce the computations required for training models with the ablation mechanism; an issue that becomes of utmost importance in the adoption of this technique for larger models. Conversely, the global approach (Figure \\ref{fig:global-revised}) utilizes an initial forward pass through the entire network to gather information before making ablation decisions for all layers simultaneously to be used for a subsequent pass. This global strategy is motivated by the inherent feature hierarchy in neural networks \\citep{deeplearning}, where complex representations emerge in later layers, potentially leading to more informed ablation based on a holistic view of the input. \\subsection{Training Data} Our models are trained on the TinyStories dataset, a synthetic corpus of 2 million short stories \\citep{tinystories}. Each entry within the dataset was generated using a restricted vocabulary, mirroring the limited lexicon understood by 3- to 4-year-olds. This vocabulary constraint, coupled with simple grammatical structures, results in stories that are both semantically and syntactically uncomplicated. Although the simplicity of the dataset allows for rapid prototyping and training of language models, it also imposes limitations on the complexity of learned behaviours. We do not anticipate the emergence of highly intricate neural circuits within models trained on TinyStories. However, the dataset's inherent structure, which includes fundamental language concepts like subject-verb-object relationships, provides a suitable environment for the development of simpler circuits, such as Indirect Object Identification (IOI) observed in previous work \\citep{ioi-circuit, acdc}."}
{"input": "transformer block after the MLP layer. We trained these SAEs with an expansion factor of 16 until their total loss converged. We use the L0 norm as our primary sparsity metric, which directly measures the average number of non-zero feature activations in the SAE's bottleneck. A lower L0 score indicates each activation is being represented by a more selective set of features. To evaluate the SAE's ability for reconstruction, we adopt the cross-entropy loss score. This metric quantifies the change in model performance when the original activations are replaced with their SAE reconstructions during a forward pass. The cross-entropy loss score ranges from 0 to 1, with higher values indicating that the SAE's reconstruction preserves more of the original model's behaviour. These widely used metrics \\citep{saebench}, allow us to examine whether self-ablation leads to more disentangled and interpretable internal representations. We report the exact hyper-parameters used in our SAE experiments in Appendix \\ref{apx:sae} \\subsection{Neuron Explainability} To assess neuron interpretability, we employ an automated approach by \\citet{automated-interpretability}, which utilizes an examiner LLM (in our case, GPT-4o mini \\citep{4o-mini}) to generate and score natural language explanations of neuron behaviour. While this automated process is still computationally expensive, it allows us to analyze neuron behaviour at a larger scale than manual methods. The process involves generating a natural language explanation of a neuron's behaviour using the examiner LLM, then using this explanation to simulate a hypothetical neuron's behaviour on new inputs."}
{"input": "``explanation score'', quantifying the explanation's accuracy. Due to monetary and time constraints, we focused on analyzing the neurons of one of our self-ablated models: a global ablation model with $k$=2. We obtain a distribution of explanation scores by applying this process to every neuron within the model. A higher average explanation score across all neurons suggests better overall neuron interpretability, as it indicates more consistent and explainable behaviour. \\subsection{Neuron to Graph} We use the Neuron to Graph (N2G) framework developed by \\citet{foote2023neuron} to analyze how self-ablation affects neuron behaviour in our ablated language models. Our methodology examines neuron behaviour through comparative analysis techniques. % I'm keeping this vague now because I do not know which model was used % We obtain neuron graphs for both models following the authors' framework, adapting them to accommodate our model's architecture and work without input from the Neuroscope resource \\cite{nanda2022neuroscope}. For our aggregated analysis of the base and ablated models' graphs, we examine several metrics that help us understand neuron behaviour: graph density, out-degree, and graph transitivity. Graph density is calculated as the ratio of actual edges in the graph to the maximum possible number of edges; lower density indicates more selective neuron behaviour, as the neuron forms fewer connections between tokens. Out-degree is the average number of outgoing connections per node; higher values in later layers combined with low density can suggest better feature abstraction, maintaining meaningful connections while eliminating spurious ones."}
{"input": "\\section{Results} \\begin{figure*}[tb] \\centering \\begin{subfigure}[b]{0.48\\textwidth} \\centering \\includegraphics[width=\\textwidth]{figures/ioi-circuit-comparison.png} \\label{fig:ioi-circuit} \\end{subfigure} \\hfill \\begin{subfigure}[b]{0.48\\textwidth} \\centering \\includegraphics[width=\\textwidth]{figures/cosmic-leaf-81-neurons.png} \\label{fig:neuron-explainability} \\end{subfigure} \\caption{Self-ablation improves interpretability as shown through (left) IOI circuit simplification (baseline: 79 edges, self-ablated: 30 edges) where fewer model components are required to perform a task and (right) a shift towards higher neuron explanation scores in the self-ablated model (red) compared to the baseline (blue). These results indicate that self-ablation leads to more focused and interpretable circuits and neurons.} \\label{fig:combined-results} \\end{figure*} Our experiments demonstrate that self-ablation significantly enhances model interpretability across multiple evaluation methods, while incurring only a modest cost in language modelling performance. Synthesizing these findings reveals a consistent picture: self-ablation produces more localized functional circuits (evidenced by substantially fewer edges identified by ACDC \\citep{acdc}), more concise and disentangled feature representations (indicated by lower L0 norms and strong reconstruction scores using SAEs \\citep{saefeatures, saebench}), more readily explainable individual neuron behaviors (supported by higher automated explanation scores \\citep{automated-interpretability}), and more focused neuron activation patterns (reflected in sparser connectivity graphs and increased specialization metrics from N2G analysis \\citep{foote2023neuron}). These improvements are observed compared to the baseline transformer, with quantitative details provided in Table \\ref{tab:ablation_results} and Table \\ref{tab:graph_analysis}. Importantly, this enhanced interpretability comes at a manageable cost to the model's core capabilities: validation perplexity increases by no more than 20\\% across all self-ablation configurations compared to the baseline (Table \\ref{tab:ablation_results}). The subsequent sections will elaborate on the specific findings from our circuit, feature, and neuron-level analyses."}
{"input": "ACDC and SAE evaluations. The ACDC analysis shows that self-ablation dramatically reduces circuit complexity as observed in Figure \\ref{fig:combined-results}. The most notable example is our local ablation model with $k$=4, which achieves an IOI circuit edge count of just 30, compared to 79 in the baseline - a 62\\% reduction. This substantial decrease in edge count indicates that self-ablation produces more localized and focused circuits. Such circuit simplification is particularly valuable for interpretability, as it makes it easier to trace and understand the specific computations underlying model behaviour. \\subsection{Feature Representation Analysis} The SAE results provide further evidence for the interpretability gains achieved through self-ablation. Two key metrics highlight this improvement: the L0 norm and the Cross-Entropy (CE) score. The L0 norm, which measures the average number of active features in the SAE's bottleneck layer, is lower in our ablated models. For example, the $k=4$ local ablation model achieves an L0 norm of 4.01, compared to the baseline's 7.22. This 44\\% reduction in active features indicates that self-ablation leads to sparser, more focused representations. Each feature captures a more specific aspect of the model's behaviour, suggesting a greater degree of disentanglement. Furthermore, our models consistently achieve higher CE scores, which quantify how well the SAE reconstructions preserve the original model's behaviour. The $k=8$ local model attains a CE score of 0.72, surpassing the baseline's score of 0.63. This improvement demonstrates that despite using fewer active features, the ablated model's representations remain faithful to the original computation."}
{"input": "neuron interpretability analysis, focusing on a self-ablated model trained with global ablation and $k=2$, reveals a shift towards higher explanation scores compared to the baseline model (see Figure \\ref{fig:combined-results}). Quantitatively, the self-ablated model achieves a mean explanation score of 0.46, while the baseline model has a mean score of 0.41. This difference suggests that the neurons in the self-ablated model are, on average, more amenable to meaningful interpretation, supporting the notion that our self-ablation mechanism enhances neuron interpretability. \\subsection{Neuron Graph Analysis} Our NeuronGraph analysis reveals significantly more focused neuron behaviour in the ablated model compared to the baseline (Table \\ref{tab:graph_analysis}). The baseline model exhibits consistently higher graph density across all layers (0.235-0.446), while the ablated model shows significantly lower densities (0.046-0.191), indicating sparser, more focused connectivity patterns. This is further supported by the increase in activation node count, which measures connections leading to strongly activating tokens. In the baseline model, this metric remains high in later layers (1.30-1.57), while the ablated model shows a decreasing trend from 0.45 in Layer 0 to 0.07 in Layer 7, suggesting increasing neuron selectivity. Additionally, neuron specialization analysis based on the entropy of token substitutions demonstrates increased specialization in higher layers of the ablated model. The baseline maintains relatively high entropy in later layers (2.45-2.65), but the ablated model exhibits significantly lower values (1.14-2.16). This is corroborated by the token diversity analysis, where neurons in higher layers of the ablated model respond to fewer token types (3-5) compared to the baseline (7-8)."}
{"input": "collectively demonstrate that neurons in the ablated model develop more focused, specialized roles, responding to fewer types of tokens more consistently. The relationship between transitivity, out-degree, and graph density reveals sophisticated changes in how neurons process information in the ablated model. The ablated model's lower transitivity, particularly in layers 2-7 where it approaches zero, indicates fewer interconnected activation patterns and more selective behaviour. Notably, the combination of higher out-degree in later layers (0.98-1.01) with significantly reduced graph density and edges to activation suggests that the ablated model maintains diverse, meaningful connections while eliminating spurious ones. This contrasts with the baseline model's stable but less selective connectivity, characterized by consistent out-degree (0.86-0.91) alongside higher density and more activating edges. This combination of metrics reveals that the ablated model achieves a crucial balance: it maintains meaningful connections (high out-degree) while eliminating spurious ones (low density and transitivity). This suggests that the model learns to detect specific, abstract features while filtering out noise - a key characteristic of interpretable representations. \\subsection{Language Modelling Performance} While self-ablation does lead to a slight increase in validation perplexity, indicating a trade-off between interpretability and raw language modelling performance, this increase is relatively modest. The largest increase is seen with global ablation with $k=8$, where the perplexity rises from 5.73 (baseline) to 6.97. The local ablation models, however, maintain perplexity levels that are closer to the baseline, with the $k=4$ local model achieving a perplexity of 6.58."}
{"input": "& \\textbf{0.45} & \\textbf{0.23} & \\textbf{0.17} & \\textbf{0.10} & \\textbf{0.08} & \\textbf{0.06} & \\textbf{0.04} & \\textbf{0.07} \\\\ \\midrule \\multirow{2}{*}{O.Deg} & Base & 0.88 & 0.91 & 0.86 & 0.86 & 0.86 & 0.86 & 0.86 & 0.89 \\\\ & Ablated & 0.78 & 0.74 & 0.92 & 0.98 & 0.98 & 0.98 & 1.01 & 1.00 \\\\ \\midrule \\multirow{2}{*}{N.Ent $\\downarrow$} & Base & 2.08 & 2.45 & 2.57 & 2.10 & 2.59 & 2.53 & 2.65 & 2.45 \\\\ & Ablated & 2.38 & 2.48 & 2.48 & 2.44 & \\textbf{1.14} & \\textbf{2.40} & \\textbf{2.16} & \\textbf{2.10} \\\\ \\midrule \\multirow{2}{*}{T."}
{"input": "\\section{Self Ablation Time Complexity} \\label{apx:time-complexity} Self-ablation dynamically controls the activation of components during the forward pass, effectively inducing a form of structured sparsity. While the mechanism introduces a sorting step, its time complexity remains comparable to that of a standard Multi-Layer Perceptron (MLP). The sorting operation has a complexity of O(N log N), where N is the number of units (neurons or attention heads) being ablated. Since N is a constant determined by the model's architecture and not the input size, it does not significantly impact the overall time complexity for sufficiently large inputs. The primary computational cost remains dominated by the matrix multiplications inherent in the MLP and attention mechanisms, similar to the standard transformer architecture."}
{"input": "\\section{Automatic Circuit Discovery} \\label{apx:acdc} This section details our application of the Automatic Circuit Discovery (ACDC) algorithm, introduced by \\citet{acdc}, to identify the Indirect Object Identification (IOI) circuit within our self-ablated models. We utilized the Auto Circuit library by \\citet{autocircuit}, to efficiently implement ACDC. To generate the IOI dataset, we adapted the generator script from the Auto Circuit repository. This script employs a set of predefined templates (e.g., `BABA\\_TEMPLATES', `ABBA\\_TEMPLATES') and a vocabulary of names, places, and objects, which we modified to align with the TinyStories dataset used to train our models. Prompts follow structures like ``Then, [B] and [A] went to the [PLACE]. [B] gave a [OBJECT] to [A]'', where `[A]' and `[B]' represent names, `[PLACE]' a location, and `[OBJECT]' an object. The script also generates corrupted versions of these prompts by swapping or replacing names. We used the Kullback-Leibler (KL) divergence as the primary metric to evaluate the impact of activation patching during the ACDC procedure. This metric quantifies the difference between the model's output distribution on a clean prompt and its distribution when activations are patched from a corrupted prompt. A threshold parameter $\\tau$ of 0.03 was used to control the sparsity of the discovered circuit, with lower values leading to circuits with more edges."}
{"input": "a linear system is solved using the least-squares method. The solution to the linear system yields the coefficients of the MLP output level, thereby establishing the solution to the equation. Nevertheless, due to the adoption of the weak formulation of the governing equations, the resulting linear system in WRFM differs significantly from that of RFM. Given that a weak formulation necessitates the satisfaction of an equation for an infinite set of test functions, this could potentially result in a challenge during the implementation of the method. However, drawing inspiration from the approximation theorem \\cite{weierstrass1885}, we circumvent this issue by employing a finite set of predefined functions to approximate the test functions. %6)Description of following parts The subsequent sections of this paper are structured as follows. First, we remind the main ideas of the standard RFM in Section \\ref{sec:method}. Next, in Section \\ref{sec:new} we introduce Weak RFM and describe in detail its features and differences from the original method. In Section \\ref{sec:num_results} we provide several numerical experiments in 2D and 3D cases, compare Weak RFM with existing methods and evaluate how hyperparameters can affect accuracy. Finally, Section \\ref{sec:conclusion} inevitably summarizes the results of this work."}
{"input": "form of equation is under consideration, loss function have the following form: \\begin{equation*} \\begin{aligned} \\text { Loss }=\\sum_{\\boldsymbol{x}_i \\in C_I} \\sum_{k=1}^{E_I} \\left\\| \\mathcal{L}^k u(\\boldsymbol{x}_i)-f^k(\\boldsymbol{x}_i) \\right\\|_{l^2}^2 \\\\ +\\sum_{\\boldsymbol{x}_j \\in C_B} \\sum_{\\ell=1}^{E_B} \\left\\|\\mathcal{B} u(\\boldsymbol{x_j})-g^l(\\boldsymbol{x_j}) \\right\\|_{l^2}^2\\,. \\end{aligned} \\end{equation*} And total number of conditions imposed on $u_M$ is $N=E_I \\cdot C_I + E_B \\cdot C_B$. Due to the solution design, standard algorithms for least-squares approximation can be employed to solve this optimization problem. One of the most effective way to find parameters $\\left\\{u_{nj}\\right\\}_{j=1}^{J_n}$ for all $S$ blocks is to compose a linear system, which incorporates governing equations as well as boundary conditions simultaneously. In case $d=1$, $E_I=1$, $E_B=1$ (for simplicity), the system has the form \\eqref{eq:2}: \\begin{equation}\\label{eq:2} A \\times \\boldsymbol{u}=\\boldsymbol{v}\\,, \\end{equation} where \\begin{equation*} \\begin{aligned} A=\\begin{pmatrix} A_{1}\\\\ A_{2}\\\\ A_{3}\\\\ A_{4} \\end{pmatrix},&\\quad \\boldsymbol{u}=\\begin{pmatrix} u_{11}\\\\ u_{21}\\\\ \\vdots \\\\ u_{J_{S} S} \\end{pmatrix},\\quad \\boldsymbol{v}=\\begin{pmatrix} \\boldsymbol{v_{1}} \\\\ \\boldsymbol{v_{2}} \\\\ \\boldsymbol{v_{3}} \\\\ \\boldsymbol{v_{4}} \\end{pmatrix}\\,. \\end{aligned} \\end{equation*} Let us denote amount of points in $n$-th domain as $P_n$ and total amount of points as $P=\\sum_{n=1}^{S} P_n$."}
{"input": "\\end{equation*} And exactly as in case of strong formulation, this optimization problem can be reformulated as a linear system and solved by large spectrum of algorithms. In case $d=1$, $E_I=1$, $E_B=1$ and $\\mathcal{B} u=u$ the system has the form \\eqref{eq:4}. \\begin{equation}\\label{eq:4} \\mathcal{A} \\times \\boldsymbol{u}=\\boldsymbol{w}\\,, \\end{equation} where \\begin{equation*} \\begin{aligned} \\mathcal{A}=\\begin{pmatrix} \\mathcal{A}_{1}\\\\ \\mathcal{A}_{2}\\\\ \\mathcal{A}_{3} \\end{pmatrix},&\\quad \\boldsymbol{u}=\\begin{pmatrix} u_{11}\\\\ u_{21}\\\\ \\vdots \\\\ u_{J_{S} S} \\end{pmatrix},\\quad \\boldsymbol{w}=\\begin{pmatrix} \\boldsymbol{w_{1}} \\\\ \\boldsymbol{w_{2}} \\\\ \\boldsymbol{w_{3}} \\end{pmatrix}\\,. \\end{aligned} \\end{equation*} We denote amount of test functions $\\varphi^n$ corresponding to the $n$-th block as $K_n$ and total amount of test functions as $K=\\sum_{n=1}^S K_n$. At the same time, there is no need in test functions for boundary conditions $\\tilde \\varphi$ since $\\mathcal{B}$ does not contain any derivatives in the considered case. Therefore, structure of $\\mathcal{A}$ and $\\boldsymbol{w}$ takes the form as follows: \\begin{equation*} \\begin{aligned} &\\mathcal{A}_1=\\begin{pmatrix} (\\int_{\\Omega_1} \\phi_{1 j}(x)\\cdot\\mathcal{L}\\varphi_{i}^{1}(x)dx)_{i=1,\\;j=1}^{K_1,\\;J_1} & \\ldots & 0\\\\ \\vdots & \\ddots & \\vdots\\\\ 0 & \\ldots & (\\int_{\\Omega_S} \\phi_{S j}(x)\\cdot\\mathcal{L}\\varphi_{i}^{S}(x)dx)_{i=1,\\;j=1}^{K_S,\\;J_S}\\\\ \\end{pmatrix},\\\\ \\\\ &\\mathcal{A}_2=\\begin{pmatrix} (\\phi_{1 j}(x_0))_{j=1}^{J_1} & \\ldots& 0\\\\ 0 & \\ldots & (\\phi_{S j}(x_1))_{j=1}^{J_S} \\end{pmatrix},\\\\ \\\\ \\end{aligned} \\end{equation*} \\begin{equation*} \\begin{aligned} &\\mathcal{A}_3=\\begin{pmatrix} -(\\phi_{1 j}(x_{P_1}^{1}))_{j=1}^{J_1} & (\\phi_{2 j}(x_{1}^{2}))_{j=1}^{J_2} & 0 & \\ldots & 0\\\\ 0 & -(\\phi_{2 j}(x_{P_2}^{2}))_{j=1}^{J_2} & (\\phi_{3 j}(x_{1}^{3}))_{j=1}^{J_3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\ddots & \\ddots & \\vdots\\\\ 0 & 0 & 0 & -(\\phi_{S-1\\ j}(x_{P_{S-1}}^{S-1}))_{j=1}^{J_{S-1}} & (\\phi_{S j}(x_{1}^{S}))_{j=1}^{J_S}\\\\ \\end{pmatrix},\\\\ \\\\ \\end{aligned} \\end{equation*} \\begin{equation*} \\begin{aligned} &\\boldsymbol{w_1}=\\begin{pmatrix} \\int_{\\Omega_1} f(x)\\cdot\\varphi_{1}^{1}(x)dx\\\\ \\ldots\\\\ \\int_{\\Omega_1} f(x)\\cdot\\varphi_{K_1}^{1}(x)dx\\\\ \\ldots\\\\ \\ldots\\\\ \\int_{\\Omega_S} f(x)\\cdot\\varphi_{K_S}^{S}(x)dx\\\\ \\end{pmatrix},\\quad\\quad\\quad\\quad \\boldsymbol{w_2}=\\begin{pmatrix} g(x_0)\\\\ g(x_1)\\\\ \\end{pmatrix},\\quad\\quad\\quad\\quad \\boldsymbol{w_3}=\\begin{pmatrix} 0\\\\ \\ldots\\\\ 0\\\\ \\end{pmatrix}."}
{"input": "is defined exactly as in expression \\eqref{repr:1}: $$\\varphi_k^n=w_{[x_0^n,x_1^n]}\\cdot\\sin\\left(\\frac{\\pi k (x-x_0^n)}{x_1^n-x_0^n}\\right)\\,,$$ where $x_0^n$ and $x_1^n$ are boundaries of $\\Omega_n$. In comparison with the standard RFM, dimensionality of matrices is different: \\begin{itemize} \\item{$\\mathcal{A}_1\\in \\mathbb{R}^{K\\times M}$ and $\\boldsymbol{w_1}\\in \\mathbb{R}^{K}$ correspondingly impose the condition for satisfying the weak form of equation} \\item{$\\mathcal{A}_2\\in \\mathbb{R}^{2\\times M}$ and $\\boldsymbol{w_2}\\in \\mathbb{R}^{2}$ impose the boundary condition on the function} \\item{$\\mathcal{A}_3\\in \\mathbb{R}^{(S-1)\\times M}$ and $\\boldsymbol{w_3}\\in \\mathbb{R}^{S-1}$ put continuity conditions at boundaries of blocks, it is also noteworthy that there are no differentiability conditions} \\end{itemize} This algorithm was implemented for up to 3D cases for both simple and complex geometry. Results are presented in the next section."}
{"input": "\\section{Numerical Results} \\label{sec:num_results} This section evaluates the performance of Weak RFM by comparing it to two popular methods, namely PINN and WAN. PINN inherently has the ability to find weak solutions, as reformulating the equation into an optimization problem enables it to identify solutions that satisfy the equation almost everywhere—a concept closely aligned with the definition of weak solutions. WAN, introduced in \\cite{zang2020wan}, builds upon the PINN framework but incorporates the weak formulation of the equation and employs an adversarial neural network as the test function. It is reported to be more effective than PINN in finding weak solutions, particularly in higher-dimensional cases. To assess the performance of these methods, we compare their accuracy, consuming time, and the number of parameters. Specifically, $L_2$ and $L_{\\infty}$ norms are used to quantify accuracy: \\begin{equation*} L_2^u:=\\frac{\\left\\|u-u_e\\right\\|_2}{\\left\\|u_e\\right\\|_2}, \\quad L_{\\infty}^u:=\\frac{\\left\\|u-u_e\\right\\|_{\\infty}}{\\left\\|u_e\\right\\|_{\\infty}}, \\end{equation*} where $u_e$ is the reference solution. \\subsection{Experimental Setup} The PyTorch implementation of PINN is employed, incorporating standard enhancements such as exponential learning rate decay and the use of the L-BFGS optimizer following initial training with ADAM. For fair comparison, WAN is also reimplemented in PyTorch. All neural network training is performed on a GPU (Tesla V100-PCIE-32GB), whereas WRFM solves its linear system exclusively on a CPU (Intel Xeon CPU E5-2630 v4 @ 2.20GHz). Optimal hyperparameters for each method are carefully selected to ensure a balance between accuracy and comparable training time. The best-performing hyperparameters for WRFM are summarized in Table \\ref{tab:hyperparameters}."}
{"input": "of methods in solving the Helmholtz equation on regular domain.} \\label{tab:helmholtz_reg} \\end{table} \\begin{figure}[h] \\centering \\includegraphics[width=3.7in]{charts/Helmholtz_reg_bc.png} \\caption{Helmholtz equation: comparison of obtained solutions on boundary.} \\label{pic:Helmholtz_bc} \\end{figure} \\begin{figure}[H] \\centering \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/PINN_Helmholtz_reg_solution.png} \\caption{PINN solution} \\end{subfigure}% \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/PINN_Helmholtz_reg_error.png} \\caption{PINN absolute error} \\end{subfigure} \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/WAN_Helmholtz_reg_solution.png} \\caption{WAN solution} \\end{subfigure}% \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/WAN_Helmholtz_reg_error.png} \\caption{WAN absolute error} \\label{fig:sub4} \\end{subfigure} \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/RFM_Helmholtz_reg_solution.png} \\caption{WRFM solution} \\end{subfigure}% \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/RFM_Helmholtz_reg_error.png} \\caption{WRFM absolute error} \\label{fig:sub4} \\end{subfigure} \\caption{Helmholtz equation on regular domain.} \\label{pic:Helmholtz_reg} \\end{figure} Equation \\eqref{eq:Helmholtz} is also examined on a complex domain: \\begin{equation*} \\begin{aligned} \\Omega = \\{(x,y)\\in\\mathbb{R}^2|\\ x\\in[-2,1], y\\in[0,1]\\} \\setminus \\bigcup_{i=1}^N D_r(x_i,y_i)\\,,\\\\ \\end{aligned} \\end{equation*} where \\( D_r(x_i, y_i) \\) denotes a disk with radius \\( r \\) and center \\( (x_i, y_i) \\). In the case under consideration, \\( r = 0.05 \\), \\( N = 8 \\), and the disks are arranged in a hexagonal packing pattern at the center of the domain. \\begin{figure}[H] \\centering \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/PINN_Helmholtz_com_solution.png} \\caption{PINN solution} \\end{subfigure}% \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/PINN_Helmholtz_com_error.png} \\caption{PINN absolute error} \\end{subfigure} \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/WAN_Helmholtz_com_solution.png} \\caption{WAN solution} \\end{subfigure}% \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/WAN_Helmholtz_com_error.png} \\caption{WAN absolute error} \\end{subfigure} \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/RFM_Helmholtz_com_solution.png} \\caption{WRFM solution} \\end{subfigure}% \\begin{subfigure}{.5\\textwidth} \\centering \\includegraphics[width=2.3in]{charts/RFM_Helmholtz_com_error.png} \\caption{WRFM absolute error} \\end{subfigure} \\caption{Helmholtz equation on complex domain.} \\label{pic:Helmholtz_com} \\end{figure} The considered methods are still capable of providing precise solutions, as shown in Figure \\ref{pic:Helmholtz_com}, with WAN again outperforming PINN in terms of accuracy. However, the best result in this case is obtained by WRFM, which produces a more accurate solution using fewer trainable parameters and requiring fewer resources for training, as summarized in Table \\ref{tab:helmholtz_com}."}
{"input": "no strong solution due to the incompatibility of the specified boundary conditions, which are not continuous. Nevertheless, a weak solution to this problem exists: \\begin{equation*} u(x,y) = \\left(\\frac{9}{2}(x^2-x) + 1\\right)\\cdot\\mathbb{I}_{[(x-y)(y+x-1)]} + \\left(\\frac{9}{2}(y-y^2) - 1\\right)\\cdot\\mathbb{I}_{[(y-x)(y+x-1)]}, \\end{equation*} where \\begin{equation*} \\begin{aligned} \\ &\\mathbb{I}_{[f(x,y)]} = \\begin{cases} 1\\ \\text{if}\\ f(x,y)>0\\,,\\\\ 0\\ \\text{if}\\ f(x,y)\\leq 0\\,. \\end{cases} \\end{aligned} \\end{equation*} \\begin{figure}[H] \\centering \\begin{subfigure}{0.5\\textwidth} \\includegraphics[width=2.45in]{charts/PINN_Static_Heat_error.png} \\caption{PINN} \\end{subfigure}% \\begin{subfigure}{0.5\\textwidth} \\includegraphics[width=2.45in]{charts/RFM_Static_Heat_error.png} \\caption{WRFM} \\end{subfigure}% \\caption{The absolute error for static heat equation on complex domain: (a) PINN, (b) WRFM.} \\label{pic:Static_Heat} \\end{figure} Although WAN seems to fail in this case, the other two methods successfully solve the problem (see Figure \\ref{pic:Static_Heat}) and demonstrate comparable accuracy. However, PINN utilized more computational resources to achieve similar results. It is notable that in instances where the analytical solution is discontinuous ($y=x$ or $y=1-x$), the resulting accuracy is diminished. The reason is that both methods are trying to find a continuous solution and approximate the break. This also helps to explain why $L_{\\infty}$ metrics are high while $L_{2}$ are moderate in Table \\ref{tab:static}. \\begin{table}[H] \\centering \\begin{tabular}{|l|cccc|} \\hline \\textbf{Method} & \\textbf{Params} & \\textbf{Time (s)} & \\textbf{$L_2$} & \\textbf{$L_\\infty$} \\\\ \\specialrule{.15em}{.0em}{."}
{"input": "0,037 & 0,136 \\\\ ~ & \\textbf{2940} & \\textbf{616} & \\textbf{0,036} & \\textbf{0,133} \\\\ \\hline WAN & 940 & 494 & 0,051 & 0,170 \\\\ ~ & \\textbf{2010} & \\textbf{479} & \\textbf{0,040} & \\textbf{0,151} \\\\ ~ & 2940 & 492 & 0,046 & 0,183 \\\\ \\hline WRFM & 160 & 418 & 0,072 & 0,093 \\\\ ~ & \\textbf{200} & \\textbf{438} & \\textbf{0,062} & \\textbf{0,074} \\\\ ~ & 220 & 608 & 0,073 & 0,091 \\\\ \\hline \\end{tabular} \\caption{Performance of methods in solving the Poisson`s equation.} \\label{tab:poisson} \\end{table} \\subsection{3D Heat equation} Heat equation is considered on regular domain $\\Omega=\\{(x,y,t)\\in\\mathbb{R}^3|\\ x\\in[-1,1], y\\in[0,1], t\\in[0,1]\\}$ for $a=1$: \\begin{equation}\\label{eq:Heat} \\left\\{ \\begin{aligned} &\\Delta u - a^2 u_{t} = 2\\sin\\left(|x|+|y|-1\\right)(t-1)e^{-\\frac{t^2}{a^2}}\\,,\\\\ &u|_{\\partial\\Omega\\setminus\\{(x,y,1)\\}} = \\sin\\left(|x|+|y|-1\\right)e^{-\\frac{t^2}{a^2}}\\,. \\end{aligned} \\right. \\end{equation} In this case all methods take a considerable amount of time for finding solution. Performance of PINN is notably inferior while WRFM and WAN demonstrate significantly better results (see Table \\ref{tab:heat}). It is noteworthy that in this particular case, the use of WAN appears to offer an advantage in terms of accuracy. Nevertheless, WRFM persists in demonstrating enhanced satisfaction with boundary conditions as well as superior handling of non-smooth parts of the solution (see Figure \\ref{pic:Heat}). \\begin{table}[H] \\centering \\begin{tabular}{|l|cccc|} \\hline \\textbf{Method} & \\textbf{Params} & \\textbf{Time (s)} & \\textbf{$L_2$} & \\textbf{$L_\\infty$} \\\\ \\specialrule{.15em}{.0em}{."}
{"input": "boundaries of metrology, optoelectronics and quantum computing\\cite{Mashiko2018,Garg2016,Schiffrin2013,Schultze2013,Krausz2014,Zong2023,clark_2007,Wang2017}. While ultrafast spectroscopy provides a unique window into non-equilibrium systems, the interpretation and explanation of experimental observations is often a difficult task. Factors like noise and impurities present in real experiments are compounded by the inherent complexity of a time-evolving system. Thus in practice it can be difficult to unambiguously determine the mechanisms behind observed phenomena. Theory is often used alongside experiment as a secondary source of confirmation and can offer more flexibility, aiding in verification of proposed hypotheses. For example, by simulating the same system with different parts of the model switched on or off (e.g, thermal effects, electron-phonon interactions, spin-orbit coupling, etc.) one can determine more concretely what contributes to specific observations. Therefore, in parallel to advances in time-resolved experimental techniques there has been a broad effort in the theory community to develop practical computational tools to study time-dependent systems and to work in conjunction with experiment to improve understanding of non-equilibrium phenomena. Developing efficient non-equilibrium simulations is challenging as it takes the already difficult problem of simulating equilibrium quantum systems and adds the additional complexities of coupling to time-dependent fields, tracking the systems evolution and---depending on observables of interest---accurately capturing dissipation effects. Exact solutions to this problem scale exponentially with system size making them intractable for anything beyond small systems. In practice approximations are necessary, requiring trade-offs between accuracy and computational cost. Different approaches are employed depending on a variety of factors, such as the quantities of interest (e.g."}
{"input": "function formalism---. Furthermore, as with TD-DFT's---at least in theory---exchange correlation potential, the self-energy introduces dependence on the system's past into the time-evolution. A major advantage the GF approach holds over TD-DFT is that although the exact form of the self-energy is not known, outside of small, exactly solvable systems, there is a well defined procedure to approximate it and to systematically improve upon these approximations\\cite{Hedin_1965,martin2016interacting,Mejuto_2022,Stefanucci_2014,Kadanoff_1962}. Unfortunately, despite the non-equilibrium GF (NEGF) formalism being known for six decades\\cite{Keldysh_1964} and the advantages it holds, it has not found wide adoption beyond relatively simple model problems. This challenge arises primarily from the structure of the Green's function equations of motion, known as the Kadanoff–Baym equations (KBEs). Solving the KBEs scales cubically with the duration of the time evolution, due to the memory effects introduced by the time-nonlocal self-energy, which manifest in the equations of motion as integrals over the systems past states. Several attempts have been made to reduce the scaling of the GF equations of motion, however until recently these approaches have either focused on the time evolution of the equal time GF (equivalent to the density matrix and not applicable to TR-ARPES) or have failed to reduce the computational cost to the ultimately desired linear scaling needed for practical ab-initio simulations\\cite{Lipavsky_1986,kaye_2021,schlunzen_2020,Perfetto_2022,chan_2021}. Seeing a clear need for a practical framework for the efficient simulation of time-resolved spectral properties we have recently introduced a method to addresses this\\cite{Reeves_2024}."}
{"input": "time GF, the key ingredient in time-resolved spectra, while also including dynamical correlation effects, crucial for properly accounting for the many-body nature of the system, without explicit integral evaluation. This allows for the GF to be evolved only within the window which is probed by the TR-ARPES measurement (typically much smaller than the total time-evolution) reducing the scaling from $O(N_t^3)$ to $O(N_t + \\Tilde{N_t}^2)$, where $N_t$ is the total number of time steps and $\\Tilde{N}_t$ scales with the width of the probe window with $\\Tilde{N}_t \\ll N_t$. In the remainder of this perspective we will give a brief theoretical introduction to NEGFs before discussing the new method. We will introduce the theory behind our scheme and provide several illustrative examples showing its ability to accurately capture dynamical correlations in time-resolved spectra, and it's scalability by showing simulation of a system several times larger than has previously been possible while including fully dynamical self-energy effects necessary to capture emergent phenomena. We will conclude with an outlook of how the RT-DE can be used as the base of a framework for the \\emph{ab-intio} simulation of time-resolved spectra and discussion of current and future work to make this a reality."}
{"input": "\\section{Theoretical background} \\subsection{Notation} In the remainder of this perspective we will make use of notation commonly used in the field of NEGFs. The relevant notations encountered in this work and their descriptions are provided in Table \\ref{tab:notation}. \\begin{table} \\centering \\begin{tabular}{||c|c||} \\hline Notation& Description \\\\ \\hline & GF corresponding to\\\\$G^<(t,t')$ & occupied portion of the spectrum\\\\ \\hline & GF corresponding to\\\\$G^>(t,t')$ & unoccupied portion of the spectrum\\\\ \\hline & GF corresponding to \\\\$G^R(t,t')$&the full spectrum of the system\\\\ \\hline & Self-energy computed with \\\\$\\Sigma(t,t')$&many-body perturbation theory which \\\\ & can also have a R$/\\lessgtr$ component\\\\ \\hline & GF computed \\\\$G^\\mathrm{MF}(t,t')$&using a mean-field Hamiltonian\\\\ \\hline & Time and momentum resolved \\\\$\\mathcal{A}^{</\\mathrm{R}}(k,\\omega,t_p)$ & spectral function computed with \\\\&$G^<(t,t')$/$G^\\mathrm{R}(t,t')$ centered around $t_p$\\\\ \\hline \\end{tabular} \\caption{Notation for the non-equilibrium \\\\Green's function formalism} \\label{tab:notation} \\end{table} \\subsection{The time-resolved spectral function} As mentioned in the introduction, one of the key experimental probes of quantum materials both in and out of equilibrium is ARPES and it's time-resolved extension\\cite{Krausz_2009,Boschini_timeresolved_2024}. Photoemission experiments probe the energy distribution of electrons in a material by using photons to eject electrons from a sample. By measuring the electron's outgoing kinetic energy, $E^\\mathrm{kin}$, the electron's binding energy can be computed with $\\varepsilon^\\mathrm{binding} = E^\\mathrm{kin} - \\hbar\\omega^\\mathrm{photon}$, where $\\hbar\\omega^\\mathrm{photon}$ is the energy of the photon. Equivalently however, $\\varepsilon^\\mathrm{binding}$ is the difference between the energy of the system before and after the removal of the electron."}
{"input": "the energy after removal of a specific electron. This quantity is directly related to the GF, which is a probability amplitude of an electron (or hole) injected at one space-time coordinate to be found at a different space-time coordinate and is expressed mathematically below in terms of electron creation and annihilation operators, $c^\\dagger(\\mathbf{r},t)/c(\\mathbf{r},t$, and the many-body ground-state wavefunction for the $N$ particle system, $|\\Psi^N_0\\rangle$. We will now show that $\\varepsilon^\\mathrm{binding}$ can be extracted theoretically by considering the time evolution of the GF. \\begin{align*} G(\\mathbf{r},t; \\mathbf{r}',t') &= \\langle \\Psi_0^N|c^\\dagger (\\mathbf{r},t)c(\\mathbf{r}',t')|\\Psi^N_0\\rangle\\\\ &= \\langle \\Psi_0^N|e^{-i\\mathcal{H}t}c^\\dagger(\\mathbf{r}) e^{-i\\mathcal{H}(t'-t)}c(\\mathbf{r}')e^{i\\mathcal{H}t'}|\\Psi^N_0\\rangle\\\\ &=e^{-iE_0^N(t-t')}\\langle \\Psi_0^N|c^\\dagger(\\mathbf{r}) e^{-i\\mathcal{H}(t'-t)}c(\\mathbf{r}')|\\Psi^N_0\\rangle\\\\ &=\\sum_{i}e^{-iE_0^N(t-t')}\\langle \\Psi_0^N|c^\\dagger(\\mathbf{r}) e^{-i\\mathcal{H}(t'-t)}|\\Psi_i^{N-1}\\rangle \\langle \\Psi_i^{N-1}|c(\\mathbf{r}')|\\Psi^N_0\\rangle\\\\ &=\\sum_{i}e^{-iE_0^N(t-t')}\\langle \\Psi_0^N|c^\\dagger(\\mathbf{r}) e^{-iE_i^{N-1}(t'-t)}|\\Psi_i^{N-1}\\rangle \\langle \\Psi_i^{N-1}|c(\\mathbf{r}')|\\Psi^N_0\\rangle\\\\ &=\\sum_{i}e^{-i(E_0^N-E_i^{N-1})(t-t')}\\langle \\Psi_0^N|c^\\dagger(\\mathbf{r})|\\Psi_i^{N-1}\\rangle \\langle \\Psi_i^{N-1}|c(\\mathbf{r}')|\\Psi^N_0\\rangle\\\\ &=\\sum_i\\psi^{N-1}_i(\\mathbf{r})\\psi_i^{N-1^*}(\\mathbf{r}')e^{-i\\epsilon_i^{\\mathrm{binding}}(t-t')}. \\end{align*} Here we have an included explicit coordinate $\\mathbf{r}$, this can a spatial variable or thought of as a discreet index labeling spin, bands, orbitals etc, $\\varepsilon^\\mathrm{binding}_i$ corresponds to the removal energy of the $i^{\\mathrm{th}}$ electron and $\\psi^{N-1}_i(\\mathbf{r})$ is the corresponding Dyson orbital with coordinate $\\mathbf{r}$. In the above we have inserted a complete set of eigenstates and made use of the fact that there will be zero overlap between $c|\\Psi^N_0\\rangle$ and any state that differs by more than the removal of a single particle. It is clear from the above expression that by Fourier transforming the GF to frequency space we gain access to the oscillation frequencies which correspond to the electron removal energies measured in photoemission experiments."}
{"input": "state (non-interacting or mean-field) and adiabatically switching on the self-energy contribution to prepare the correlated ground state. Adiabatic switching poses a possible issue for initial state generation as it often fails in systems with strong correlations, however in practice the self-energy approximation, based on MBPT, also breaks down for strongly correlated systems. Therefore, adiabatic switching is typically suitable in practical numerical simulations where the GF formalism is reliable. We emphasize that equation \\eqref{eq:GF_eom} is exact in principle but in practice the exact form of the self-energy operator, $\\Sigma$, is not known and must be approximated with many-body perturbation theory\\cite{Hedin_1965,Kadanoff_1962,martin2016interacting}. \\begin{figure} \\centering \\includegraphics[width=.5\\linewidth]{Fig1.png} \\caption{Schematic representation of the two-time grid on which the non-equilibrium Green's function lives. Propagation of the NEGF requires time-evolution in both time variables at each time-step.} \\label{fig:two_time_grid} \\end{figure} The primary reason the NEGF formalism has not found a wide adoption, especially for ab-initio studies of materials, is due to the structure of the KBEs. Unlike equilibrium, where time translation invariance dictates that the GF is a function of a time difference, the NEGF depends explicitly on two times and must be propagated simultaneously in both coordinates. This is shown visually in Fig. \\ref{fig:two_time_grid} where in order to time-step by one increment the GF must be propagated for all $t_1$ while keeping $t_2$ fixed and vice-versa, as well as along the time-diagonal."}
{"input": "practical first principles calculations and they are extensively used only in application to various model systems\\cite{Kwong_1998,Kwong_2000,Kremp_1999,Lake_1992,Schmitt_Rink_1988,Dahlen_2007,Banyai_1998,blommel_2025,Ong_2025}. Overcoming the KBEs scaling issue has been one of the primary goals of NEGF research\\cite{Lipavsky_1986,kaye_2021,Kaye_2023,schlunzen_2020,Perfetto_2022,chan_2021,stahl_2022,blommel_2024,zhu_2025,chan_2023}. In particular, in recent years this has given rise to several important developments towards practical \\textit{ab-initio} simulations using NEGFs. This includes practical approximations to the KBEs, the most widely used of these being the Hartree-Fock generalized Kadanoff-Baym ansatz (HF-GKBA)\\cite{Lipavsky_1986} which is based on the factorization of the collision integral and neglects correlation effects coming from the non-equal time GF. The HF-GKBA has found great success in simulating the non-equilibrium density matrix even achieving linear scaling in the number of time-steps\\cite{schlunzen_2020,Perfetto_2022,Pavlyukh_2022,Pavlyukh_2022_2,Pavlyukh_2022_3}. Despite its widespread use and generally good performance, the HF-GKBA faces several significant limitations. First, it approximates the time-off-diagonal GF propagation at the Hartree-Fock level, neglecting dynamical correlation effects. As a result, it is suitable for describing the dynamics of the equal-time GF only. Second, the method inconsistently mixes dynamically correlated time evolution along the time diagonal with statically correlated evolution off of the time diagonal, introducing a potential source of error that is difficult to quantify. Finally, beyond modifying the self-energy, there is no clear or systematic framework for incorporating additional correlations into the off-diagonal components, hindering systematic improvement of the HF-GKBA. Another direction that has been pursued is in finding systematic time compression via low-rank approximations to the full two-time GF."}
{"input": "to evaluate the self-energy self-consistently this approximation gives rise to another profound result that has the effect of reducing the scaling of the KBEs from cubic to linear in the total number of time-steps. This will be discussed in the next section as well as outlining the RT-DE equations of motion and the steps of its derivation. Here we emphasize that since the RT-DE extends the equilibrium one-shot correction it should be straightforward to implement the RT-DE on top of real-time equilibrium GF codes. In particular we envisage combining the RT-DE with highly efficient real-time stochastic GF methods that have found great success in equilibrium\\cite{vlcek_2017,vlcek_2018,vlcek_2019} to study large-scale non-equilibrium problems. \\subsection{Time-evolution of two-body propagators} The key behind the reduced scaling of the RT-DE comes from the fact that the collision integrals from equation \\eqref{eq:GF_eom}, that contain the contributions from the dynamical self-energy, are equivalently expressed in the following integral free form: \\begin{equation}\\label{eq:coll_to_F} I^{\\lessgtr/\\mathrm{R}}_{im}(t,t') = -\\sum_{klp}w_{iklp}\\mathcal{L}^{C,\\lessgtr/\\mathrm{R}}_{lpmk}(t,t'),\\\\ \\end{equation} where we have explicitly included the matrix indices of the collision integral, $I^{\\lessgtr/\\mathrm{R}}(t,t')$, the two-body interaction tensor, $w_{ijkl}$, and an auxiliary two-particle GF, $\\mathcal{L}^{C,\\lessgtr/\\mathrm{R}}_{lpmk}(t,t')$. $w_{ijkl}$ can be a statically screened or bare Coulomb kernel. $\\mathcal{L}^{C,\\lessgtr/\\mathrm{R}}(t,t')$ holds the information of the self-energy and GF convolution and its exact form depends on the self-energy approximation used. It is the correlated portion of the full two-particle GF, meaning it cannot be expressed as a product of one-particle GFs, ie. $\\mathcal{L}^C = \\mathcal{L}^{(2)} - GG$ where $\\mathcal{L}^{(2)}$ is the full two-particle GF."}
{"input": "comes from the derivation of the GF equation of motion resulting in the Martin-Schwinger hierarchy that links the time-evolution of an $N$ particle GF to that of an $N+1$ particle GF\\cite{stefanucci2013nonequilibrium,martin_1959}. Clearly, knowing the time-evolution of $\\mathcal{L}^{C,\\lessgtr/\\mathrm{R}}(t,t')$ removes the need to explicitly evaluate $I^{\\lessgtr/\\mathrm{R}}(t,t')$ and consequently the need to store the entire two-time GF. The primary result of Ref. \\cite{Reeves_2024} is to show that this can be achieved under the approximation outlined in the previous section by allowing for the derivation of a coupled set of ODEs for $G^{\\lessgtr/\\mathrm{R}}(t,t')$ and $\\mathcal{L}^{C,\\lessgtr/\\mathrm{R}}(t,t')$. In practice this approach is conceptually equivalent to ``upfolding'' the self-energy to a higher dimensional representation. As a result this step removes the time-non-local integration. We will now outline the steps involved in deriving the RT-DE equations of motion. For simplicity we derive the equations for the retarded GF [$G^\\mathrm{R}(t,t')]$ using the second-Born self-energy, but emphasize that our scheme is generalizable to other components of the GF as well as other self-energy approximations including $GW$\\cite{Reeves_2024}. For the second Born self-energy $\\Sigma^{\\mathrm{R}}$ is given by \\begin{equation*} \\begin{split} \\Sigma^{\\mathrm{R}}_{ij}[G(t,t')] &= -\\sum_{klpqrs} w_{iklp}w_{qrsj}^\\mathrm{x}\\bigg{[}G^>_{lq}(t,t')G^>_{pr}(t,t')G^<_{sk}(t',t) \\\\ &\\hspace{75mm}- G^<_{lq}(t,t')G^<_{pr}(t,t')G^>_{sk}(t',t)\\bigg{]},\\\\ \\end{split} \\end{equation*} where $w_{qrsj}^\\mathrm{x} = w_{qrsj} - w_{qrjs}$ encodes the direct and exchange portion of the second-Born self-energy. Using the approximation of equation \\eqref{eq:one_shot} we get, \\begin{equation*} \\begin{split} \\implies \\Sigma^{\\mathrm{R}}_{ij}[G^{,\\mathrm{MF}}(t,t')] &= -\\sum_{klpqrs} w_{iklp}w_{qrsj}^\\mathrm{x}\\bigg{[}G^{>,\\mathrm{MF}}_{lq}(t,t')G^{>,\\mathrm{MF}}_{pr}(t,t')G^{<,\\mathrm{MF}}_{sk}(t',t) \\\\&\\hspace{60mm}- G^{<,\\mathrm{MF}}_{lq}(t,t')G^{<,\\mathrm{MF}}_{pr}(t,t')G^{>,\\mathrm{MF}}_{sk}(t',t)\\bigg{]}.\\\\ \\end{split} \\end{equation*} From this and the definition of $I^{\\mathrm{R}}(t,t')$ in equation \\eqref{eq:GF_eom} we have \\begin{equation*} \\begin{split} I^{\\mathrm{R}}_{im}(t,t') &= -\\sum_{klp} w_{iklp}\\bigg{[}\\sum_{qrsj}\\int_{t'}^t d\\Bar{t}\\medspace w_{qrsj}^\\mathrm{x}\\bigg{[}G^{>,\\mathrm{MF}}_{lq}(t,\\Bar{t})G^{>,\\mathrm{MF}}_{pr}(t,\\Bar{t})G^{<,\\mathrm{MF}}_{sk}(\\Bar{t},t) \\\\&\\hspace{60mm}- G^{<,\\mathrm{MF}}_{lq}(t,\\Bar{t})G^{<,\\mathrm{MF}}_{pr}(t,\\Bar{t})G^{>,\\mathrm{MF}}_{sk}(\\Bar{t},t)\\bigg{]}G^{\\mathrm{R}}_{jm}(\\Bar{t},t')\\bigg{]} \\\\ &= -\\sum_{klp}w_{iklp}\\mathcal{L}^{C,\\mathrm{R}}_{lpmk}(t,t')."}
{"input": "the second-Born self-energy approximation $\\mathcal{L}^{C,\\mathrm{R}}(t,t')$ has the following form, \\begin{equation}\\label{eq:explicit_F_form} \\begin{split} \\mathcal{L}^{C,\\mathrm{R}}_{lpmk}(t,t') &= \\int_{t'}^t d\\Bar{t}\\medspace w_{qrsj}^\\mathrm{x}\\bigg{[}G^{>,\\mathrm{MF}}_{lq}(t,\\Bar{t})G^{>,\\mathrm{MF}}_{pr}(t,\\Bar{t})G^{<,\\mathrm{MF}}_{sk}(\\Bar{t},t) \\\\&\\hspace{60mm}- G^{<,\\mathrm{MF}}_{lq}(t,\\Bar{t})G^{<,\\mathrm{MF}}_{pr}(t,\\Bar{t})G^{>,\\mathrm{MF}}_{sk}(\\Bar{t},t)\\bigg{]}G^{\\mathrm{R}}_{jm}(\\Bar{t},t')\\bigg{]}. \\end{split} \\end{equation} Taking the time derivative of the right hand side with respect to the integral's time argument (denoted as $\\int$ in the subscript) gives, \\begin{equation*} \\begin{split} \\left[\\frac{\\mathrm{d}\\mathcal{L}^{C,\\mathrm{R}}_{lpmk}(t,t')}{\\mathrm{d}t}\\right]_{\\int} &= \\sum_{qrsj} w_{qrsj}^\\mathrm{x}(t)\\Big[ G^{>,\\mathrm{MF}}_{lq}(t)G^{>,\\mathrm{MF}}_{pr}(t)G^{<,\\mathrm{MF}}_{sk}(t) - G^{<,\\mathrm{MF}}_{lq}(t)G^{<,\\mathrm{MF}}_{pr}(t)G^{>,\\mathrm{MF}}_{sk}(t)\\Big{]} G^{\\mathrm{R}}_{jm}(t,t'). \\end{split} \\end{equation*} Taking the derivative of the $t$ dependent mean-field GFs appearing in equation \\eqref{eq:explicit_F_form} (denoted as $G^\\mathrm{MF}$ in the subscript) and making use of the special form of the mean-field GF equations of motion, \\begin{equation}\\label{eq:MF_eom} \\begin{split} \\frac{\\mathrm{d}G^{\\lessgtr,\\textrm{MF}}_{ij}(t,t')}{\\mathrm{d}t} &= -i\\sum_k h^{\\mathrm{MF}}_{ik}(t)G^{\\lessgtr,\\textrm{MF}}_{kj}(t,t'),\\\\ \\frac{\\mathrm{d}G^{\\lessgtr,\\textrm{MF}}_{ij}(t',t)}{\\mathrm{d}t} &= i\\sum_kG^{\\lessgtr,\\textrm{MF}}_{ik}(t',t)h^{\\mathrm{MF}}_{kj}(t),\\\\ \\end{split} \\end{equation} leads to the following \\begin{equation*} \\begin{split} &\\left[\\frac{\\mathrm{d}\\mathcal{L}^{C,\\mathrm{R}}_{lpmk}(t,t')}{\\mathrm{d}t}\\right]_{G^\\mathrm{MF}} = -i\\sum_{x}\\Big{[} h^\\mathrm{MF}_{lx}(t) \\mathcal{L}^{C,\\mathrm{R}}_{xpmk}(t,t') + h^\\mathrm{MF}_{px}(t) \\mathcal{L}^{C,\\mathrm{R}}_{lxmk} (t,t')- \\mathcal{L}^{C,\\mathrm{R}}_{lpmx}(t,t') h^\\mathrm{MF}_{xk}(t)\\Big{]}. \\end{split} \\end{equation*} These are combined to give the RT-DE equations of motion for the second-Born self-energy, \\begin{equation}\\label{RT-DE} \\begin{split} \\frac{\\mathrm{d}G^{\\mathrm{R}}(t,t')}{\\mathrm{d}t} &= -i\\left[h^{\\mathrm{MF}}(t) G^{\\mathrm{R}}(t,t') + I^{\\mathrm{R}}(t,t')\\right],\\\\ I^{\\mathrm{R}}_{im}(t,t') &= -\\sum_{klp}w_{iklp}(t) \\mathcal{L}^{C,\\mathrm{R}}_{lpmk}(t,t'),\\\\ \\frac{\\mathrm{d}\\mathcal{L}^{C,\\mathrm{R}}_{lpmk}(t,t')}{\\mathrm{d}t} &= \\sum_{qrsj} w_{qrsj}^\\mathrm{x}(t)\\Big{[} G^{>,\\mathrm{MF}}_{lq}(t)G^{>,\\mathrm{MF}}_{pr}(t)G^{<,\\mathrm{MF}}_{sk}(t) - G^{<,\\mathrm{MF}}_{lq}(t)G^{<,\\mathrm{MF}}_{pr}(t)G^{>,\\mathrm{MF}}_{sk}(t)\\Big{]} G^{\\mathrm{R}}_{jm}(t,t') \\\\&\\hspace{15mm}-i\\sum_{x}\\Big{[} h^\\mathrm{MF}_{lx}(t) \\mathcal{L}^{C,\\mathrm{R}}_{xpmk}(t,t') + h^\\mathrm{MF}_{px}(t) \\mathcal{L}^{C,\\mathrm{R}}_{lxmk} (t,t') - \\mathcal{L}^{C,\\mathrm{R}}_{lpmx}(t,t') h^\\mathrm{MF}_{xk}(t)\\Big{]}.\\\\ \\end{split} \\end{equation} The RT-DE proceeds with an initial time evolution of $G^\\mathrm{\\lessgtr,MF}(t,t)$ using \\begin{equation}\\label{eq:diagonal_eom} \\frac{dG^\\mathrm{\\lessgtr,MF}(t)}{dt} = -i [G^\\mathrm{\\lessgtr,MF}(t),h^{\\mathrm{MF}}(t)], \\end{equation} for some $h^{\\textrm{MF}}(t)$. We emphasize this can be any static single particle Hamiltonian such that the time-evolution of $G^{\\lessgtr,\\textrm{MF}}$ follows the form of equations \\eqref{eq:MF_eom} and \\eqref{eq:diagonal_eom}. Using this information for the time diagonal components in equation \\eqref{RT-DE} we can time-step in the $t$ variable in the range $t'<t<T_{\\mathrm{max}}$ with the following initial conditions for each $t'$, \\begin{equation*} \\begin{split} G^{\\mathrm{R}}(t',t') &= -i,\\\\ \\mathcal{L}^{C,\\mathrm{R}}(t',t') &= 0."}
{"input": "the reader to Ref.\\cite{Reeves_2024} for more detailed derivation and discussion, as well as derivations for the $GW$ self-energy approximation. We note that a similar approach has also been applied to the time-diagonal GF in a linear scaling HF-GKBA scheme\\cite{schlunzen_2020,joost_2020} but our work is the first application of this ODE approach for non-equal time GF and time-resolved spectral properties. \\subsection{Scalability of the RT-DE} Clearly, the trade-off for removing the explicit collision integrals is the need to now work with a four-index quantity, $\\mathcal{L}^{C}_{ijkl}$. This introduces two potential problems when considering scaling to large systems. The first is related to the numerical scaling which scales with system size $N$ as $O(N^5)$ for second-Born and $O(N^6)$ for the $GW$ self-energy\\cite{Reeves_2024}. This scaling comes from tensor contractions appearing in equation \\eqref{RT-DE}. Fortunately, these types of tensor summations are highly amenable to parallelization on GPUs and CPUs. Our current implementation of the RT-DE enables calculations to be run with GPU or CPU parallelization and allows for non-equilibrium spectra to be generated for multi-band systems several times larger than has previously been possible while accounting for dynamical correlations. Another avenue for optimization is through compression of the four-index tensor $\\mathcal{L}^{C,\\lessgtr/\\mathrm{R}}$. It is likely the case, especially as we increase the size of the system, that $\\mathcal{L}^{C,\\lessgtr/\\mathrm{R}}$ will have some degree of compressibility through a low-rank approximation. This can be applied to $\\mathcal{L}^{C,\\lessgtr/\\mathrm{R}}$ and the summations recast in a way that their scaling depends more favorably on the system size as well as the rank of $\\mathcal{L}^{C,\\lessgtr/\\mathrm{R}}$."}
{"input": "This includes approaches such as Cholesky decomposition and tensor cross interpolation\\cite{Pedersen_2024,Fernandez_2022,OSELEDETS2010}. The implementation of tensor decomposition in the RT-DE is a current focus of ours and combining this with our already GPU parallelized code will allow us to push to systems orders of magnitude larger than what has previously been possible. A final route to scalability of the RT-DE, that has already been briefly mentioned, is the leveraging of stochastic GF algorithms. These have been used to simulate systems with tens of thousands of electrons and have the advantage of already being used in real-time (as opposed to frequency space common to many equilibrium GF methods)\\cite{vlcek_2017,vlcek_2018,vlcek_2019}. The combining of the RT-DE with these stochastic real-time methods is a theoretically straightforward extension that would allow for the first-principles simulation of non-equilibrium quantum systems with thousands of electrons. The second potential issue introduced by time-evolving the two-particle GF $\\mathcal{L}^{C}$ is in the memory storage requirements. $\\mathcal{L}^C(t,t')$ now contains $N^4$ elements compared to $N^2$ for the single particle GF, where $N$ is the systems size. At first it may seem that storing this four index quantity, even just within the probing window of the full two-time grid, would lead to prohibitively expensive memory requirements for systems beyond $N\\sim O(10)$. However, this is not the case in practice: Firstly, we propagate ODEs instead of IDEs, so that each time-evolution in the $t$ direction only requires storage of $\\mathcal{L}^C$ at the \\emph{current} time-step."}
{"input": "$t=t'$, meaning that even for systems large enough that running each propagation simultaneously is an issue --- scaling as $\\tilde{N}_t\\times N^4$ for $\\tilde{N}_t$ RT-DE starting points --- the problem can be broken down into smaller components and run sequentially requiring at most the storage of a single $\\mathcal{L}^C$. Ultimately in order to extract single particle spectral properties the GF will be needed and so the memory requirements in reality will scale as $\\tilde{N}_t^2\\times N^2$ compared to the $N_t^{2}\\times N^2$ of the KBEs where $\\tilde{N}_t \\ll N_t$. Thus in addition to saving computational cost by reducing scaling from cubic to linear in the number of time-steps the RT-DE also offers huge savings in memory, necessary for simulating large scale systems. We highlight the importance of the ability to parallelize over temporal coordinates as well as other degrees of freedom. This adds even further to the scalability of the RT-DE and is relatively simple to implement simply requiring the starting points within probe region to be sub-divided evenly depending on the desired number of independent runs. Furthermore it can be trivially combined with the more traditional parallelization schemes discussed above. Finally, as we will discuss and demonstrate later, this independence of each RT-DE time-evolution can be combined with interpolation in the time-domain to reduce the number of RT-DE time-evolutions that must be performed along the $t$ direction."}
{"input": "computed with equation \\eqref{eq:spectral_function_perspective} using $G^<(t,t')$ and correspond to the occupied portion of the spectrum. Fig. \\ref{fig:finite_system} a) shows the ground state and Fig. \\ref{fig:finite_system} b)-d) show the spectra after excitation by the Hamiltonians given in equation \\eqref{eq:h_NE}. Detailed analysis and discussion of these results can be found in Ref. \\cite{Reeves_2024}. Here we highlight two important points. Firstly, we can see the inability of the HF-GKBA to produce results beyond those given by TD-HF, which shows the need for a method beyond the HF-GKBA (a current state of the art method for NEGF propagation). Second and most importantly the results demonstrate the ability of the RT-DE to quantitatively capture ground and excited state spectral features that are missed by the HF-GKBA and TD-HF results and are the result of dynamical correlations in the system. \\subsection{Excitonic insulator} The next application of the RT-DE we discuss is for the prediction of ground state properties of a model excitonic insulator\\cite{Jerome_1967,Kaneko_2025}, a type of correlated insulator that host excitons (bound electron-hole pairs) in its ground state. This system can be thought of as analogous to a superconducting system where Cooper pairs (bound pairs of electrons) are replaced by excitons and the superconducting pairing is replaced by Coulombic electron-hole attraction. Fig. \\ref{fig:excitonic_insulator} shows the full (occupied and unoccupied) ground state spectrum computed using $G^\\mathrm{R}(t,t')$ computed with Hartree-Fock and the RT-DE using the second-Born self-energy for the Hamiltonian given below."}
{"input": "of spinless fermions on a periodic lattice where $c$ and $v$ correspond to the conduction and valence band respectively. Similarly to the previous model the $J$ determines the strength of hopping between nearest neighbor sites in a given band and $U$ determines the interaction strength between particles on the same site and different bands. $\\Delta$ is the onsite potential for each orbital. Despite the translational invariance---under translation by a single site index---of the full many-body Hamiltonian, the excitonic insulating states can break this symmetry through the phenomenon of spontaneous symmetry breaking\\cite{Beekman_2019}. To preserve periodicity it is necessary to double the unit cell, making it pairs of neighboring sites. This doubling of the unit cell results in the Brillouin zone being reduced and ``back-folded'' into $k\\in [-\\frac{\\pi}{2},\\frac{\\pi}{2}]$. The symmetry breaking is taken into account as outlined in Ref.\\cite{Tuovinen_2020} and we use the parameters from this same reference for direct comparison between the RT-DE and KBE spectra: $J=1, \\Delta = 1.4J$ and $U=3.5$. \\begin{figure} \\centering \\includegraphics[width=.5\\linewidth]{Fig4.png} \\caption{Ground state spectrum for excitonic insulator described by equation \\eqref{eq:excitonic_ham} with $\\Delta = 1.4J$ and $U=3.5J$. Result computed with 28 sites backfolded to give $N_k=14$. The result is computed using a) HF (equivalent to $\\mathcal{L}^C =0$ in the RT-DE equations of motion) and b) the RT-DE evaluated with the second-Born self-energy} \\label{fig:excitonic_insulator} \\end{figure} Two striking differences exist between the mean-field (HF) and dynamically correlated (RT-DE) method."}
{"input": "we attribute to non-local self-energy effects. Second, the RT-DE result shows a pronounced back-bending in the dispersion around the gamma point, consistent with an excitonic ground state\\cite{Tuovinen_2020,Shao_2024,Werdehausen_2018,Huang_2024}. In essence, this is a signature of a large population of interacting excitons that comes from observations in superconducting systems\\cite{Rinott_2017,Takahiro_2020} where analogy is drawn between Cooper pairs and excitons. As RT-DE includes explicit propagation of a two-particle GF ($\\mathcal{L}^C$) it captures the formation of excitons, an emergent feature missing in the mean-field result. Both of these features are qualitatively and quantitatively consistent with what is predicted in Ref. \\cite{Tuovinen_2020} using the full KBE treatment. This example demonstrates the RT-DEs capability to predict properties of systems with non-trivial correlated ground states. In the future the RT-DE will allow us to perform time-resolved simulations of driven excitonic systems and to determine spectral signatures of excitonic phenomena such as melting of excitonic order or the excitonic BCS-BEC crossover\\cite{Bretscher2021,Takamura_2024,Perfetto_2019,Phan_2010}. Of particular interest is the comparison of the RT-DE and KBEs for time-resolved properties driven excitonic insulators, though KBE propagation's are limited to relatively small systems and short simulation times."}
{"input": "in the model: $J_\\alpha = \\{1,-1,-3,3,1,1\\}$, $U = 4J$, $E=0.25J$, $w_p = 8.0J$, $t_0 = 10J^{-1}$, $T_p = 10J^{-1}$ and $\\varepsilon = 2$. b) Spectral function computed using coarsened grid of RT-DE starting points interpolated with cubic interpolation. Original data uses $dt=0.025$ between starting points on the time-diagonal for the RT-DE propagation. Using our interpolation scheme it is possible to coarsen this by as much as 40 times with minimal loss of accuracy. Interpolated data reconstructed using $dt=1.0$ between starting points, while RT-DE propagation still uses $dt=0.025$. c) shows the error introduced by the interpolation scheme which is negligible even for this relatively coarse grid. We note that the interpolation is done parallel to the time-diagonal as opposed to perpendicular/parallel to the $t$ axis.} \\label{fig:interpolation} \\end{figure} This model primarily serves as a demonstration of what can be achieved using the RT-DE formalism as benchmarking in a system of this size is effectively impossible, even with an approximate method such as the KBEs. One route forward is comparing to Bethe-Salpeter calculations in a linear response regime (ie. in the low photodoping limit) but benchmarking beyond linear response will be a challenge due to lack of reference methods. In other words RT-DE allows for simulation of strongly driven (out of equilibrium) systems which cannot be described by other methods. However, the RT-DE does give a feasible way to simulate experimentally relevant systems which can be a more direct point of comparison."}
{"input": "calculations can still have high cost per time-step. Numerical extrapolation techniques such as the previously mentioned DMD\\cite{Yin_2022,Reeves_2023} can be included within the RT-DE framework as a post-processing step to reduce the number of time-steps that must be explicitly evaluated. The amount of explicit time-stepping can be further reduced by leveraging the interpolation that has already been demonstrated in Fig. \\ref{fig:interpolation}. Future work will improve upon this relatively simple fixed spacing interpolative scheme by allowing for adaptive interpolation\\cite{blommel_2024}. This will minimize the number of RT-DE update steps without sacrificing accuracy and remove the need to laboriously test many different interpolation spacings for each different problem. Machine learning in the form of recurrent neural networks (RNN) has also been used to reduce the cost of NEGF propagation\\cite{BASSI2024,zhu_2025}. We are currently investigating how this can be integrated with the RT-DE time-evolution as an advanced extrapolation \\emph{or} interpolation tool. \\item \\textbf{Advanced self-energy approximations:} Currently our results make use of the second Born self-energy. This is commonly used in NEGF calculations due to its low cost of implementation and relatively good accuracy. However, the practical workhorse of GF calculations in equilibrium \\emph{ab-initio} settings is the $GW$ approximation, thus it is desirable to work with this also in non-equilibrium. We have shown explicitly that an RT-DE equation of motion can be derived for the $GW$ approximation\\cite{Reeves_2024} and our theory can be extended to T-matrix and the dynamically screened ladder approximation\\cite{joost_2022,joost_2020}."}
{"input": "top of the KBEs poor scaling, but the savings offered by the RT-DE will allow for use of these more advanced self-energy approximations in practical calculations. \\item \\textbf{Tensor compression:} Tensor contractions appear throughout physics and algorithms for reducing their scaling\\cite{Evenbly_2022,Ran_2020,Khoromskij_2018}, typically based on decomposition into sums of tensors of lower order, are plentiful. These decompositions can reduce the cost of tensor contractions, typically having lower scaling with size of the tensors and gaining a dependence on the rank or approximate rank of the full tensor. Implementing this of course adds additional steps and cost for the decomposition steps, however this problem can take advantage of existing, highly optimized linear algebra software libraries to perform these steps efficiently. This optimization should be relatively straightforward using an existing algorithm such as tensor cross interpolation, but will require further investigation into the rank structure of $\\mathcal{L}^C$. \\item \\textbf{Extending stochastic real-time GF methods to non-equilibrium:} The final future direction we will mention is the extension of our groups existing real-time stochastic codes from equilibrium to non-equilibrium systems using the RT-DE formalism. This represents the most technically challenging of these points but it also holds the most potential for the simulation of large scale systems from first principles. The stochastic GF approach already uses a real-time formulation and so the extension is conceptually straightforward."}
{"input": "\\section{Introduction} The use of multi-agent reinforcement learning (MARL) in practical applications may face significant challenges primarily due to the high energy consumption and computational demands in training models {[}1{]}. The energy requirements of MARL systems, specifically those utilising large deep learning models, can be prohibitive, limiting their deployment in energy-sensitive environments such as mobile robotics and embedded systems {[}2, 3{]}. Traditional reinforcement learning (RL) algorithms show inefficiency in terms of computational demand and power uses, particularly when scaled to larger systems. There has also been a greater push in exploring energy efficient solutions in MARL and other deep learning applications such as deep-neural networks {[}4, 5{]}. By designing solutions to RL problems with higher levels of energy efficiency, the use of RL in multi-agent environment systems can be extended to a broader range of applications ensuring more sustainable and versatile deployments. However, it should be ensured that the shift towards energy efficient solutions does not compromise on the learning and decision-making capabilities of the RL agents. Many multi-agent systems require agents to undertake different roles within the environment, such as in RoboCup soccer, where robots are assigned specific roles to maximise team performance {[}6{]}. This role assignment may be achieved through inter-agent communication and coordination {[}7{]}, or through implicit role development {[}8{]}. These communication or design overheads can become substantial, and act as a hindrance to the performance as the number of agents in the environment increases."}
{"input": "communication or definition, creates a rigid structure in the population and can hamper adaptability and scalability. For these approaches to successfully develop complex behaviours and strategies the overhead of the solution significantly increases {[}9{]}, but by implementing a reward function that allows for implicit role definition, this increase can be mitigated while promoting flexible and scalable agent behaviours. In this paper, we propose a single-agent learning approach designed to reduce both the energy and computational demands of MARL systems in the case of resource foraging. This proposed solution leverages a model sharing mechanism where a single agent\\textquoteright s learned strategies are disseminated across the population of non-learning agents. This approach not only conserves energy by reducing the number of agents that engage in the computationally intensive learning process but also minimises the need for inter-agent communication by limiting the real-time data exchange to essential model updates. By reducing the number of learning agents in the environment, the time required for training in this environment will be reduced. Additionally, we introduce a reward function designed to facilitate implicit role development among agents, removing the need for explicit role definitions or cooperative communication and the associated overhead. The proposed reward function rewards the agent based on their success in resource collection and their proximity to other agents, which aims to encourage agents to naturally assume roles that optimise the collective performance, relying on environmental cues and agent interaction rather than predefined assignments."}
{"input": "\\section{Background: learning to forage} Q-Learning, a type of reinforcement learning, has advanced the field of reinforcement learning by allowing agents to learn optimal actions in a Markovian context without needing a model of the environment. It has laid the foundation for further innovations such as Deep Q-Learning (DQL), which integrates deep neural networks (DNNs) with Q-Learning to handle more complex problems with high-dimensional state spaces. These methods have not only propelled theoretical advancements but also practical applications in robotics, gaming, and notably in solving foraging problems {[}11{]}. Foraging problems, which involve searching for and gathering valuable items or information in an environment, mirror many real-world challenges across robotics and natural resource management {[}12{]}. The application of Q-Learning and DQL to these problems has opened up new avenues for research and optimisation in autonomous systems."}
{"input": "\\section{System model} \\subsection{The environment} The environment built for simulating and evaluating the performance of reinforcement learning approaches to resource collection is created using a square, discrete, grid-based spatial representation. The environment is bounded by hard borders restricting the agent movement to the grid boundaries. Hard borders were chosen over toroidal, or \\textquoteright wrap around\\textquoteright{} borders, in which an agent attempting to move off the edge of the grid would reappear on the corresponding cell on the opposite edge, due to the computational simplicity and structure they provide. The hard borders also better represent a real environment than the torodial border, where the resources only appear in a given area rather than infinite environment with no restrictions on movements. All agents within the environment are limited to orthogonal movements (up, down, left, right). The environment contains a resource density parameter $\\rho$, that dictates the probability of a resource being present within any given cell. The initial position of the ally and adversarial agents in the environment at the beginning of every episode is determined using a centred random distribution. The DQN agent is placed in the centre of the environment for the initial episode and for subsequent episodes follows the same distribution as the other agents. To establish a performance baseline within the simulated environment, adversarial agents are introduced into the environment to compete with the reinforcement learning agents for the resources available in the environment."}
{"input": "but comparative benchmark to the proposed approach. They follow a simplified resource collection algorithm that prioritises the closest visible resource. The choice of a fixed, simple strategy helps us in maintaining a clear performance baseline; in more complex scenarios, more advanced strategies for the adversary team should be considered. In brief, we have two competing teams of agents foraging a resource randomly distributed on a square grid. In the remainder of the paper, ``agent'' usually refers to a member of the team for which learning is happening, and not the adversarial team. \\subsection{The learning} The strategy chosen employs a single-agent reinforcement learning strategy within the foraging environment containing adversarial agents as an approach to solve a multi-agent reinforcement problem. Agents are divided into \\textquotedbl leaders\\textquotedbl{} and \\textquotedbl allies,\\textquotedbl{} with leaders engaging in continuous learning and policy refinement through Deep Q-learning (DQL). Deep Q-Networks (DQNs) are chosen for their proven ability to handle complex state spaces and process spatial information, crucial for effective navigation and decision-making in this context {[}11{]}. Leaders\\textquoteright{} learned models are periodically shared with allies, accelerating learning and promoting successful strategies across the group. In this case a single leader is chosen to learn in the environment. The use of a single learning agent will see a large reduction in the training time of a population and while aiming to match the performance of other approaches like multi-agent reinforcement learning (MARL) and centralised networks."}
{"input": "of specialised roles within the agent population. By balancing incentives around proximity to adversarial agents and distance from ally agents, the reward structure creates a complex optimisation landscape for the agents. This landscape gives the opportunity for the development of different agent roles within the environment. We can imagine a possible scenario in which the roles of \\textquoteright disruptors\\textquoteright{} and \\textquoteright explorers\\textquoteright{} are developed. The ally agents and the leader can be carrying out two different actions within the populations. The leader agent and two ally agents could be operating in close proximity to the enemy agents, acting as disruptors while two other ally agents are exploiting the resources away from all other agents. Implicit role development, as facilitated by the reward function, offers several advantages over explicit role assignment in multi-agent systems. The adaptability it allows is crucial in dynamic and unpredictable environments, enabling agents to respond in real-time to changes and optimise their strategies to current conditions rather than adhering to potentially outdated predefined roles. This flexibility enhances the system\\textquoteright s overall resilience and effectiveness, as agents continuously learn and evolve their behaviors to maximise the collective reward. Scalability is another significant advantage of implicit role definition. Managing and coordinating explicit roles becomes increasingly complex and impractical as the number of agents in the system grows."}
{"input": "the performance metric when comparing the frequencies of model sharing. Varying the model sharing frequency facilitates understanding the impact of model sharing on the collective learning curve and strategic synchronization of the agent network. With more frequent model sharing, allies quickly receive updates from the learning agent, potentially leading to a rapid alignment with the leader\\textquoteright s current strategy. This could result in less diversity in role specialization initially, as all allies quickly converge on the strategies proven effective by the learning agent. However, it also means that any strategic adjustments or role shifts identified by the learning agent are promptly transferred to the allies, allowing for a dynamic and responsive collective strategy. Conversely, less frequent model sharing allows ally agents more time to operate independently between updates, potentially fostering a broader exploration of the environment and the strategic space afforded by the reward function. This could encourage a greater diversity in role specialization, as allies might engage in different aspects of the environment or interact with adversarial agents in varying capacities, leading to a more heterogeneous set of strategies within the collective. \\begin{figure}[tb] \\begin{centering} \\textsf{\\includegraphics[scale=0.57]{fig61}\\label{f61}} \\par\\end{centering} \\caption{Average and variance of resource collection for each team for each configuration of episodes and lifetimes during the 50 evaluation episodes.} \\end{figure} \\begin{figure}[htbp] \\begin{centering} \\textsf{\\includegraphics[scale=0.5]{fig62}\\label{f62}} \\par\\end{centering} \\caption{Number of successes by each team across the 50 evaluation episodes for each configuration of episodes and lifetimes.} \\end{figure} The results of varying model sharing frequency are shown in Figs. 1 and 2."}
{"input": "multi-agent reinforcement learning (MARL) configuration is considered, wherein each leader agent independently learns and refines its strategy, absent of model sharing or collaborative learning mechanisms. This approach represents an autonomous learning environment where each agent is self-reliant and evolves its policy based on personal interactions with the environment, unaided by the experiences of ally agents. The second, a centralised DQN, wherein all agents\\textquoteright{} visible areas are inputted and processed by the network and every agent\\textquoteright s optimal action is outputted. This centralised approach is indicative of a collective intelligence paradigm where the decision-making process is unified for the cohort of agents. Both of these strategies are trained for 400 episodes, just as the single-learning agent strategy was to ensure consistency in training and a fair comparison. The same environment parameters for grid size, resource density and adversarial agents, shown in Table I, are used again in training and evaluating these comparison models. \\begin{figure}[tb] \\begin{centering} \\textsf{\\includegraphics[scale=0.56]{fig64}\\label{f64}} \\par\\end{centering} \\caption{Average and variance of resource collection for each team for each configuration of episodes and lifetimes during the 50 evaluation episodes.} \\end{figure} \\begin{figure}[tb] \\begin{centering} \\textsf{\\includegraphics[scale=0.5]{fig65}\\label{f65}} \\par\\end{centering} \\caption{Number of successes by each team across the 50 evaluation episodes for each RL approach} \\end{figure} \\begin{figure}[tb] \\begin{centering} \\textsf{\\includegraphics[scale=0.56]{fig66}\\label{f66}} \\par\\end{centering} \\caption{Average and variance of completion time per time step (in seconds) by each team across the 400 training episodes for each RL approach} \\end{figure} The results for the two comparison models and the single-agent learning approach are visualised in Figs 4 and 5."}
{"input": "and variance execution time of a single time step across the three approaches. Comparing the single-agent learning approach with MARL and centralised DQN model, it can be seen that the MARL approach shows the highest performance in ally team success but falls short of the average resource collection performance of the single-agent learning. The variance observed in the single-agent learning approach\\textquoteright s performance, suggests a broader range of outcomes. It implies that the single-agent learning model, at its best, outperforms the MARL approach in resource collection, but it can also fall behind under certain conditions. This results in a higher average resource collection but a lower success rate over the adversarial agents for the single-agent learning approach compared to MARL. The proposed approach sees a significantly higher performance in average resource collection compared to the centralised DQN approach, but performs equally in success rate. This suggests that our approach is well-suited to scenarios in which higher total resource collection across all episodes is considered success rather than individual episode success. Comparing the average execution time per timestep, it can be seen that the proposed single-agent learning approach significantly outperforms the MARL approach. The proposed approach also outperforms MARL in average resource collection, showing its ability to perform well, with lower training time. However, it does fail to reach the training speed of the centralised DQN."}
{"input": "\\ref{related work} for a detailed review. EEG classification is a challenging task given the low signal-to-noise ratio in EEG signals and the presence of inter-subject variability requiring models to generalize well across individuals. Traditional machine learning methods for EEG classification have relied on standard hand-crafted features as inputs to models such as support vector machines, random forests or neural networks \\cite{chaturvedi2017quantitative} \\cite{betrouni2019electroencephalography}. These pipelines often use frequency-based features combined with select statistics of the EEG signal \\cite{sahota2024interpretable}. One key limitation for traditional feature-based methods has been poor performance in settings where prior knowledge is limited. For this reason, in the last decade research has focussed more and more on deep learning methodologies. Deep learning methods in this context range from convolutional neural networks (CNN) applied to the raw EEG data to audio-inspired networks that apply a CNN to EEG spectrograms (time-frequency image of the EEG) \\cite{oh2020deep}. Recent advances in deep learning architectures have also led to transformer-inspired networks such as EEG Conformer \\cite{eegconformer} and EEG2Rep \\cite{mohammadi2024eeg2rep} (self-supervised representation-based model) for classification. Although the performance across EEG classification tasks has improved, deep learning models are often opaque and their inner workings are hard to interpret. This leads researchers to post-hoc explainability methods in an attempt to understand models \\cite{posthoc_2022}. Explainability is a particularly critical issue for EEG as it can enhance insights into neurophysiological phenomena, aid clinical decision-making and improve trust in the model."}
{"input": "Deep learning architectures have also been developed to deal directly with sequential data from RNN variants such as LSTMs and GRU's to more recently Transformers. Transformers excel at capturing long-range dependencies and as such have been successfully used for EEG classification. In \\cite{song2022eeg} the authors present a novel architecture that uses a convolutional module to generate embeddings for input EEG data which is then passed to a Transformer Encoder module for classification. This EEG Conformer achieves state of the art performance on many brain computer interface based tasks \\cite{song2022eeg}. One issue with deep neural networks is susceptibility to over-fitting \\cite{mohammadi2024eeg2rep}. With EEG data this is particularly problematic as signals are noisy and inter-subject variability can be present \\cite{schirrmeister2017deep}. Self-supervised learning (SSL) aims to solve this problem by learning a representation of the EEG and generating a self-supervisory signal from the data \\cite{weng2024self}. These models can learn from labelled or unlabeled data and the learnt representations used for classification. State-of-the-art SSL deep learning models include BENDR \\cite{kostas2021bendr}, BIOT \\cite{yang2024biot} and most recently EEG2Rep \\cite{mohammadi2024eeg2rep}. Thus, in summary, many deep learning models have been applied to and developed specifically for EEG data. Deep learning has been successful in the classification of EEG. However, explainability remains an issue with deep learning models being opaque and there is yet to be a model that is effective across task domains."}
{"input": "\\section{Methodology of KnowEEG} \\label{method} \\subsection{Problem Statement} We address the problem of EEG classification. Each EEG sample $X_i$ from a dataset $\\{X_1, X_2, \\dots, X_N\\}$ maps to a corresponding label $y_i$ from the set $\\{y_1, y_2, \\dots, y_N\\}$. The label is a scalar value corresponding to the class of each respective sample. Each EEG sample $X_i$ is multi-dimensional consisting of K electrode channels each with a sequence length of L. Our goal is to learn an explainable high-performance classifier that can map samples $X_i$ to labels $y_i$. This is primarily measured with performance. Accuracy and AUROC are used as performance metrics for binary classification. Balanced Accuracy and Weighted F1 Score are used for multi-class classification. Explainability is established via generating understandable features and using a tree-based model that provides direct access to feature importances. The explainability of KnowEEG is demonstrated in \\ref{analysis}. \\subsection{KnowEEG Model Architecture} % A full set of the per electrode features calculated can be found in the supplementary material % \\paragraph{Thread One : Per electrode features} \\ \\subsubsection*{\\textbf{Thread One : per electrode features.}} Per electrode we calculate 783 features from both the generalized time series domain and the EEG-specific domain. This includes basic features like mean, standard deviation and kurtosis to more complex time series features like autocorrelation at different lags and EEG specific features related to frequency components present in the signal. In order to do this calculation the time series Python package TSFresh \\cite{christ2018time} is used."}
{"input": "of connectivity metrics that they expect could be informative for classification. These connectivity metrics can be calculated for the entire signal or on sub components of the signal. We propose calculating these metrics for each sub band of the EEG signal with regards to the bands defined in \\ref{feat_eeg}. Therefore per connectivity metric, for the 14 electrode datasets there are 91 (features per sub band) x 6 sub bands (delta, theta, alpha, sigma, beta, gamma) which equates to 546 total features per metric (720 features for 16 electrode datasets). Some connectivity metrics require segmentation of the raw signal into epochs prior to calculation. We follow standard protocol for segmentation and detail this in Appendix \\ref{appendix_connectivity}. Therefore we have 8 candidate connectivity metrics (correlation - Spearman and Pearson) aswell as the afore stated 7 specific EEG metrics. We consider selection of a single connectivity metric in this thread of the pipeline as a hyperparameter. We propose selecting a single connectivity metric as opposed to selecting some features from all metrics for ease of interpretability. This hyperparameter is selected using 'local parameter selection' in order to reduce the computational requirement of KnowEEG. Local parameter selection means that only the connectivity data is being used for parameter selection and not the per electrode statistics."}
{"input": "Further information on each dataset and pre-processing can be found in Appendix \\ref{datasets}. \\begin{table}[ht] \\centering \\caption{Properties of the EEG datasets used (adapted from \\cite{mohammadi2024eeg2rep}). DREAMER \\cite{katsigiannis2017dreamer}, STEW \\cite{lim2018stew}, Crowdsourced \\cite{williams2023crowdsourced}, TUEV \\cite{TUEV_dataset} and TUAB \\cite{TUAB_dataset} } \\label{eeg_datasets} \\vskip 0.15in \\renewcommand{\\arraystretch}{1.5} \\resizebox{\\columnwidth}{!}{% \\begin{tabular}{lclccc} \\toprule \\textbf{Dataset} & \\textbf{Classification Task} & \\textbf{Dim.} & \\textbf{Freq.} & \\textbf{Duration} & \\textbf{Samples} \\\\ \\midrule DREAMER & Emotion Detection & 14 & 128Hz & 2s & 77,910 \\\\ STEW & \\makecell{Mental Workload} & 14 & 128Hz & 2s & 26,136 \\\\ Crowdsourced & \\makecell{Eyes Open/Closed } & 14 & 128Hz & 2s & 12,296 \\\\ TUEV & Event Detection & 16 & 256Hz & 5s & 112,464 \\\\ TUAB & \\makecell{Abnormal EEG } & 16 & 256Hz & 10s & 409,455 \\\\ \\bottomrule \\end{tabular} } \\renewcommand{\\arraystretch}{1} % Reset to default \\end{table} \\subsection{State-of-the-art Methods } \\label{competitors} We compare our pipeline to many state-of-the-art methods from the literature. Recent research has seen representation based deep learning methods excel on EEG data \\cite{kostas2021bendr}\\cite{mohammadi2024eeg2rep}. Thus we compare versus four deep learning representation based classification models. The recently developed EEG2Rep \\cite{mohammadi2024eeg2rep} as well as BIOT \\cite{yang2024biot}, BENDR \\cite{kostas2021bendr} and MAEEG \\cite{chien2022maeeg}. Per \\cite{mohammadi2024eeg2rep} these self-supervised models achieve the highest performance if they are first able to learn a representation of the EEG by training on the data in a self-supervised fashion without labels before being trained on the data with labels. This is referred to as the 'Fine Tuning' setting. Thus, we deploy these models in this fine-tuning fashion where they perform best."}
{"input": "We deploy EEG2Rep in both the fine-tuning fashion and the default fashion where it is deployed directly on the data without a pre-training phase. We also select the EEG Conformer \\cite{eegconformer} as a competitor model (deep learning model for classification). This model is adapted from transformer architecture. Finally, we select a general time series feature-based method, Catch22 \\cite{lubba2019catch22}. These models were all implemented using publicly available code to ensure fair evaluation. \\subsection{Experimental Procedure} \\label{experimental_procedure} For all datasets our pipeline KnowEEG (see \\ref{method} ) and all competitor models (see \\ref{competitors} ) were trained on the training split of the dataset, had hyperparameters tuned using the validation dataset and were tested on a holdout test set. This was done for five different random seeds. Mean and standard deviation of performance metrics were used to assess model performance. For binary classification, accuracy and Area Under ROC Curve (AUROC) are selected as the performance metrics. For multi-class classification on the TUEV dataset Balanced Accuracy and Weighted F1 score are used. This is the exact same experimental procedure as in \\cite{mohammadi2024eeg2rep}. For KnowEEG the TSFresh \\cite{christ2018time} package is used with 'Efficient' settings to calculate per electrode statistics. The MNE Python package \\cite{MNE_2013} is used for calculating connectivity metrics. For the Fusion Forest the number of trees hyperparameter was selected from the list [50,100,200,500, 800, 1000]. Further details for the set up of KnowEEG are provided in Appendix \\ref{appendix_knowEEG}."}
{"input": "and TUAB datasets. With 10 total performance metrics across 5 datasets KnowEEG achieves best performance vs. all other models on 6/10 metrics, second best on 3/10 (with 2/10 not significantly worse than the best model) and third best on 1/10 metrics. Thus, judging only by performance, KnowEEG is the best performing model overall versus state-of-the-art competitors. Crowdsourced is the EEG task where models perform best with the task of classifying eyes open vs eyes closed EEG data. Even on this dataset, KnowEEG has the highest accuracy. For AUROC KnowEEG significantly outperforms all other models with 98.27 and performs consistently across seeds with a small standard deviation of only 0.2. Notably, the other feature based method Catch22 \\cite{lubba2019catch22} performs well on this dataset achieving 89.57 \\% accuracy and 95.92 AUROC (second best). However, Catch22 performs poorly on all other datasets illustrating that traditional feature-based methods can struggle to perform well across a diverse set of EEG classification tasks. On STEW with the task of mental workload classification KnowEEG significantly outperforms all other models in both accuracy and AUROC with standard deviations of under 1 for both metrics again illustrating consistent performance across seeds. On DREAMER performance is more variable across seeds and closer to other models. However, KnowEEG still performs best in accuracy (though not significantly) and second best in AUROC (again not significant). For the TUEV dataset KnowEEG is third in balanced accuracy yet significantly outperforms all other models in weighted F1 score."}
{"input": "6 classes of EEG events. Therefore in this case balanced accuracy is not necessarily the best measure of overall model performance. This is because balanced accuracy ignores class distributions resulting in smaller classes having a disproportional impact on the balanced accuracy score and can be a drawback if targeting good accuracy on the entire dataset\\cite{grandini2020metrics}. Weighted F1-score combines both precision and recall with each class weighted proportionally. Thus, it is notable that KnowEEG outperforms all other models in this metric significantly. For abnormal EEG classification on TUAB KnowEEG performs second best to EEG2Rep on both accuracy and AUROC. However KnowEEG is within standard deviation of EEG2Rep so this result is not significant. BIOT also reaches the same performances as EEG2Rep and KnowEEG when accounting for standard deviation across runs. EEG2Rep Pre-trained \\cite{mohammadi2024eeg2rep} is overall the second best performing model achieving best performance or second best performance on 8/10 metrics across the five datasets. Besides performance, when compared with Deep Learning models, KnowEEG has the significant advantage of not requiring GPU for training. All self-supervised representation-based models (except EEG2Rep Default) were trained first on the training data in self-supervised fashion and then tuned in supervised fashion. This requires significant GPU resource, in particular for the larger datasets. KnowEEG does not require any GPU resource. KnowEEG also has the significant advantage of using calculated features that are inherently interpretable and a tree-based model which enables users to access feature importances directly. We demonstrate the benefits of this explainability in Section \\ref{analysis}."}
{"input": "73.11 $\\pm$\\scriptsize{1.95} & 43.88$\\pm$\\scriptsize{1.62} & 67.20$\\pm$\\scriptsize{2.31} & 78.36 $\\pm$\\scriptsize{1.71} & 83.89 $\\pm$\\scriptsize{2.64} \\\\ EEG2Rep (Default) & 54.61$\\pm$\\scriptsize{2.22} & 53.61$\\pm$\\scriptsize{2.07} & 91.19$\\pm$\\scriptsize{1.18} & 91.22$\\pm$\\scriptsize{1.23} & 70.26$\\pm$\\scriptsize{1.59} & 69.77$\\pm$\\scriptsize{2.03} & 44.25$\\pm$\\scriptsize{3.01} & 68.95$\\pm$\\scriptsize{2.89} & 77.85$\\pm$\\scriptsize{3.14} & 84.91$\\pm$\\scriptsize{3.07} \\\\ EEG2Rep (Pre-Trained) & \\underline{60.37}$\\pm$\\scriptsize{1.52} & \\textbf{59.42}$\\pm$\\scriptsize{1.45} & \\underline{94.13}$\\pm$\\scriptsize{1.21} & 94.13$\\pm$\\scriptsize{2.17} & 73.60$\\pm$\\scriptsize{1.47} & \\underline{74.40}$\\pm$\\scriptsize{1.50} & \\textbf{52.95}$\\pm$\\scriptsize{1.58} & \\underline{75.08}$\\pm$\\scriptsize{1.21} & \\textbf{80.52}$\\pm$\\scriptsize{2.22} & \\textbf{88.43}$\\pm$\\scriptsize{3.09}\\\\ \\rowcolor{customblue} \\textbf{KnowEEG} & \\textbf{61.53}$\\pm$\\scriptsize{2.59} & \\underline{55.29$\\pm$}\\scriptsize{2.58} & \\textbf{94.53}$\\pm$\\scriptsize{0.35} & \\textbf{98.27}$\\pm$\\scriptsize{0.13} & \\textbf{77.86}$\\pm$\\scriptsize{0.19} & \\textbf{85.96}$\\pm$\\scriptsize{0.8} & 45.06$\\pm$\\scriptsize{0.71} & \\textbf{78.28}$\\pm$\\scriptsize{0.56} & \\underline{80.18}$\\pm$\\scriptsize{0.24} & \\underline{87.71}$\\pm$\\scriptsize{0.23}\\\\ \\hline \\end{tabular} } % end resizebox \\renewcommand{\\arraystretch}{1} \\end{table*} \\subsection{KnowEEG Explainability Analysis} \\label{analysis} %Firstly, it is straightforward to inspect the importance's per feature and directly observe them. For brevity we do not present the global top feature list here. Here, we analyze the KnowEEG model trained on the training and validation sets for Crowdsource and demonstrate the key explainability benefits. The Crowdsource dataset presents the problem of binary classification on 14-channel EEG data. The two classes are eyes open and eyes closed. For interpretability, we propose analyzing the connectivity and statistical features separately. In hyperparameter selection, Coherence was selected as the connectivity measure for the Crowdsource dataset. As described in Section \\ref{method}, the selected connectivity metric is calculated between each electrode pair over six defined frequency bands (alpha, theta, delta, sigma, beta, gamma). Therefore, there are 91 x 6 = 548 coherence features. The number 91 refers to every combination of electrode-electrode pairs for the 14 electrodes (excluding pairing an electrode with itself)."}
{"input": "a surface plot of the 14 electrodes on the head, where each electrode is plotted as the sum of alpha connectivity values to all other electrodes. \\begin{figure}[h!] \\centering \\Description{Heatmap of the head showing alpha coherence for eyes closed and eyes open.} \\includegraphics[width=1.0\\columnwidth]{Figures/alpha_open_closed.png} \\caption{Alpha coherence across participants in training and validation set for eyes closed and eyes open plotted as heatmap on the head.} \\label{alpha_coh_head_plot} \\end{figure} In figure \\ref{alpha_coh_head_plot} we plot the mean alpha coherence for the eyes open and eyes closed classes. The dark colours for eyes open show low coherence across the entire head for alpha coherence in contrast to higher alpha coherence for the entire head for eyes closed participants. Alpha coherence is particularly high for the frontal region of the brain for eyes closed participants. This result is consistent with existing EEG literature which shows that \"functional connectivity in the alpha band decreases in the eyes open condition compared to eyes closed\" \\cite{gomezram_eyes_op_close}. This is referred to as alpha de-synchronization and demonstrates the effectiveness of our pipeline. Without sacrificing performance, we are able to explain how our model arrives at results and correctly discover knowledge about our classes. As implied by the feature importances graph, the corresponding heat map plots for the other power bands show noticeable differences between the two classes for gamma and beta and smaller differences for delta, theta and sigma. For reference the lowest importance band plot (theta) is provided in Appendix \\ref{theta_coh}."}
{"input": "of the brain is responsible for visual processing as it \"houses the visual cortex responsible for processing and interpreting visual stimuli\" \\cite{fossa2024occipital}. Therefore, again, from our KnowEEG pipeline we have been able to discover knowledge (signals from the Occipital brain region being most discriminative for the two classes) and have verified this versus existing neuroscience literature. This shows that our pipeline can aid discovery of new information where differences between classes are not yet understood. We extend this feature importance ranking beyond the top 10 features. In Figure \\ref{feat_import_graph} we count how many features from each brain region are within the top 5, 10, 20 and 100 features by feature importance. The Occipital brain regions continues to outscore all other regions in the Top 20 and Top 100 features confirming this finding. \\begin{figure}[h!] \\centering \\Description{Bar chart showing count of feature importances per Brain Region} \\includegraphics[width=1.0\\columnwidth]{Figures/Feat_importances.png} \\caption{Count of features in top N importances (N = 5, 10, 20, 100) by Brain Region} \\label{feat_import_graph} \\end{figure} Another interesting finding from Table \\ref{tab:feature_importance} is that the three highest importance features and 6 of the top 10 are permutation entropy from electrodes in the Occipital brain region. Permutation entropy is considered as 'a natural complexity measure for time series' \\cite{bandt2002permutation}. The calculation of permutation entropy requires two parameters (Dimension and Tau) which is why this feature can appear multiple times for the same electrode in Table \\ref{tab:feature_importance}. %The full mathematical definition of permutation entropy is provided in appendix A."}
{"input": "for eyes closed and eyes open for the highest importance ranked feature in Figure \\ref{perm_entropy_kde}. It is clear from the figures that the eyes closed group has a much broader distribution compared with the eyes open group. The eyes open group has a narrower (standard deviation 0.16 vs 0.47) and slightly higher peak. When testing the distributions for this feature, we use the two sample Kolmogorov–Smirnov test we find that the two distributions vary significantly (p-value\\textless0.0001). \\begin{figure}[h!] \\centering \\Description{KDE plot showing distribution of permutation entropy feature on O1 Channel for eyes open and eyes closed classes} \\includegraphics[width=1.0\\columnwidth]{Figures/Permutation_Entropy_KDE.png} \\caption{KDE for Permutation Entropy of EEG from O1 Channel (Occipital Lobe) for eyes open and eyes closed classes. Corresponds to rank 1 feature from Table \\ref{tab:feature_importance}} \\label{perm_entropy_kde} \\end{figure} Therefore, again our pipeline has enabled us to find out useful information about our classes. In this case that the distributions for permutation entropy on the electrode O1 channel calculated with dimension 6 and tau 1 are significantly different for the eyes closed and eyes open groups. \\subsection{Benefits and Limitations} \\label{Benefits} \\paragraph{Benefits} The main benefit of KnowEEG is the explainability of the model which is in contrast to many state of the art deep learning models. The features themselves have meaning in the generalized time series or EEG domain and the Fusion Forest is a tree-based model that allows users to directly access feature importances. We demonstrate this benefit in \\ref{analysis}."}
{"input": "\\section{Datasets} \\label{datasets} Datsets and preprocessing follow the same protocol as in \\cite{mohammadi2024eeg2rep}. \\subsection{Emotiv datasets} All Emotiv datasets were bandpass filtered and windowed into segments of length 256. This data is recorded at a sampling rate of 128Hz. Therefore, each segment is of 2 seconds in length. \\subsubsection*{\\textbf{DREAMER}} The DREAMER \\cite{katsigiannis2017dreamer} dataset is a multimodal database containing both electroencephalogram (EEG) and electrocardiogram (ECG) signals. These signals are recorded during affect elicitation with audi-visual stimuli \\cite{katsigiannis2017dreamer}. Data is recorded from 23 participants along with self-assessment of affective state after each stimuli. Self assessment is in terms of valence, arousal and dominance. For classification we use the arousal labels as per \\cite{mohammadi2024eeg2rep}. We utilize the toolkit Torcheeg for preprocessing which consists of low-pass and high-filters. We do not use the ECG data and use only the EEG data. The DREAMER dataset can be accessed here\\footnote{https://zenodo.org/records/546113}. \\subsubsection*{\\textbf{Crowdsource} } The Crowdsourced \\cite{gomezram_eyes_op_close} dataset was recorded with participants at rest in the eyes closed or eyes open state. Each recording is 2 minutes in total. There are 60 total participants with only 13 recording data for both states. Data is recorded using 14-channel EPOC devices and is initially recorded at 2048Hz and then downsampled to 128Hz. This data can be accessed via the Open Science Framework \\footnote{https://osf.io/9bvgh} \\subsubsection*{\\textbf{Simultaneous Task EEG Workload (STEW)}} The STEW dataset \\cite{lim2018stew} is an open access EEG dataset for multitasking and mental workload analysis. There are 48 total subjects and data is recorded using a 14-channel Emotiv EPOC headset."}
{"input": "\\section{Introduction} Recent experimental breakthroughs in atomic physics, quantum optics, and nanoscience have enabled the realization and control of highly tunable quantum many-body systems, establishing out-of-equilibrium dynamics as a central topic in both theoretical and experimental condensed matter research~\\cite{polkovnikov2011colloquium,eisert2015quantum,defenu2024out}. % The synergy between theory and experiment has shed new light on fundamental questions---such as how thermalization occurs in closed quantum systems~\\cite{neumann2010proof}---and is key to developing quantum technologies, including quantum computing and quantum simulation, where precise real-time control of quantum states is crucial. % One of the most studied protocols for inducing out-of-equilibrium dynamics is the quantum quench, where a system is prepared in an eigenstate $|\\psi_0\\rangle$ of a Hamiltonian $H_0$ and subsequently evolved under a different Hamiltonian $H$ for which $|\\psi_0\\rangle$ is not an eigenstate~\\cite{farhi2000quantum,lauchli2008spreading,fagotti2008evolution,rigol2009quantum,altshuler2010anderson,chen2011quantum,calabrese2012quantum,trotzky2012probing}. Universal features of the post-quench dynamics are captured by quantum information quantities, such as the scaling of the von Neumann entropy~\\cite{calabrese2004entanglement,calabrese2005evolution,dechiara2006entanglement,calabrese2007quantum,kim2013ballistic} and other entanglement measures \\cite{coser2014entanglement,murciano2022quench,jafari2025entanglement} with time. % Quantum information tools abstract away system-specific physical properties. % In particular, the dynamics of the von Neumann entropy $S_A$ of a region $A$ reveals how correlations spread between $A$ and the rest of the system. % In generic clean one-dimensional systems with local interactions and with approximately homogeneous initial states, $S_A$ increases linearly with time before saturating to a value proportional to the size of $A$~\\cite{calabrese2005evolution,dechiara2006entanglement,calabrese2007quantum,kim2013ballistic}."}
{"input": "the thermal ensemble or its generalizations~\\cite{rigol2007relaxation,alba2017entanglement}. % Through the holographic principle and the Ryu-Takayanagi formula~\\cite{ryu2006holographic}, the study of quantum information dynamics has a broad relevance also in the context of general relativity and black holes~\\cite{nozaki2013holographic}. Given the difficulty in carrying out ab initio calculations of quantum information dynamics, simple heuristic pictures have been developed. % Examples are the entanglement quasiparticle picture~\\cite{calabrese2006time,calabrese2016introduction,calabrese2020notes}, where correlations are spread by pairs of entangled quasiparticles with opposite momenta, the membrane picture~\\cite{nahum2017quantum,jonay2018coarse,zhou2020entanglement}, and the entanglement tsunami~\\cite{liu2014entanglement}. % These pictures are based on the description of the quantum information dynamics at the local level, which provides deeper insights than global quantities such as the von Neumann entropy. % Other approaches in this direction include the entanglement link representation~\\cite{singha2020entanglement,singha2021link,santalla2023entanglement} and the entanglement contour~\\cite{chen2014entanglement,kudlerflam2019holographic}. This local perspective can in principle reveal novel fundamental aspects of information dynamics---for example, information interface effects that might arise in setups relevant to quantum transport where two systems with different quantum information properties, such as critical and noncritical systems, are suddenly connected~\\cite{eisler2007evolution,calabrese2007entanglement,eisler2008entanglement,stephan2011local,eisler2014area}, as well as the peculiar features of quantum information transport in topological systems~\\cite{smirnov2015majorana,sela2019detecting,bauer2023quench}. % However, constructing a universal framework that is capable of decomposing quantum information at the local level systematically at any evolution time is nontrivial since quantum information is intrinsically nonlocal. % Recent works~\\cite{artiaco2024universal,klein2022time,artiaco2024efficient,harkins2025nanoscale} demonstrated that the \\textit{information lattice} provides precisely such a decomposition for one-dimensional systems with open boundary conditions."}
{"input": "they act as ``reservoirs'' of local information~\\cite{beenakker2004quantum}. % We characterize information interface phenomena in this quench protocol. % In Sec.~\\ref{sec:majorana-teleportation-quench}, we analyze the flow of local information when a topological Kitaev chain is suddenly connected to a critical tight-binding chain. % The Kitaev chain hosts a pair of Majorana edge modes forming a delocalized fermionic mode, which can be occupied at zero energy cost~\\cite{kitaev2001unpaired,lathinen2017ashortintroduction,alicea2012new}. % While topologically protected, the delocalized fermionic mode is susceptible to decoherence---even under local coupling to just one of the constituent Majoranas~\\cite{goldstein2011decay,budich2012failure}. % We use the information lattice to systematically characterize this decoherence process. % Additionally, it allows us to derive simple analytical arguments that fully explain the fractional von Neumann entropy signatures recently observed in similar quench setups~\\cite{sela2019detecting,bauer2023quench}. % The quenches examined in this article serve as illustrative examples focusing on local quench protocols within noninteracting (topological) fermionic systems. % However, our framework is universal and applicable to any system, including interacting, disordered, or dissipative chains. % In Sec.~\\ref{sec:conclusions}, we summarize our findings and discuss potential future applications. \\begin{figure}[t] \\centering \\includegraphics[width=\\columnwidth]{plots/IL-Overview-Partition-withInterface.png} \\caption{(a) Illustration of the subsystem $\\mathcal{C}^\\ell_n$ encompassing $\\ell + 1$ physical sites centered around $n$. % (b) Schematic of the information lattice. % Shown is the information lattice for a Bell-pair product state on nearest-neighbor sites (bottom black-filled circles connected by red lines) for $N=8$. % Colors quantify local information: bright red indicates $i^\\ell_n=2$; gray dots correspond to $i^\\ell_n=0$. % Nonzero local information is present exclusively at $\\ell=1$ on every second lattice site $n$."}
{"input": "\\section{Information lattice} \\label{sec:information-lattice} \\subsection{Definition} In this section, we define the information lattice, which we then use to analyze the time evolution of local information in various quantum-quench protocols. % For simplicity, we assume that the physical sites have only two quantum degrees of freedom (qubits). % The von Neumann information (or total information) $I(\\rho)$ in a quantum state $\\rho$ equals the deficit of the von Neumann entropy $S(\\rho)$ from its maximum: % \\begin{align} I(\\rho) &= \\log_2[\\dim(\\rho)] - S(\\rho) \\notag \\\\ &= N + \\mathrm{Tr}[\\rho \\log_2(\\rho)], \\label{eq:von_neumann_information} \\end{align} % where $\\dim(\\rho)$ is the Hilbert space dimension of the entire system and $N$ is the total number of qubits. % $I(\\rho)$ is the information stored in the state $\\rho$ concerning measurement outcomes, that is, it is the additional predictive power gained by knowing the state $\\rho$ compared to assuming a maximally mixed state~\\footnote{ Specifically, $I(\\rho)$ is the additional average number of binary outcomes per measurement that can be predicted with certainty in the asymptotic limit of infinitely many measurements~\\cite{shannon1948bell}. % For example, if exactly one binary-outcome measurement can be predicted with certainty---as in the case of a qubit in a pure state---then $I(\\rho)=1$ bit.}. % Analogously, the von Neumann information in the reduced density matrix $\\rho_A = \\mathrm{Tr}_{\\bar{A}}(\\rho)$ of the subsystem $A$, % \\begin{equation} I(\\rho_A) = N + \\mathrm{Tr}[\\rho_A \\log_2(\\rho_A)], \\end{equation} % is the information stored in $\\rho_A$. We define local information as the decomposition of the total information in the state $I(\\rho)$ into local contributions at each scale and spatial location."}
{"input": "% In a one-dimensional system under open boundary conditions, such a decomposition is constructed in the following way. % We decompose the chain in all possible subsystems $\\mathcal{C}^\\ell_n$ made of $\\ell+1$ neighboring sites centered around position $n$ such that subsystems with $\\ell=0$ are the physical sites, as illustrated in Fig.~\\ref{fig:IL-Partition}(a). % The reduced density matrix of subsystem $\\mathcal{C}^\\ell_n$ is $\\rho^\\ell_n = \\mathrm{Tr}_{\\bar{\\mathcal{C}}_n^{\\ell}}(\\rho)$. % The local information $i^\\ell_n$ is then defined as the decomposition of the total information in $\\rho^\\ell_n $ for any $(n,\\ell)$, % \\begin{equation} \\label{eq:def-local-information-decomposition} I(\\rho^\\ell_n) = \\sum_{(n^\\prime,\\ell^\\prime) \\in \\mathcal{D}^\\ell_n} i^{\\ell^\\prime}_{n^\\prime}, \\end{equation} % with $\\mathcal{D}^\\ell_n = \\{ (n^\\prime, \\ell^\\prime) | \\mathcal{C}^{\\ell^\\prime}_{n^\\prime} \\subseteq \\mathcal{C}^\\ell_n \\}$. % This gives % \\begin{eqnarray} \\label{eq:def-local-information-mutual-information} i^\\ell_n = I(\\rho^\\ell_n) - I(\\rho^{\\ell-1}_{n-1/2}) - I(\\rho^{\\ell-1}_{n+1/2}) + I(\\rho^{\\ell-2}_{n}), \\end{eqnarray} % where it is implicit that the von Neumann information of empty subsystems is zero. % The local information $i^\\ell_n$ quantifies how much more we can predict about measurement outcomes by knowing the density matrix $\\rho^\\ell_n$ rather than the density matrices $\\rho^{\\ell-1}_{n-1/2}$ and $\\rho^{\\ell-1}_{n+1/2}$ of the smaller-scale subsystems contained in $\\mathcal{C}^\\ell_n$; it follows that $i^\\ell_n \\geq 0$. % As an example, consider a two-site system in a Bell pair state: $ \\lvert \\beta \\rangle = 1/\\sqrt{2} (\\lvert \\uparrow \\downarrow \\rangle + \\lvert \\downarrow \\uparrow \\rangle )$. % The single-site density matrices are $\\rho^0_0 = \\rho^0_1 = \\frac{1}{2} \\mathbb{1}$; thus, $i^0_0 = i^0_1 = 0$."}
{"input": "outcomes occurs with equal probability $1/2$. % In contrast, $i^1_{1 / 2} = 2$ shows that the total information in the state is accessible by measurements of two-site operators. The decomposition~\\eqref{eq:def-local-information-decomposition} defines the information lattice---a hierarchical triangular structure in which sites are labeled by indexes $(n,\\ell)$ and are associated with local information $i^\\ell_n$ in Eq.~\\eqref{eq:def-local-information-mutual-information}. % Fig.~\\ref{fig:IL-Partition}(b) illustrates the information lattice for an example state given by the product of Bell pairs $\\lvert \\beta \\rangle$ between nearest-neighbor sites. % Gray dots contain zero local information while red dots correspond to 2 bits. % As $i^\\ell_n$ precisely accounts for the information present only at a given scale and spatial location, in this example nonzero local information is present only at $\\ell=1$ on every second lattice site. % Information lattice sites with $\\ell > 1$ are associated with zero local information, as there are no correlations shared between sites belonging to different Bell pairs. Notice that the information lattice is also well-defined for mixed states. % The only distinction is that the total information in the system is less than that of a pure state, that is, $I(\\rho) < \\log_2 \\left[ \\dim(\\rho) \\right]$. % However, the definition of local information~\\eqref{eq:def-local-information-mutual-information} and all its associated properties (see Sec.~\\ref{sec:info-lattice-properties}) remain unchanged. \\subsection{Properties} \\label{sec:info-lattice-properties} The total information in the system $I(\\rho)$ defined in Eq.~\\eqref{eq:von_neumann_information} is conserved under unitary time evolution."}
{"input": "i^{\\ell}_{n} = \\mathrm{const.}$, with $\\mathcal{D} = \\{ (n, \\ell) \\, | \\, \\mathcal{C}^{\\ell}_{n} \\subseteq \\mathcal{S} \\}$ and $\\mathcal{S}$ the entire physical chain. % The decomposition of the total information performed by the information lattice makes $I(\\rho)$ akin to a hydrodynamic conserved quantity with well-defined local densities (that is, $i^\\ell_n$) and local currents. % Local information currents are defined similarly to those of a locally conserved operator in a system with a local Hamiltonian. % Their explicit form can be derived from the von Neumann equation of motion for subsystem density matrices~\\cite{klein2022time,artiaco2024efficient}. % Under the unitary time evolution governed by a local Hamiltonian, these currents propagate through the information lattice along the diagonal black lines shown in Fig.~\\ref{fig:IL-Partition}(b)-(c). % In conclusion, the information lattice acts as an ``information microscope'', revealing the fine-grained structure of correlations and their time evolution across different scales and spatial locations. The local perspective provided by the information lattice extends conventional quantum information approaches that study information transport using global quantities, such as bipartite von Neumann entropy and mutual information~\\cite{nielsen2010quantum,wilde2013quantum}. % Global approaches are limited in several ways compared to the information lattice. % For instance, by measuring the von Neumann entropy $S(\\rho_A)$ of a region $A$, one cannot know with which part of the complement region $A$ is entangled. % In other words, $S(\\rho_A)$ does not provide any knowledge about correlations at scales that exceed the size of the region $A$, and the length of $A$ defines the maximum scale of information that can be detected."}
{"input": "% As a second example, the mutual information cannot directly be used to quantify at which scale correlations are; the mutual information between two disjoint regions $A$ and $B$ exactly captures the correlations at the scale corresponding to the distance between $A$ and $B$ only if the state of the entire system is in a product state between $A \\cup B$ and the rest of the system. % In general, estimating the information at a certain scale using the mutual information between disjoint regions can lead to either an underestimation or an overestimation of the correlations~\\cite{artiaco2024universal}. \\subsection{Information lattice partitions according to regions $Q$, $X$, and $P$} By summing local information within two-dimensional partitions of the information lattice, well-defined global information quantities can be extracted. % This is again a consequence of definition~\\eqref{eq:def-local-information-decomposition}, which implies that different nonoverlapping partitions of the information lattice contain information of independent scale and spatial regions of the system. % Following Ref.~\\cite{bauer2023quench}, for the quench protocols discussed in this article we consider the physical chain as divided into three regions: $Q$ composed of $l_Q$ physical sites; $X$ made of $l_X$ physical sites on the right of $Q$; and $P$ of length $l_P$ on the right of $X$. % $Q$ contains the quenched physical site(s). % The separation layer $X$ provides a knob to investigate the time of flight, tails, and interface effects in the local information flow. % $P$ denotes a probe region to test the development of correlations within the system over time."}
{"input": "\\section{Releasing one particle in an empty tight-binding chain} \\label{sec:potential-well-quench} \\begin{figure}[tbp] \\centering \\includegraphics[width=\\columnwidth]{plots/occupation_density_NonCrit_TimeEvo.png} \\caption{Time evolution of the occupation density after releasing a particle at the central site $\\eta=100$ in an empty tight-binding chain with $N=201$ sites. % Before the quench, the system is in the ground state of the Hamiltonian~\\eqref{eq:HamTightBinding} with $\\mu_i = -20 \\tau_p$ and $\\mu_p = 20 \\tau_p$. % After the quench, the dynamics is governed by the Hamiltonian~\\eqref{eq:HamTightBinding} with $\\mu_f = \\mu_p = 20 \\tau_p$. } \\label{fig:occupation_density_NonCrit_TimeEvo} \\end{figure} \\begin{figure*}[tbp] \\centering \\includegraphics[width=\\textwidth]{plots/il_TB_SingleSite_NonCritical_TimeEvoPlots.png} \\caption{(a)-(c) Time evolution of local information after releasing a particle at the central site $\\eta = 100$ in a tight-binding chain with $N=201$ sites. % At $t=0^-$, the system is in the ground state of the Hamiltonian~\\eqref{eq:HamTightBinding} with $\\mu_i = - 20 \\tau_p$ and $\\mu_p = 20 \\tau_p$. % The post-quench dynamics is governed by the Hamiltonian~\\eqref{eq:HamTightBinding} with $\\mu_f = \\mu_p = 20 \\tau_p$. % Sites with local information $1 \\pm 0.01$ bits are marked in green. % The inset in (a) zooms into the information lattice near $\\eta$ at $t=0^-$. % } \\label{fig:il_TB_SingleSite_NonCritical_SummaryPlots} \\end{figure*} As a first step in studying the local information flow in quantum quench dynamics, we consider a simple protocol where one fermionic particle is released at the center of an empty tight-binding chain. % The system is governed by the tight-binding Hamiltonian % \\begin{align} \\nonumber H_\\mathrm{TB}(\\mu) &= (\\mu-\\mu_p) c_\\eta^\\dagger c^{\\phantom{\\dagger}}_\\eta \\\\ &+ \\sum_{i=0}^{N-1} \\mu_p c_i^\\dagger c^{\\phantom{\\dagger}}_i + \\sum_{i=0}^{N-2} \\left( \\frac{\\tau_p}{2} c_i^\\dagger c^{\\phantom{\\dagger}}_{i+1} +\\text{H.c."}
{"input": "spinless fermions, $N$ is the total number of sites in the physical chain, $\\mu$ is the tunable chemical potential at the central site $\\eta = \\lfloor (N-1)/2 \\rfloor$, and $\\mu_p$ and $\\tau_p$ are the chemical potential and the hopping amplitude of the rest of the chain. % By tuning $\\mu$, we create a potential well ($\\mu < \\mu_p$) or a potential barrier ($\\mu > \\mu_p$) at site $\\eta$. % At time $t=0$, we perform a local quench by suddenly changing $\\mu$ from $\\mu_i$ to $\\mu_f$. % Specifically, at $t=0^-$, the system is prepared in the ground state $|\\psi_0\\rangle$ of the initial Hamiltonian $H_\\mathrm{TB}(\\mu_i)$ with $\\mu_i = -20\\tau_p$ and $\\mu_p = 20\\tau_p$. % For $t \\geq 0$, the system evolves under the final Hamiltonian $H_\\mathrm{TB}(\\mu_f)$ with $\\mu_f = \\mu_p = 20\\tau_p$, which is homogeneous. % Thus, the initial Hamiltonian $H_\\mathrm{TB}(\\mu_i)$ includes a potential well at $\\eta$; its ground state $\\lvert \\psi_0 \\rangle$ contains only one particle localized at $\\eta$ while the rest of the chain is empty. % Due to the finite depth of the potential well, the wave function of the only particle present in the system has a small weight on the sites adjacent to $\\eta$. % This is reflected in the occupation-density distribution shown in Fig.~\\ref{fig:occupation_density_NonCrit_TimeEvo}, in which at $t=0^-$ the central site is approximately fully occupied and all other sites are empty."}
{"input": "0$, as illustrated in Fig.~\\ref{fig:il_TB_SingleSite_NonCritical_SummaryPlots}(a) where the green color corresponds to $i^\\ell_n \\approx 1$. % This short-scale localization of local information is characteristic of local product-like states~\\cite{artiaco2024universal}. % Near site $\\eta$, a small amount of information extends to $\\ell \\approx 1$ due to the short-range correlations associated with the overlap of the single-particle wave function localized at $\\eta$ on adjacent sites [inset of Fig.~\\ref{fig:il_TB_SingleSite_NonCritical_SummaryPlots}(a)]. % In the final Hamiltonian $H_\\mathrm{TB}(\\mu_f)$, all sites have the same chemical potential $\\mu = \\mu_f \\gg \\mu_i$. % As a consequence, during the post-quench dynamics, the particle released at $\\eta$ propagates throughout the empty tight-binding chain in both the left and right directions. % This propagation is captured by the flow of local information from short scales $\\ell \\approx 0$ throughout the information lattice, as shown in Figs.~\\ref{fig:il_TB_SingleSite_NonCritical_SummaryPlots}(b)-(c). The flow of local information occurs mainly in two distinct directions. % Horizontally, local information flows away from $(n, \\ell) \\approx (\\eta, 0)$ symmetrically, forming fronts that travel towards the left and right boundaries of the information lattice. % These fronts drag oscillating and decaying tails of local information, which are primarily concentrated at small scales and correspond to short-range correlations. % Vertically, local information moves from smaller to larger scales, signifying the development of longer range correlations. % The vertical front peaks at $n \\approx \\eta$ and travels at the same speed as the horizontal fronts. % It indeed carries the increasingly longer range correlations that emerge between the two horizontally moving fronts."}
{"input": "scales, local information patterns emerge that connect short- and long-range correlation fronts, moving diagonally within the information lattice. % They arise from the tails of the horizontally propagating fronts. % As discussed in Sec.~\\ref{sec:info-lattice-properties}, the flow of information within the information lattice is local. % This implies that both horizontal and vertical fronts propagate with finite speed $v_\\mathrm{LB} \\approx \\tau_p^{-1}$ according to the Lieb-Robinson bounds~\\cite{lieb1972the}. To understand the local information dynamics in this simple quench protocol, we start by considering the time evolution of the local occupation density in Fig.~\\ref{fig:occupation_density_NonCrit_TimeEvo}. % This shows that the initially localized wave function of the particle at the central site spreads over time throughout the physical chain. % This spreading is governed by the expansion coefficients of the localized wave function in terms of the eigenstates of the final Hamiltonian, which are standing waves, and leads to the particle delocalization along the physical chain. % The information lattice quantifies the total amount of correlations at the local level that develop during the delocalization process. % Let us now turn to Fig.~\\ref{fig:il_TB_SingleSite_NonCritical_SummaryPlots_totalInfo}, which illustrates the time behavior of $\\Gamma_\\Lambda$ in Eq.~\\eqref{eq:totalInformation} for $l_Q=101$ (that is, $Q$ includes the left half of the chain with $\\eta$ as the rightmost site), $l_X = 10$, and $l_P=90$. % As time progresses, local information shifts from the short-scale regions $\\overline{Q}$ and $\\overline{P}$ to the large-scale regions $\\overline{QX}$ and $\\overline{QXP}$."}
{"input": "at intermediate scales connecting short- and long-range fronts, in a finite-size system the convergence to these quantized asymptotic values is not perfect.}. % In particular, the change of total information in $\\overline{Q}$ and $\\overline{P}$ converges to $-1$ bit. % At larger scales (first in $\\overline{QX}$ and then in $\\overline{QXP}$) it converges to $2$ bits, which corresponds to the local information shared between two maximally entangled systems with local Hilbert space dimension $2$. % This behavior reflects the spread of the initially localized wave function throughout the system. % The two horizontally propagating fronts evenly distribute the probability of finding the particle among the left and right halves of the chain. % Moreover, detecting the particle in either half determines its absence in the other one. % Thus, the left and right halves are in a maximally entangled state: $1/\\sqrt{2} \\left( | 0 \\rangle_\\mathrm{L} | 1 \\rangle_\\mathrm{R} + e^{i \\theta} | 1 \\rangle_\\mathrm{L} | 0 \\rangle_\\mathrm{R} \\right)$ with $\\theta$ the relative phase difference. % The negative values of $\\Gamma_\\Lambda$ in $\\overline{Q}$ and $\\overline{P}$ may seem counterintuitive as information is transported into these regions via the horizontally moving fronts. % However, the scale resolution of the information lattice clarifies this effect: while a positive information packet propagates toward $\\overline{P}$, local information at $ \\ell \\approx 0 $ decreases, leading to a net negative change in total information. % The reduction of the information corresponds to an increase of the von Neumann entropy of $P$ with the rest of the system."}
{"input": "as $l_X \\ll l_{P/Q}$, $\\Gamma_{\\overline{X}}$ tends asymptotically to zero, which is the equilibrium value of the total information change in a small subsystem when the particle is delocalized over an infinitely large system. \\begin{figure}[t] \\centering \\includegraphics[width=\\columnwidth]{plots/il_TB_SingleSite_NonCritical_TotalInfoPlots.png} \\caption{Change of total information $\\Gamma_\\Lambda$ in Eq.~\\eqref{eq:totalInformation} for the chain decomposition in Fig.~\\ref{fig:IL-Partition}: $Q$ (left half, $l_Q = 101$ including $\\eta$), $X$ ($l_X = 10$) and $P$ ($l_P = 90$). % The dynamics is induced by a quench protocol in which the system is prepared in the ground state of the Hamiltonian~\\eqref{eq:HamTightBinding} with $\\mu_i = -20 \\tau_p$ and $\\mu_p = 20 \\tau_p$, and then evolved under the same Hamiltonian with $\\mu_f = \\mu_p = 20 \\tau_p$. % Horizontal dotted lines indicate expected asymptotic values of $\\Gamma_\\Lambda$ in each partition."}
{"input": "\\section{Quenching a single site in a critical tight-binding chain} \\label{sec:critical-states-quench} \\begin{figure*}[tb] \\centering \\includegraphics[width=\\textwidth]{plots/il_TB_SingleSite_Critical_TimeEvoPlots.png} \\caption{(a)-(c) Time evolution of local information after removing a potential barrier at the central site $\\eta = 100$, which initially separates two tight-binding chains of $100$ sites each in a critical state. % At $t=0^-$, the system is in the ground state of the Hamiltonian~\\eqref{eq:HamTightBinding} with $\\mu_i = 20 \\tau_p$ and $\\mu_p = 0$. % The post-quench dynamics is governed by the Hamiltonian~\\eqref{eq:HamTightBinding} with $\\mu_f = \\mu_p = 0$. % Sites with local information $1 \\pm 0.01$ bits are marked in green. % The inset in (a) zooms into the information lattice near $\\eta$ at $t=0^-$, while the inset in (b) shows the local-information change relative to the initial state [$\\Delta i_n^\\ell(t) = i_n^\\ell(t) - i_n^\\ell(0^-)$] at \\(t = 40 \\) with red (blue) indicating an increase (decrease) of local information. % Darkest red correspond to $i^\\ell_n \\geq 0.01 $; darkest blue to $i^\\ell_n \\leq -0.01 $. } \\label{fig:il_TB_SingleSite_Critical_SummaryPlots} \\end{figure*} Connecting two initially isolated quantum systems is a fundamental quench protocol to probe transport and information dynamics. % Such a quench drives the system out of equilibrium and triggers the transport of particles or quasiparticles~\\cite{piroli2017transport,alba2018entanglement}. % In this section, we investigate the local information flow generated by the sudden removal of a potential barrier at the center of a critical chain, effectively joining two previously disconnected critical halves~\\cite{eisler2007evolution,calabrese2007entanglement,eisler2008entanglement,stephan2011local,eisler2014area,bernard2012energy,bhaseen2015energy,fischer2020energy}."}
{"input": "at the central site $\\eta = \\lfloor (N-1)/2 \\rfloor$ set to $\\mu_i=20\\tau_p$, while in the rest of the chain $\\mu_p=0$~\\footnote{To avoid numerical degeneracies, we use a small but finite $\\mu_p = 10^{-5} \\tau_p$ in simulations, while referring to $\\mu_p = 0$ in the main text for conciseness.}. % For $t \\geq 0$, the system evolves under the homogeneous Hamiltonian $H_\\mathrm{TB}(\\mu_f)$ with $\\mu_f= \\mu_p =0$. % Thus, before the quench, the system is composed of two critical tight-binding chains separated by a finite potential barrier at the central site. % The setup is reminiscent of a ``break junction\" \\cite{moreland1985electron,agrait2003quantum}. % To analyze the initial ground state $| \\psi_0 \\rangle$ of $H_\\mathrm{TB}(\\mu_i)$, it is instructive to first consider the limit of an infinite chemical potential at $\\eta$, that is, $\\mu_i \\to \\infty$. % In this case, the two critical chains are completely decoupled and $| \\psi_0 \\rangle$ is the product of their two ground states and the empty state at the potential barrier site: $ |\\psi_0\\rangle = |\\psi_{0}^{\\mathrm{half}}\\rangle \\otimes |\\psi_{0,\\eta}\\rangle \\otimes |\\psi_{0}^{\\mathrm{half}}\\rangle$. % Here, $|\\psi_{0}^{\\mathrm{half}}\\rangle = (\\prod_{k \\geq k_\\mathrm{F}} c_k^\\dagger) |0\\rangle$ with $c_k^\\dagger = \\sqrt{\\frac{2}{L+1}}\\sum_{i=1}^{L} \\sin(ki)\\, c_i^\\dagger$, where $L$ is the length of the tight-binding chain, and $k_\\mathrm{F} = \\pi/2$ the Fermi wave number."}
{"input": "states---in particular, those with the lowest energies---occupied. % Therefore, the system is at half filling, critical, and has gapless excitations~\\footnote{Notice that, strictly speaking, gapless excitations are only present in the thermodynamic limit. % The cost of the excitations decays linearly with system size.}. % When the potential barrier is finite, the two tight-binding chains to the left and right of $\\eta$ are weakly coupled. % Then, the lowest energy single-particle state of the composed system is the symmetric linear combination of the lowest energy single-particle states of left and right chains; the second lowest energy single-particle state is the antisymmetric combination of the lowest energy single-particle states of left and right chains; and so on~\\cite{jelic2012thedouble-well}. % The ground state $| \\psi_0 \\rangle$ features half of the single-particle states of the composite system---those with the lowest energies---occupied. % Thus, for a large but finite potential barrier, in $\\ket{\\psi_0}$ there is a small nonzero probability of finding a particle at the barrier site $\\eta$. Fig.~\\ref{fig:il_TB_SingleSite_Critical_SummaryPlots}(a) illustrates the information lattice for the initial ground state $| \\psi_0 \\rangle$ of $H_\\mathrm{TB}(\\mu_i)$ with $\\mu_i=20 \\tau_p$ and $\\mu_p=0$. % To the left and right of $\\eta$, two critical regions appear in which nonzero local information is present at all the information lattice sites of the triangles that extend up to $\\ell = \\lfloor (N-1)/2 \\rfloor$. % Within these triangles, the average local information on scale $\\ell$ decays as a power law, $\\langle i^\\ell_n \\rangle \\propto \\ell^{-2}$, as implied by real-space scale invariance~\\cite{artiaco2024universal}."}
{"input": "Fig.~\\ref{fig:il_TB_SingleSite_Critical_SummaryPlots}(a) depicts a blow-up of the interface between the critical regions and the potential barrier, illustrating that nonzero local information at $\\ell=0$ is present exclusively at $\\eta$ while in the critical regions finite values of $i^\\ell_n$ are only present for $\\ell \\geq 1$. % The green dot at $\\eta$ signals that $i^0_\\eta \\approx 1$ bit. % This is a consequence of the barrier which approximately sets to zero the number of particles on $\\eta$, implying that the outcome of the single-site operator $c^\\dagger_\\eta c^{\\phantom{\\dagger}}_\\eta$ measuring the number of particles at $\\eta$ has almost a certain outcome in $\\ket{\\psi_0}$. % However, the finite height of the potential barrier causes a small leakage of the single-particle wave function centered at $\\eta$ on neighboring sites, as well as of the wave functions of the critical regions towards the barrier. % As a consequence, a small amount of local information ($i^\\ell_n \\lesssim 10^{-4}$) is present in the central region of the information lattice that extends in a light-cone shape from the site $\\eta$. % With the chosen color scale, this effect is only visible at short scales along the diagonal interfaces between the critical triangles and the central region of the information lattice. % The absence of local information at $\\ell=0$ within the critical regions is a consequence of them being approximately at half-filling. % The expectation value of the number operator at half-filling is $\\langle c_i^\\dagger c_i \\rangle = 1/2$, which implies that the eigenvalues of $\\rho_i$ are both $\\lambda_{1,2} = 1/2$."}
{"input": "physical sites centered at $\\eta=100$. % The barrier initially separates two critical chains.} \\label{fig:il_TB_MultipleSite_Critical_SummaryPlots} \\end{figure} Fig.~\\ref{fig:il_TB_SingleSite_Critical_TotalInfoPlots} shows the time dependence of $\\Gamma_\\Lambda$ in Eq.~\\eqref{eq:totalInformation} for $l_Q=101$ (where $Q$ includes the left half of the chain with $\\eta$ as the rightmost site), $l_X=10$ and $l_P=90$. % As in the previous quench scenario in Sec.~\\ref{sec:potential-well-quench}, we observe a flow of local information toward larger-scale partitions $\\overline{QX}$ and $\\overline{QXP}$, while information within the smaller-scale partitions $\\overline{Q}$, $\\overline{X}$ and $\\overline{P}$ decreases. % The total-information change $\\Gamma_\\Lambda$ in $\\overline{Q}$, $\\overline{X}$ and $\\overline{P}$ exhibits characteristic signatures observed in the time evolution of the von Neumann entropy following local bond-defect quenches in critical chains~\\cite{eisler2007evolution,calabrese2007entanglement,eisler2008entanglement,stephan2011local}. % Specifically, the von Neumann entropy of a subsystem has a characteristic logarithmic behavior over time, reaches a maximum that scales logarithmically with subsystem size, and for finite-size subsystems within a sufficiently large system eventually decays to its equilibrium value~\\cite{eisler2007evolution,calabrese2007entanglement}. % $\\Gamma_\\Lambda$ within $\\overline{Q}$ and $\\overline{P}$ follows a log-scale decrease, while $\\Gamma_{\\overline{X}}$ seems to converge to a negative finite value. % Throughout the time evolution, $\\Gamma_\\Lambda$ in $\\overline{Q}$, $\\overline{X}$ and $\\overline{P}$ exhibit step-like oscillations due to finite-size effects and the nonlinear dispersion~\\cite{eisler2007evolution,eisler2008entanglement}. % For the larger-scale partitions $\\overline{QX}$ and $\\overline{QXP}$, we observe similar trends in the change of total information. % $\\Gamma_{\\overline{QX}}$ follows a logarithmic increase, briefly stabilizes in a plateau regime, and then decays, approaching a finite value."}
{"input": "\\section{Connecting a topological Kitaev chain to a critical tight-binding chain} \\label{sec:majorana-teleportation-quench} \\begin{figure*}[tb] \\centering \\includegraphics[width=\\textwidth]{plots/il_MZM_teleportation_small_TimeEvoPlots.png} \\caption{(a)-(c) Time evolution of local information after coupling a topological sweet-spot Kitaev chain in the $Q$ region to a tight-binding chain in the $XP$ region. % At $t=0^-$, the system is in the product state of the ground states of the Hamiltonians~\\eqref{eq:HamKitaevChainAtTSS} and \\eqref{eq:HamQuenchProbeTB} with $\\tau_t = 0$, $\\tau = 20 \\tau_p$, and $\\tau_p = 1$. % The post-quench dynamics is governed by the Hamiltonians~\\eqref{eq:HamKitaevChainAtTSS} and \\eqref{eq:HamQuenchProbeTB} with $\\tau_t = \\tau_p$, $\\tau = 20 \\tau_p$, and $\\tau_p = 1$. % The length of the Kitaev chain is $l_Q=10$ and of the tight-binding chain is $l_X+l_P=110$. % Sites with local information $1 \\pm 0.01$ bits are marked in green. % The inset in (a) zooms into the information lattice in partition $\\overline{Q}$ at $t=0^-$, while the inset in (c) shows changes in local information relative to the initial state [$\\Delta i_n^\\ell(t) = i_n^\\ell(t) - i_n^\\ell(0^-)$] at $t = 81$ with red (blue) indicating an increase (decrease) of local information. % Darkest red correspond to $i^\\ell_n \\geq 0.01 $; darkest blue to $i^\\ell_n \\leq -0.01 $. } \\label{fig:il_MZM_teleportation_SummaryPlots} \\end{figure*} As a final quench protocol, we consider suddenly connecting a topological Kitaev chain to a critical tight-binding chain. % This setup is inspired by Ref.~\\cite{bauer2023quench}. % The Kitaev chain forms the $Q$ region in Fig.~\\ref{fig:IL-Partition}(c), extending over $l_Q$ physical sites. % It is governed by the sweet-spot Hamiltonian % \\begin{align} H^Q_{\\mathrm{KC}}&= \\frac{\\tau}{2}\\sum_{i=0}^{l_Q-2} \\left( c_i^\\dagger c^{\\phantom{\\dagger}}_{i+1} + c^{\\phantom{\\dagger}}_i c^{\\phantom{\\dagger}}_{i+1}+\\text{H.c."}
{"input": "of the ground state of the sweet-spot Kitaev chain $|\\psi_{0,\\mathrm{KC}}\\rangle$, which verifies $f |\\psi_{0,\\mathrm{KC}}\\rangle = d_i |\\psi_{0,\\mathrm{KC}}\\rangle = 0$ for all $d_i$, and the critical ground state of the tight-binding chain at half filling $|\\psi_{0,\\mathrm{TB}}\\rangle$ described in Sec.~\\ref{sec:critical-states-quench}. Fig.~\\ref{fig:il_MZM_teleportation_SummaryPlots}(a) depicts the information lattice for the initial state $|\\psi_{0}\\rangle$. % The topological ground state $|\\psi_{0,\\mathrm{KC}}\\rangle$ has 1 bit of local information at the top of partition $\\overline{Q}$, $i^{l_Q-1}_{(l_Q-1)/2} = 1$. % Such a $\\mathcal{O}(1)$ contribution to the information at system size scales is the hallmark of topological states~\\cite{artiaco2024universal}. % For $|\\psi_{0,\\mathrm{KC}}\\rangle$, $i^{l_Q - 1}_{(l_Q - 1)/2} = 1$ implies the existence of a two-outcome measurement of the correlations between the edge sites $0$ and $l_Q-1$ that can be predicted with certainty. % Specifically, the measurement amounts to determining the eigenvalue of $n_f = f^\\dagger f$---the occupation of the delocalized fermionic mode. % $|\\psi_{0,\\mathrm{KC}}\\rangle$ also has an extensive amount of local information at scale $\\ell=1$, where 1 bit is shared between every neighboring physical site, reflecting the ``dimer\" structure of the bulk of the sweet-spot Kitaev chain apparent from the last term in Eq.~\\eqref{eq:HamKitaevChainAtTSS}. % Local information in the critical ground state of the tight-binding chain presents the power-law decay $\\langle i^\\ell_n \\rangle \\propto \\ell^{-2}$ discussed in Sec.~\\ref{sec:critical-states-quench}. % Of particular interest for this quench is the information lattice interface $\\widetilde{QXP}$ illustrated in Fig."}
{"input": "$(n, \\ell) = (l_Q - 1/2, 1)$. % At $t=0^-$, there is no information in $\\widetilde{QXP}$, as shown in Fig.~\\ref{fig:il_MZM_teleportation_SummaryPlots}(a), indicating that, in the initial state, there are no correlations involving the rightmost physical site of the Kitaev chain $l_Q-1$ and the $XP$ region. For $t \\geq 0$, the system evolves under the Hamiltonians~\\eqref{eq:HamKitaevChainAtTSS} and \\eqref{eq:HamQuenchProbeTB} with intra-chain parameters $\\tau$ and $\\tau_p$ identical to the pre-quench configuration ($\\tau = 20 \\tau_p$) and the inter-chain tunneling strength set to $\\tau_t = \\tau_p$. % Figs.~\\ref{fig:il_MZM_teleportation_SummaryPlots}(b)–(c) show two snapshots of the time-evolved local information. % The most prominent feature is the clear separation between the local information flow originating from the top of $\\overline{Q}$ and the flow across the rest of the information lattice. % This decoupling implies that the topological information of the Kitaev chain remains well-defined and quantized to 1 bit. % This topological information flows towards larger scales along the diagonal left boundary of the information lattice, confined to a single site in the horizontal direction while exhibiting a long oscillatory tail diagonally. % Due to this decoupling, the sum of local information along the diagonal starting at $(n,\\ell)=(\\frac{l_Q-1}{2},l_Q-1)$ and ending at the top of the information lattice remains quantized to 1 bit. % Local information at the top of $\\overline{Q}$, $i^{l_Q-1}_{(l_Q-1)/2}(t)$, decays over time with a quadratic exponential form as illustrated by the black curve in Fig.~\\ref{fig:QX-Interface_TotalInfoPlots}, in agreement with analytical predictions for Majorana correlations and their lifetimes in similar setups~\\cite{goldstein2011decay,budich2012failure}."}
{"input": "packet that travels across the critical region on top of the local information reservoir toward the right boundary of the information lattice. % Over time, this right-traveling packet slowly spreads, gradually smearing information across the entire critical region. % Simultaneously, local information quickly flows from the partitions $\\overline{X}$ and $\\overline{XP}$ to the $\\widetilde{QXP}$ interface. % The presence of such a fast multiscale transfer is the hallmark of critical-state dynamics and resembles the behavior observed in Fig.~\\ref{fig:il_TB_MultipleSite_Critical_SummaryPlots}. % This transfer stops when the total information in $\\widetilde{QXP}$ converges to 1 bit, as illustrated by the blue curve in Fig.~\\ref{fig:QX-Interface_TotalInfoPlots}. % The asymptotic distribution of local information in $\\widetilde{QXP}$ has a power-law decay $i^\\ell_n \\propto \\ell^{-2}$, resembling the pre-quench distribution in the critical state of the tight-binding chain (see inset). \\begin{figure}[t] \\centering \\includegraphics[width=\\columnwidth]{plots/QX-Interface_TotalInfoPlots.png} \\caption{Time evolution of $i^{l_Q-1}_{(l_Q-1)/2}(t)$ (black), which is the information linked to the delocalized fermionic mode occupation $n_f = f^\\dagger f$, and $\\Sigma(t) = \\sum_{(n,\\ell) \\in \\widetilde{QXP}} i^\\ell_n(t)$ (blue), which is the total information in the $\\widetilde{QXP}$ interface, for the same quench protocol as in Fig.~\\ref{fig:il_MZM_teleportation_SummaryPlots}. % An exponential fit $i_{(l_Q-1)/2}^{l_Q-1}(t)\\propto\\exp(-\\alpha t^2)$ (green dashed, with $\\alpha \\approx 0.36$) matches analytical predictions from Majorana correlation functions\\protect~\\cite{goldstein2011decay,budich2012failure}. % Horizontal dotted lines mark expected asymptotic values. % The inset shows local information across $\\widetilde{QXP}$ at $t \\approx 172$ (black crosses), showing power-law decay $i^\\ell_{l_Q-1+\\ell/2} \\propto \\ell^{-2}$ (red), for the same quench protocol as in Fig.~\\ref{fig:il_MZM_teleportation_SummaryPlots} but in a larger system with $l_Q = 10$ and $l_X + l_P = 190$."}
{"input": "of the Hamiltonians~\\eqref{eq:HamKitaevChainAtTSS} and \\eqref{eq:HamQuenchProbeTB} with $\\tau_t = 0$, $\\tau = 20 \\tau_p$, and $\\tau_p = 1$, and then evolved under the same Hamiltonian with $\\tau_t = \\tau_p$, $\\tau = 20 \\tau_p$, and $\\tau_p = 1$. % Horizontal dotted lines indicate expected asymptotic values of $\\Gamma_\\Lambda$ in each partition. } \\label{fig:il_MZM_teleportation_TotalInfoPlots} \\end{figure} Let us start by considering the short-time behavior in Figs.~\\ref{fig:il_MZM_teleportation_SummaryPlots}, \\ref{fig:QX-Interface_TotalInfoPlots}, and \\ref{fig:il_MZM_teleportation_TotalInfoPlots}, that is, the rapid loss of 1 bit of information in $\\overline{Q}$ flowing to larger scales from the top of this partition, accompanied by the simultaneous loss of 1 bit in $\\overline{X}$ and the gain of 2 bits in $\\overline{QX}$. % This is particularly clear in Fig.~\\ref{fig:il_MZM_teleportation_TotalInfoPlots} illustrating the time dependence of $\\Gamma_\\Lambda$ in Eq.~\\eqref{eq:totalInformation} for $\\Lambda \\in \\{ \\overline{Q}, \\overline{X}, \\overline{P}, \\overline{QX}, \\overline{QXP} \\}$ and $l_Q=10$, $l_X=50$ and $l_P=140$. % We notice that the effective model in Eq.~\\eqref{eq:effHamQuenchProbeTB} is critical and exhibits particle-hole symmetry under the transformation $f \\leftrightarrow f^\\dagger$. % Thus, for $t \\gtrsim \\tau_{t/p}^{-1}$ the time-evolved state locally respects particle-hole symmetry~\\cite{calabrese2006time}, implying $\\braket{n_f} = 1/2$. % This means that the state of the effective single site in the $Q$ region becomes maximally mixed. % Since this state was initially pure, this implies that $\\overline{Q}$ loses 1 bit of information. % Moreover, since the full system is pure, the von Neumann entropy of the $XP$ region must necessarily equal that of the $Q$ region."}
{"input": "2 gained bits are equally distributed between $\\overline{QX}$ and $\\overline{QXP}$. To address these questions, we note that the energy of the time-evolved state exceeds that of the ground state of the Hamiltonian~\\eqref{eq:effHamQuenchProbeTB} by at most $2\\tau_t$---the difference between the largest and smallest eigenvalues of the first term in the Hamiltonian. % Since the system is an energy conductor, this excess in energy density initially located at the boundary between $Q$ and $XP$ dissipates over time and, in the long-time limit, the reduced density matrices of small subsystems converge to those of the ground state, assuming a sufficiently large system. % In the quench protocol of Fig.~\\ref{fig:il_MZM_teleportation_TotalInfoPlots}, where $l_X \\ll l_P$, the long-time reduced density matrices within the $QX$ region thus approach those of the ground state. % If the effective Hamiltonian~\\eqref{eq:effHamQuenchProbeTB} was nondegenerate, its critical ground state would feature a power-law decay of local information $i^\\ell_n \\propto \\ell^{-2}$, and for $l_X \\gg 1$ the full 2 bits of correlation between $Q$ and $XP$ would be localized within $\\overline{QX}$. % The same would then hold for the post-quench time-evolved state at late times. % However, $H_{\\mathrm{eff}}^{QXP}$ is two-fold degenerate, which complicates this picture. % Since $H_{\\mathrm{eff}}^{QXP}$ commutes with $f - f^\\dagger$, any eigenstate $\\ket{\\phi}$ of $H_{\\mathrm{eff}}^{QXP}$ with definite fermion parity has a degenerate partner $(f - f^\\dagger)\\ket{\\phi}$ with opposite parity. % In the presence of such a two-fold degeneracy, the ground state may host 1 nonlocal bit of information~\\cite{artiaco2024efficient}."}
{"input": "previously discussed phenomena result solely from the validity of the effective Hamiltonian~\\eqref{eq:effHamQuenchProbeTB}: % (i)~The flow of 1 bit of information associated with topological edge correlations towards larger scales along the diagonal left boundary of the information lattice is decoupled from the local information flow within the critical region, as shown in Fig.~\\ref{fig:il_MZM_teleportation_SummaryPlots}(a). % (ii) At short times, partitions $\\overline{Q}$ and $\\overline{X}$ lose 1 bit of information each, while $\\overline{QX}$ gains 2 bits of information, as shown in Fig.~\\ref{fig:il_MZM_teleportation_TotalInfoPlots}. % (iii) At long times, the bit associated with topological edge correlations transfers to $\\overline{QXP}$, indicating a net flow of 1 bit from $\\overline{QX}$ to $\\overline{QXP}$. % Simultaneously, half a bit flows from $\\overline{X}$ to $\\overline{P}$, as $f(t) + f^\\dagger(t)$ becomes almost entirely supported within the $P$ region. The effective Hamiltonian description holds under three main conditions. % First, $l_Q \\gg \\lambda$, where $\\lambda$ is the pre-quench correlation decay length in $Q$ defined in Ref.~\\cite{artiaco2024universal}. % Second, the $Q$ region is initially in the topological phase. % Third, the energy scale $\\tau$ of the $Q$ region is much larger than that of the rest of the system. % These conditions apply not only to the sweet-spot Hamiltonian~\\eqref{eq:HamKitaevChainAtTSS}, but also to a broad class of models---including those with additional superconducting, chemical potential, or interaction terms of order $\\tau$~\\cite{Fendley2016}. % In such cases, $\\lambda$ increases, and the localized bit at the top of $\\overline{Q}$ in Fig.~\\ref{fig:il_MZM_teleportation_SummaryPlots} broadens over a region of width $\\approx \\lambda$. % This is illustrated in Fig."}
{"input": "chemical potential to the sweet-spot Hamiltonian: % \\begin{align} H_{\\mathrm{KC}}^Q= \\sum_{i=0}^{l_Q-1} \\mu \\, c_i^\\dagger c^{\\phantom{\\dagger}}_i + \\frac{\\tau}{2}\\sum_{i=0}^{l_Q-2} \\left( c_i^\\dagger c^{\\phantom{\\dagger}}_{i+1} + c^{\\phantom{\\dagger}}_i c^{\\phantom{\\dagger}}_{i+1}+\\text{H.c.} \\right), \\label{eq:HamKitaevFiniteMu} \\end{align} % with $\\mu=0.6\\tau$. \\begin{figure}[tbp] \\centering \\includegraphics[width=\\columnwidth]{plots/IL_MZM_Deviation_TSS.png} \\caption{(a)-(b) Time evolution of local information after quenching the tunnel coupling between a topological Kitaev chain with finite chemical potential Eq.~\\eqref{eq:HamKitaevFiniteMu} and a critical tight-binding chain. The parameters of the Kitaev chain are chosen away from the topological sweet spot: $\\mu/\\tau=0.6$ and $\\tau_p/\\tau=0.01$ ($l_Q=30,l_X+l_P=70$).} \\label{fig:IL_MZM_Deviation_TSS} \\end{figure} The correction terms in the effective Hamiltonian~\\eqref{eq:effHamQuenchProbeTB} of order $\\mathcal{O}(\\tau_{t/p}^2/\\tau)$ become relevant when the energy scale $\\tau$ of the $Q$ region is no longer much larger than that of the rest of the system. % In such cases, the effective Hamiltonian may no longer provide an accurate description, and the main features of the local information flow discussed above can qualitatively change. % However, at the sweet spot, that is, as long as the Kitaev chain is governed by the Hamiltonian~\\eqref{eq:HamKitaevChainAtTSS}, the validity of the effective description~\\eqref{eq:effHamQuenchProbeTB} does not depend on the condition $\\tau \\gg \\tau_{t/p}$. % The only difference in that regime is that the site at $l_Q - 2$ becomes correlated with the $XP$ region. % This is because the sweet-spot Hamiltonian possesses a set of local integrals of motion, $\\{d_i^\\dagger d_i\\}_{i=0}^{l_Q - 2}$, all of which remain conserved after the quench except for the one near the interface ($i = l_Q - 2$)."}
{"input": "features of quench protocols within critical chains, which are due to the presence of local information in the initial state not only at short but also at larger scales. % Finally, we analyzed a quench protocol in which a topological Kitaev chain in its ground state is suddenly coupled to a critical tight-binding chain in its ground state. % The local decomposition of the total information obtained through the information lattice allowed us to identify the specific features of the local information flow associated with Majorana edge modes. % Fractional signatures in the von Neumann entropy due to the presence of localized Majorana edge mode have been previously reported~\\cite{sela2019detecting,bauer2023quench}. % However, exploiting the information lattice, we distinguished two distinct origins thereof. % One arises from an interface effect between the topological and critical chain, localizing a fractional amount of information. % The other stems purely from a non-equilibrium process, driven by a propagating Majorana mode induced by the quench. % Using the microscopic flow of information provided by the information lattice, we analytically derived the origin and properties of both mechanisms. While we focused in this work on local quenches in noninteracting fermionic systems, our framework is entirely general and applies to global quenches, interacting systems, and systems subject to dephasing and dissipation. % In interacting systems, information tends to propagate from short to large scales, as dictated by the second law of thermodynamics. % This principle has been exploited in Refs.~\\cite{klein2022time,artiaco2024efficient,harkins2025nanoscale} to develop efficient time-evolution schemes for many-body dynamics."}
{"input": "eight factors are selected for experimentation; their analysis revealed that some linear, quadratic and interaction effects of the first, second, and sixth factors are important effects. From the experimental design point of view, we can split the eight factors into two groups, the first, second and sixth factors as one group and the rest as the other group, a design with better projection and space-filling properties for the first group would be more desirable, comparing to space-filling designs without this feature, as it allows more accurate estimation of the main and interaction effects of factors in the first group. Another application where the proposed designs with a group structure might be useful lies in the use of blocked (or group) additive kernels. Such kernels have been used in Gaussian processes based on functional analysis of variance decomposition \\citep{Muehlenstaedt-etal-2012}, the non-parametric regression \\citep{Pan-Zhu-2017}, and Bayesian optimization problems \\citep{Gardner-etal-2017}. An experimental design that takes into account the feature of the kernels would enable more efficient estimation of the parameters in the kernels. To address these practical needs, we propose using space-filling designs with a group structure. In this article, we focus on the projection property onto lower dimensions. It can be accomplished with an orthogonal array-based Latin hypercube \\citep{Owen-1992, Tang-1993} derived from an orthogonal array of high strength. A good low-dimensional projection property is an essential characteristic of screening designs which are indispensable for more realistic, complex computer experiments as greater numbers of input variables are employed."}
{"input": "\\cite{Wang-etal-2021}, higher strength guarantees better space-filling properties. With the group structure, we introduce {\\em grouped orthogonal arrays}, so that grouped orthogonal array-based Latin hypercubes can be constructed. The proposed grouped orthogonal arrays are of strength two but the factors can be divided into disjoint groups with higher strength or minimum aberration (reasons for this property will be given later). The newly introduced designs extend the concept of designs of variable resolution proposed by \\cite{Lin-2012} or variable strength orthogonal arrays by \\cite{Raaphorst-Moura-Stevens-2014} in which groups have higher strength than the whole array. \\cite{Lin-2012} and \\cite{Lekivetz-Lin-2016} provided several constructions for designs of variable resolution but the focus is on two-level designs. The variable strength orthogonal arrays obtained by \\cite{Raaphorst-Moura-Stevens-2014} only have groups of three factors and their run sizes are limited to $s^3$ for a prime power $s$. \\cite{Zhang-Pang-Li-2023} constructed variable strength orthogonal arrays with strength two containing strength greater than two by Galois field and some variable strength orthogonal arrays with strength $l \\geq 2$ containing strength greater than $l$ by Fan-construction. In addition to the drawback that the designs constructed have only one group with larger strength, the resulting designs have very restrictive run sizes $s^t$ for a prime power $s$ and an integer $t \\geq 4$. In this paper, we develop several construction methods for the proposed grouped orthogonal arrays."}
{"input": "\\section{Notations and background} \\label{sec:notation} In this section, we first review a few important concepts including Galois fields, orthogonal arrays and projective geometries. We refer to the monographs \\cite{Dembowski-1968}, \\cite{Lidl-Niederreiter-1986} and \\cite{Hedayat-Sloane-Stufken-1999} for the in-depth explanations. Armed with the background knowledge and notations, we then introduce the notion of grouped orthogonal arrays. Throughout we focus on the case that all factors share the same number of levels and use $s$ to denote the number of levels. Suppose that $s$ is a prime power and $k$ is a positive integer. A Galois field of order $s^k$ is a finite field with $s^k$ elements, and is denoted by $GF(s^k) = \\{\\omega_0, \\omega_1, \\ldots, \\omega_{s^k-1}\\}$ with $\\omega_0=0$ in this paper. In particular, if $s$ is a prime number and $k=1$, then the $s$ elements are denoted by $GF(s) = \\{0,1,\\ldots, s-1\\}$. All the nonzero elements of $GF(s^k)$ can be expressed as powers of an element $\\beta \\in GF(s^k)$; that is, $GF(s^k) \\setminus \\{0\\} = \\{\\beta^0, \\beta^1, \\ldots, \\beta^{s^k-2}\\}$. The element $\\beta$ is called a primitive element of $GF(s^k)$. The primitive polynomial $h(x)$ corresponding to $\\beta$ is a polynomial of degree $k$ with coefficients from the subfield $GF(s)$ such that $h(\\beta)=0$. Using addition and multiplication operations of polynomials modulo $h(\\beta)$, any nonzero element $\\beta^i$ of $GF(s^k)$ for $i=0,1\\ldots, s^k-2$ can be written as a linear combination of $1, \\beta, \\beta^2, \\ldots, \\beta^{k-1}$ with coefficients $a_{i,j} \\in GF(s)$ for $j=1,\\ldots, k-1$; that is, $$ \\beta^i = a_{i,0} + a_{i,1} \\beta + \\cdots + a_{i,k-1} \\beta^{k-1}."}
{"input": "Thus the nonzero element $\\beta^i$ can also be represented by a vector $(a_{i,0}, a_{i,1}, \\\\ \\ldots, a_{i,k-1})^T \\in GF(s)^k$ for $i=0,1\\ldots, s^k-2$. In this paper, we call $(a_{i,0}, a_{i,1}, \\ldots, a_{i,k-1})^T$ the vector format and $\\beta^i$ the power format for a nonzero element of $GF(s^k)$. %The corresponding primitive polynomial is $h(x) = b_k x^k + \\cdots + b_1 x + b_0$. An $N \\times n$ matrix with entries from $GF(s)$ is called an orthogonal array of strength $t$, and denoted by OA$(N,n,s,t)$, if any of its $N \\times t$ submatrix contains all possible level combinations equally often. An OA$(N, n, s, t)$ is said to be regular or linear if its $N$ rows form a linear space over $GF(s)$. A matrix with its rows formed by all vectors in a basis of this linear space is called a generator matrix for the OA$(N, n, s, t)$. \\begin{example}\\label{example-0} The matrix $D$ displayed in the transpose form in \\eqref{eqn:linear-OA} is an OA$(8, 4, 2, 3)$. All rows of $D$ can be generated by linear combinations of rows of $G$ in \\eqref{eqn:linear-OA}. Thus $G$ is a generator matrix for $D$."}
{"input": "the property that any two of them are linearly independent over $GF(s)$. Hence, there are a total of $(s^k-1)/(s-1)$ points in PG$(k-1,s)$. Given a primitive element $\\beta \\in GF(s^k)$, all these points are also given by the vector formats of $\\beta^i$ for $i=0, \\ldots, (s^k-1)/(s-1)-1$. A subset of PG$(k-1,s)$ is called a cap if any three points are linearly independent. Clearly, a matrix using points of a cap as its columns can generate an array with $A_j=0$ for $j=1,2,3$, i.e., an orthogonal array of strength three. An OA$(N,n,s,t)$ is said to be nonregular if its rows do not form a linear space over $GF(s)$. The run size of a nonregular orthogonal array can be very flexible as it does not need to be a prime power. Nonregular orthogonal arrays can be constructed from difference schemes \\citep{Hedayat-Sloane-Stufken-1999}. An $r \\times c$ matrix with entries from $GF(s)$ is called a difference scheme if the difference of any two columns contains all elements of $GF(s)$ equally often; we denote this matrix by DS$(r,c,s)$. We shall revisit this concept in Section \\ref{subsec:GOA-recursive}. In this paper, we study orthogonal arrays where the factors are divided into groups and higher strength, or more generally, minimum aberration, is achieved by factors in the same groups. We call these arrays the {\\em grouped orthogonal arrays} (GOAs)."}
{"input": "\\section{GOAs with strength-3 and near strength-3 groups} \\label{sec:GOA-strength3} This section centers on construction methods for grouped orthogonal arrays with strength-three or near strength-three groups. As the whole array of these designs is of strength 2, the subdesign resulting from projecting onto factors within the same group has better space-filling properties. In addition, such designs allow the main effects to be estimated without being biased by two-factor interactions in the same group. The merits of these designs will be demonstrated in Section \\ref{sec:simulation}. %This property is especially desirable if we believe in the assumption that main effects are sparse and those from the same group are more likely to be active whenever two-factor interactions arise. The construction methods presented in this section are explicit and require little use of computer search. Specifically, Section \\ref{subsec:GOA-s3s4} gives two construction methods based on caps to obtain designs of $s^3$ and $s^4$ runs for any prime power $s$. Next, a recursive construction method is introduced in Section \\ref{subsec:GOA-recursive} to generate designs with larger and flexible run sizes. \\subsection{Designs of $s^3$ and $s^4$ runs} \\label{subsec:GOA-s3s4} We first present two construction methods for grouped orthogonal arrays of $s^3$ and $s^4$ runs in which the groups have strength 3, where $s$ is a prime power."}
{"input": "0\\\\ \\omega_0^2 & \\omega_1^2 & \\omega_2^2 & \\cdots & \\omega_{s-1}^2 & 1 \\end{bmatrix}. $$ It can be directly verified that the columns of $G_0$ form a cap in PG$(2, s)$ because any three of them are linearly independent. Thus $G_0$ generates an OA$(s^3, s+1, s, 3)$. It has been shown that this orthogonal array has the largest number of factors when $s$ is odd, see Corollary 3.9 of \\cite{Hedayat-Sloane-Stufken-1999}. Now let $$ G_i = \\begin{bmatrix} 1 & 1 & 1 & \\cdots & 1 \\\\ \\omega_0 & \\omega_1 & \\omega_2 & \\cdots & \\omega_{s-1} \\\\ \\omega_i+\\omega_0^2 & \\omega_i+\\omega_1^2 & \\omega_i+\\omega_2^2 & \\cdots & \\omega_i+\\omega_{s-1}^2 \\end{bmatrix} $$ for $i = 1, \\ldots, s-1$. Then we have the following result. \\begin{theorem} \\label{thm:s3-runs} The generator matrix $G=(G_0, G_1, \\ldots, G_{s-1})$ generates a GOA$(s^3, (s+1, s, \\ldots, s), 3 \\times s, s, 2)$. \\end{theorem} As an illustration of Theorem \\ref{thm:s3-runs}, Example \\ref{ex:s3_125} below constructs a five-level GOA with 125 runs and 26 factors that include four groups of 5 factors and one group of 6 factors, and each group is of strength three. \\begin{table}[h] \\caption{The generator matrix $G$ in Example \\ref{ex:s3_125}.\\label{tab:s3_125}} \\centering \\begin{tabular}{ccccc} \\hline $G_0$ & $G_1$ & $G_2$ & $G_3$ & $G_4$\\\\ \\hline {111110} & {11111} & {11111} & {11111} & {11111}\\\\ {012340} & {01234} & {01234} & {01234} & {01234}\\\\ {014411} & {12002} & {23113} & {34224} & {40330}\\\\ \\hline \\end{tabular} \\end{table} \\begin{example} \\label{ex:s3_125} Consider the case $s=5$."}
{"input": "standard result in the literature in which $B \\oplus A$ rather than $A \\oplus B$ is more often used, see Lemma 6.27 of \\cite{Hedayat-Sloane-Stufken-1999}. Both $A \\oplus B$ and $B \\oplus A$ are OA$(Nr, cn, s, 2)$; as will be seen later, here we use $D=A \\oplus B$ so that the group structure of the factors of $D$ becomes immediately clear without re-arranging the columns of $D$. Given an orthogonal array $D = (d_1, \\ldots, d_n)$, we describe its three-column combinatorial orthogonality by the proportion of $(d_i, d_j, d_l)$'s ($1\\leq i < j < l \\leq n$) with strength 3 and denote it by $p(D)$. This measure was also used in \\cite{He-Lin-Sun-2022}. Clearly, $D$ has strength 3 if and only if $p(D) = 1$. The next lemma shows that if the array $B$ in Lemma \\ref{lem:DS-OA2} has strength 3, then the resulting design $D$ has a high $p(D)$ value. \\begin{lemma} \\label{lem:DS-OA3} Suppose that $A$ is a DS$(r,c,s)$ and $B$ is an OA$(N, n, s, 3)$. Then $D = A \\oplus B$ is an OA$(Nr, cn, s, 2)$ with \\begin{equation} \\label{eqn:pD} p(D) \\geq 1 - \\frac{(c-1)(c-2)}{(cn-1)(cn-2)}. \\end{equation} In particular, the equality holds as long as $r$ is not a multiple of $s^2$. \\end{lemma} \\begin{remark} A special case of Lemma \\ref{lem:DS-OA3} is related to a result of \\cite{He-Lin-Sun-2022} who studied the construction of orthogonal arrays of near strength 3."}
{"input": "\\section{GOAs with minimum aberration groups} \\label{sec:GOA-MA} As a refinement of the criterion of strength, the minimum aberration criterion can be used to select an orthogonal array with better projection properties \\citep{Chen-1998, Tang-2001, Wang-etal-2021}. The aim of this section is to construct regular grouped orthogonal arrays in which the groups have minimum aberration among all regular orthogonal arrays. Such designs are useful for situations where GOAs with strength-3 and near strength-3 groups do not exist, and where there are many GOAs with strength-3 groups and it is desirable to use one with better within-group projection properties. Two construction methods will be presented in Sections \\ref{subsec:consecutive-powers} and \\ref{subsec:algorithm}, respectively. %In some situations, there may be so many factors in each group that GOAs of strength 3 and near strength 3 do not exist. %While in other situations, it is possible to select a design with better within-group projection properties among GOAs of strength 3. %One approach to deal with these practical situations is to employ a GOA with minimum aberration groups, %which is also the aim of this section. %An orthogonal array with minimum aberration property is widely deemed as a design with superior projection properties. %Hence, the aim of this section is to construct GOAs with minimum aberration groups. We consider regular designs throughout this section. Suppose that $\\beta$ is a primitive element of $GF(s^k)$. As mentioned in Section \\ref{sec:notation}, the $(s^k-1)/(s-1)$ columns of PG$(k-1, s)$ can be expressed as \\begin{equation} \\label{eqn:powers-order} \\beta^0, \\beta^1, \\beta^2, \\ldots, \\beta^{(s^{k}-1)/(s-1)-1}."}
{"input": "plays an instrumental role in this section. \\begin{lemma} \\label{lem:isomorphism} For any positive integers $m$ and $j$ and any sequence of increasing integers $ i_1 < \\cdots < i_m$, the two designs generated by $(\\beta^{i_1}, \\ldots, \\beta^{i_m})$ and $(\\beta^{i_1+j}, \\ldots, \\beta^{i_m+j})$ have the same wordlength pattern. \\end{lemma} \\subsection{Forming groups by selecting consecutive powers of $\\beta$} \\label{subsec:consecutive-powers} We form groups of a GOA by taking $m$ consecutive elements in \\eqref{eqn:powers-order}; that is, the first group is given by $(\\beta^0, \\beta^1, \\ldots, \\beta^{m-1})$, the second group is given by $(\\beta^m, \\beta^{m+1}, \\ldots, \\beta^{2m-1})$ and so on. According to Lemma \\ref{lem:isomorphism}, all groups have the same wordlength pattern. The structure of each group is established by Theorem \\ref{thm:consecutive-powers}. \\begin{theorem} \\label{thm:consecutive-powers} For a primitive element $\\beta$ of $GF(s^k)$, let $D_0=(d_1, d_2, \\ldots, d_m)$ be generated by any $m$ consecutive columns in \\eqref{eqn:powers-order}. Then we have that (i) if $m \\leq k$, then $D_0$ is an OA$(s^k, m, s, m)$; and (ii) if $m > k$, then $D_0$ is a fractional factorial design, with the defining relation given by $$ I = d_1^{b_0} d_2^{b_1}\\cdots d_{k+1}^{b_k} = d_2^{b_0} d_3^{b_1}\\cdots d_{k+2}^{b_k} = \\cdots\\cdots = d_{m-k}^{b_0} d_{m-k+1}^{b_1}\\cdots d_{m}^{b_k}, $$ where, for example, $I = d_1^{b_0} d_2^{b_1}\\cdots d_{k+1}^{b_k}$ represents the defining word $b_0 d_1 + b_1 d_2 + \\cdots b_k d_{k+1} = 0$, and $b_0, b_1, \\ldots, b_k \\in GF(s)$ are the coefficients of the primitive polynomial $h(x) = b_k x^k + \\cdots + b_1 x + b_0$ corresponding to $\\beta$."}
{"input": "\\ref{thm:consecutive-powers} provides a construction method of GOA$(s^k, m \\times \\lfloor (s^k-1)/(ms-m)\\rfloor, m \\times \\lfloor (s^k-1)/(ms-m)\\rfloor, s, 2)$, where $\\lfloor \\cdot \\rfloor$ denotes the floor function, such that each group contains $s^k/s^m$ replicate(s) of the full factorial. %We remark that Steinberg and Lin (2006) and Pang, Liu and Lin (2009) made use of the special case $m=k$ to construct orthogonal Latin hypercubes. Now we focus on the case $m > k$. Part (ii) of Theorem \\ref{thm:consecutive-powers} shows that the primitive polynomial $h(x) = b_k x^k + \\cdots + b_1 x + b_0$ of $\\beta$ determines the defining words of $D_0$. The next result describes $h(x)$ such that $D_0$ has minimum aberration for $m=k+1$ and $m=k+2$. Some notations are needed to capture certain characteristics of $h(x)$. Define $b_{-1}=b_{k+1}=0$ and let $f_i=|\\{j: b_{j-1} \\neq 0, b_j/b_{j-1} = \\omega_i, j=0, \\ldots, k+1\\}|$ for $i=0,1,\\ldots, s-1$, $f_{s}=|\\{j: b_{j-1} = 0, b_j \\neq 0, j=0, \\ldots, k+1\\}|$ and $f_*=|\\{j: b_{j-1} = b_j = 0, j=0, \\ldots, k+1\\}|$, where $|A|$ denotes the cardinality of a set $A$. \\begin{proposition} \\label{prop:MA-pp} We have the following results on $D_0$ and $h(x) = b_k x^k + \\cdots + b_1 x + b_0$. \\begin{itemize} \\item[(i)] Suppose $m=k+1$. Then a minimum aberration design $D_0$ maximizes the number of nonzero elements among $b_0, b_1, \\ldots, b_{k-1}$. In particular, if $b_0, b_1, \\ldots, b_{k-1}$ are all nonzero, then $D_0$ has minimum aberration among all regular OA$(s^k, m, s, 2)$'s. \\item[(ii)] Suppose $m=k+2$."}
{"input": "Supplementary Material, which give the truncated wordlength pattern $(A_3, A_4, A_5, A_6)$ of the groups of all the obtained GOA$(s^k, m \\times \\lfloor (s^k-1)/(ms-m)\\rfloor, t \\times \\lfloor (s^k-1)/(ms-m)\\rfloor, s, 2)$'s ($m \\geq t \\geq 2$). In addition, a GOA is marked with an asterisk if its groups have minimum aberration among all regular designs, which is verified either by Proposition \\ref{prop:MA-pp} or by comparison with existing minimum aberration designs in the literature \\citep{Chen-Wu-1991, Xu-2005}. For each GOA in these tables, we give the coefficients $(b_k, b_{k-1}, \\ldots, b_1, b_0)$ of one possible $h(x)$ which can generate the design. \\subsection{Forming groups by an algorithm} \\label{subsec:algorithm} The groups of a GOA obtained in Section \\ref{subsec:consecutive-powers} may not have minimum aberration among all regular designs. For example, the groups of the GOA$(32, 6 \\times 5, 4 \\times 5, 2, 2)$ presented in Table S1 have a defining word of length 5, while the minimum aberration regular OA$(32, 6, 2, 5)$ only has one defining word of length 6. In this subsection, we use an algorithm to obtain GOAs whose groups are guaranteed to have minimum aberration among all regular designs. Given a primitive polynomial, we can express the columns of a generator matrix as powers of a primitive element. The idea of our algorithm is to use an existing regular minimum aberration design as the first group, then transform the columns of its generator matrix into power formats and apply Lemma \\ref{lem:isomorphism} to find the remaining groups."}
{"input": "the set of power formats for a minimum aberration design is not unique. On one hand, there exist many primitive polynomials for a Galois field. On the other hand, a minimum aberration design can be generated from various generator matrices. To see this, consider the following two generator matrices for a minimum aberration OA$(16, 5, 2, 4)$: $$ G_1 = \\begin{bmatrix} 1 & 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 & 1 \\\\ \\end{bmatrix} \\quad \\mbox{and} \\quad G_2 = \\begin{bmatrix} 1 & 1 & 1 & 0 & 1 \\\\ 0 & 0 & 0 & 1 & 1 \\\\ 1 & 0 & 1 & 1 & 1 \\\\ 1 & 1 & 0 & 0 & 0 \\\\ \\end{bmatrix}. $$ If we label the five columns by $r_1, r_2, r_3, r_4$ and $r_5$, then $r_5$ can be seen as generated from the other four independent columns according to $r_5=r_1+r_2+r_3+r_4$. Hence the generator matrices $G_1$ and $G_2$ generate two designs with the identical wordlength pattern, but they are distinct because two different sets of independent columns $(r_1, r_2, r_3, r_4)$ are used. Clearly, the power formats for the columns of $G_1$ and $G_2$ are different in general."}
{"input": "where $H$ is a $k\\times k$ non-singular matrix over $GF(s)$. Based on the discussion above, we describe our full algorithm as follows. \\begin{enumerate} \\item Obtain a generator matrix $G$ for a regular OA$(s^k, m, s, t)$ with minimum aberration. \\item Choose a primitive polynomial $h(x)$ for $GF(s^k)$ and generate a random non-singular $k \\times k$ matrix $H$ over $GF(s)$. Write the columns of $G_0 \\leftarrow HG$ as powers of a primitive element $\\beta$ associated with $h(x)$, say $(\\beta^{i_1}, \\beta^{i_2}, \\cdots, \\beta^{i_m})$. Set $v \\leftarrow (s^k-1)/(s-1)$ and $\\mathcal{G} \\leftarrow (\\beta^{i_1 \\bmod v}, \\beta^{i_2\\bmod v}, \\cdots, \\beta^{i_m\\bmod v})$, where $\\bmod$ is the modulo operator. \\item Set $g \\leftarrow 1$. For $j=1,\\ldots, v-1$: \\begin{enumerate} \\item Let $G_j \\leftarrow (\\beta^{(i_1+j)\\bmod v}, \\beta^{(i_2+j)\\bmod v}, \\cdots, \\beta^{(i_m+j)\\bmod v})$. \\item If $\\mathcal{G} \\cap G_j = \\emptyset$, then let $\\mathcal{G} \\leftarrow \\mathcal{G} \\cup G_j$ and $g \\leftarrow g + 1$. \\end{enumerate} \\item Repeat Steps 2 and 3 for a large number $R$ of times and output the largest $g$ as well as the corresponding GOA$(s^k, m \\times g, t \\times g, s, 2)$. % represented by $\\mathcal{G}$. \\end{enumerate} We set $R=100,000$ and then apply our algorithm to regular minimum aberration OA$(2^k, m, 2, t)$'s for $k=4,5,6$ and $m < 2^{k-1}$ \\citep{Chen-Sun-Wu-1993}, OA$(128, m, 2, t)$'s for $m \\leq 20$ \\citep{Block-Mee-2005} and OA$(3^k, m, 3, t)$'s for $k=4,5$ and $m \\leq 20$ \\citep{Xu-2005}. The resulting GOA$(s^k, m \\times g, t \\times g, s, 2)$'s are presented in Tables S3, S4, S5 and S6 in the Supplementary Material."}
{"input": "are omitted, either because better designs have been reported in Tables S1 and S2 or because the algorithm cannot find designs with $g \\geq 2$. In addition, a GOA is marked by a dagger if the maximum number of groups is attained, i.e., $g = \\lfloor (s^k-1)/(ms-m)\\rfloor$. We conclude this section with some remarks on the two methods of generating GOAs with minimum aberration groups. Compared with the method of selecting consecutive powers in Section \\ref{subsec:consecutive-powers}, the algorithm proposed in Section \\ref{subsec:algorithm} can generate GOAs whose groups have less aberration. On the other hand, the algorithm relies on the existing regular minimum aberration designs and may not yield the maximum number of groups due to the computational burden. For example, in the algorithmic search we can only find a GOA$(243, 6 \\times 18, 5 \\times 18, 3, 2)$, while Table S1 gives a GOA$(243, 6 \\times 20, 5 \\times 20, 3, 2)$ with the same wordlength pattern by choosing an appropriate primitive polynomial."}
{"input": "$\\beta_{j,q}^{(k)}$ despite the non-negligible two-factor interactions $\\beta_{j_1j_2,ll}^{(k)}$, $\\beta_{j_1j_2,lq}^{(k)}$, $\\beta_{j_1j_2,ql}^{(k)}$ and $\\beta_{j_1j_2,qq}^{(k)}$. In our simulation study, we first generate these factorial effects and the intercept $\\beta_0$ from normal distributions: $\\beta_0, \\beta_{j,l}^{(k)}, \\beta_{j,q}^{(k)} \\sim \\mathcal{N}(0, 10^2)$ and $\\beta_{j_1j_2,ll}^{(k)},\\beta_{j_1j_2,lq}^{(k)}, \\beta_{j_1j_2,ql}^{(k)},\\beta_{j_1j_2,qq}^{(k)} \\sim \\mathcal{N}(0, \\sigma^2)$. Then given a design $D$ of $N$ runs for the $n$ factors, let $X$ be the model matrix corresponding to intercept and the main effects and $Y$ be the vector of $N$ independent responses generated from model \\eqref{eqn:true_model_bias}, where, for simplicity, the random errors $\\epsilon$ are independently generated from $\\mathcal{N}(0, 1)$. By ignoring the interactions, one can estimate the vector $\\beta$ of main effects (including all $\\beta_{j,l}^{(k)}$'s and all $\\beta_{j,q}^{(k)}$'s) by removing the first entry of $(X^T X)^{-1} X^T Y$. Denote this estimate by $\\hat{\\beta}$. Then the error in $\\hat{\\beta}$ can be evaluated by the mean squared error $e(\\hat{\\beta}) = \\{\\|\\hat{\\beta} - \\beta\\|_2^2/(2n)\\}^{1/2}$ and a design is desirable if $e(\\hat{\\beta})$ is minimized on average. Two specific cases are investigated to illustrate the performance of GOAs. In case (i), we set $N=27$, $g=3$, $m_1=4$ and $m_2=m_3=3$, for which case we take the design $D$ as a GOA$(27, (4, 3, 3), 3\\times 3, 3,2)$ generated by Theorem \\ref{thm:s3-runs}. In case (ii), we set $N=81$, $g=3$ and $m_1=m_2=m_3=4$, for which case we take the design $D$ as a GOA$(81, 4 \\times 3, 3\\times 3, 3,2)$ generated by Lemma \\ref{lem:DS-OA3}. In both cases, we also take $D$ as obtained by randomly permuting columns of the regular minimum aberration designs \\citep{Xu-2005} for comparison."}
{"input": "main effects estimation, as can be seen from the results of Table \\ref{tab:bias-rmse}. This is the intuition behind the theoretical arguments of \\cite{Lin-2012}. As these arguments will be more tedious and complicated than those for two-level designs, we choose to leave it for future work. %For general GOAs, although a main effect in one group might be biased by a two-factor interaction from another group, one can use similar arguments to those in \\cite{Lin-2012} to show the GOAs tend to lead to less bias in main effects estimation. \\begin{remark} \\label{remark:group-effect-sparsity} The GOAs are also useful for the scenario of ``group-effect sparsity'', which means that among all groups of factors, perhaps only a few groups are active and really affect the response. More simulation studies are presented in the Supplementary Material to show the advantage of GOAs in this scenario. %As pointed out by one referee, for saturated case, i.e. $N=1+n(s-1)$, the average $e(\\hat{\\beta})$ is the same whether or not GOAs are used. \\end{remark} \\subsection{Predictions in computer experiments} \\label{subsec:prediction} In this subsection, we show that GOAs can be used to provide more accurate predictions in computer experiments. We adopt the same notation as in Section \\ref{subsec:bias} and assume the true underlying model can be written as \\begin{equation} \\label{eqn:true_model_CE} y = \\beta_0 + \\sum_{k=1}^g f_k(x_1^{(k)}, \\ldots, x_{m_k}^{(k)}), \\end{equation} where $f_k$ takes the same form as in \\eqref{eqn:f_k}."}
{"input": "deterministic. Given model \\eqref{eqn:true_model_CE} and a design, we can generate a training dataset in the same way as described in the last subsection. We fit the training dataset with a universal kriging model with fixed linear and quadratic main effects and the Gaussian correlation function \\citep{Gramacy-2020}. Next, a random Latin hypercube design of $N_t=1000$ runs is used as a test dataset, and the fitted model is used to predict the corresponding outputs. The R package \\texttt{DiceKriging} \\citep{Roustant-Ginsbourger-Deville-2012} is used to implement the model fitting and prediction procedure described above. Let $x_i$ be the $i$th point of the test dataset, $y(x_i)$ be the true response from \\eqref{eqn:true_model_CE}, and $\\hat{y}(x_i)$ be the prediction obtained from the fitted model. Then the prediction error can be evaluated by the root mean squared error $\\mbox{RMSE}=\\big[\\sum_{i=1}^{N_t}\\{y(x_i) - \\hat{y}(x_i)\\}^2/N_t\\big]^{1/2}$. Again, we examine the performances of GOA$(27, (4, 3, 3), 3\\times 3, 3,2)$ and GOA$(81, 4 \\times 3, 3\\times 3, 3,2)$ by comparing them with other types of designs. In addition to the minimum aberration designs (MAOAs) considered in Section \\ref{subsec:bias}, we also study Latin hypercube designs obtained by randomly expanding the levels of GOAs and MAOAs, the maximum projection Latin hypercube designs \\citep{Joseph-Gul-Ba-2015} generated by the \\texttt{MaxProLHD()} function in R package \\texttt{MaxPro} and the maximin-distance Latin hypercube designs \\citep{Ba-Myers-Brenneman-2015} generated by the \\texttt{maximinSLHD()} function in R package \\texttt{SLHD}. For each of these designs, we repeat the procedure described in the last paragraph 1000 times, and summarize the averages of the 1000 RMSE values in Table \\ref{tab:pred-rmse}."}
{"input": "of stages, the quality of the final product might be affected by several factors at each stage \\citep{Tyssedal-Kulahci-2015}. Then factors of the same stage can be viewed as coming from the same group. Once the active groups have been identified, the projection properties of GOAs provide benefits for studying the factors of these groups (see Remark \\ref{remark:group-effect-sparsity} and Section S3 of the Supplementary Material). Some intuition can be gained by considering the extreme case of one active group, as the GOA endows one with the opportunity to study the factors of this group with the same dataset since the effect aliasing among them is small due to the high strength of the design projected onto this group. The analysis strategy is different from that for traditional group screening, which typically requires two sets of experimentation, one for identifying active groups and the other for studying the factors of the active groups. Another application where GOAs could play a role is the experiments for dimensional analysis \\citep{Albrecht-etal-2013,Shen-etal-2014}. In dimensional analysis, factors are divided into groups such that those in the same group can be formulated as a dimensionless factor. By arranging the factors such that each dimensionless factor corresponds to a group of the GOA, more levels of the dimensionless factors can be observed, which would be advantageous for studying the effects of the dimensionless factors."}
{"input": "constructing orthogonal array-based Latin hypercubes \\citep{Tang-1993}. %The results of Wang et al. (2021) indicate that the resulting design will have good space-filling properties in the average sense. %One may also use an algorithm similar to that in \\cite{Xiao-Xu-2018} to optimize the resulting designs in terms of other space-filling criteria such as the maximin distance criterion. Alternatively, one can also apply the technique of \\cite{Sun-Tang-2017} to generate column-orthogonal designs from some GOAs presented in this paper. We illustrate this with the GOA$(32, 8 \\times 3, 3 \\times 3, 2, 2)$ presented in Table S1. Denote the centered version of this design (i.e., the two levels are denoted by $-1/2$ and $+1/2$) by $D=(D_1, D_2, D_3)$ and write $D_i = (D_{i1}, D_{i2})$ such that $D_{i1}$ and $D_{i2}$ each have 4 columns for $i=1,2,3$. Let $$ Q = \\begin{bmatrix} 4 & -2 & -1 & \\phantom{-}0 \\\\ 2 & \\phantom{-}4 & \\phantom{-}0 & \\phantom{-}1 \\\\ 1 & \\phantom{-}0 & \\phantom{-}4 & -2 \\\\ 0 & -1 & \\phantom{-}2 & \\phantom{-}4 \\end{bmatrix} \\quad \\mbox{and} \\quad D_{ij}' = D_{ij} Q $$ for $i=1,2,3$ and $j=1,2$. Let $D_i' = (D_{i1}', D_{i2}')$ for $i=1,2,3$. Then $D'=(D_1', D_2', D_3')$ is a column-orthogonal design of 32 runs for 24 factors with 8 levels. In addition, since $D_i$ has strength 3, it can easily be proved that the elementwise product of any two columns in $D_i'$ is orthogonal to another column in $D_i'$ for $i=1,2,3$."}
{"input": "than ordinary orthogonal array-based designs. It is therefore interesting to explore how to construct strong orthogonal arrays by using GOAs as base arrays. Many GOAs studied in this paper have constant group sizes. In practice, we may also encounter situations where the groups have different numbers of factors. One can drop columns from a GOA$(N, m \\times g, t \\times g, s, t_0)$ to obtain variable group sizes. %Fortunately, it is not uncommon for a regular minimum aberration design to embed in a large regular minimum aberration design. For example, all regular minimum aberration OA$(81, m, 3, t)$'s given in \\cite{Xu-2005} for $m \\leq 10$ are embedded in a regular OA$(81, 11, 3, 2)$. Hence, using the GOA$(81, 11 \\times 3, 2 \\times 3, 3, 2)$ in Table S5, we can construct a GOA$(81, (m_1, m_2, m_3), (t_1, t_2, t_3), 3, 2)$ such that every group has minimum aberration among all regular designs for any choice of $m_i \\leq 11$ ($i=1,2,3$). Despite being simple and useful, this approach of dropping columns obviously cannot cover all scenarios. It is thus a potential direction to construct GOAs with variable group sizes for future research. %The GOAs with minimum aberration groups obtained in Section \\ref{sec:GOA-MA} are regular designs and thus their run sizes must be prime powers."}
{"input": "sizes can be attained. %We leave this problem for future research. %, as well as the constructions when the overall strength is greater than two. Another important topic is the use of GOAs in analysis of computer experiments and beyond. Besides the universal kriging model used in Section \\ref{sec:simulation}, another approach to analyze the data, as mentioned in Section \\ref{sec:introduction}, is to use a Gaussian process with a blocked additive kernel. In such a model, the Gaussian process can be seen as a sum of several independent Gaussian processes where each process is defined over several factors called a block. It is intuitive to arrange the factors in such a way that those corresponding to the same block enjoy better space-filling properties in the experimental design. An interesting research problem is to examine how GOAs are connected to blocked additive kernels, as done in \\cite{Lin-Morrill-2014}, which showed the advantages of designs of variable resolution in model selection of linear models. %\\begin{figure}[t!] %\\includegraphics [angle=-90, scale=0.45]{carat}\\par %%\\centerline{\\epsfig{file=d:/sinicas/simuh3.eps,angle=-90,width=4.5in}}\\par %%\\centerline{\\epsfig{file=d:/sinicas/simuh4.eps,width=4.5in}}\\par %%\\centerline{\\epsfig{file=logo.eps,width=4.5in}}\\par %<- modified by Ivan (You need to put figure in the same folder; or you must specify the path to the figure.) %\\caption{Caption of the figure.} %\\end{figure} %\\begin{table}[t!] %*** %\\caption{Caption of the table.} %\\label{tab:simulation}\\par %\\resizebox{\\linewidth}{!"}
{"input": "\\textit{phantom} regime where its energy density increases with redshift, violating the null energy condition. Beyond DDE, within $\\Lambda$CDM, DESI + CMB also places stringent constraints on neutrino mass. Taken at face value, these constraints show a preference for the sum of the neutrino masses $M_\\nu = \\sum m_\\nu$ to be below that required from neutrino oscillation experiments $0.06 \\text{eV}$ at the close to $95\\%$ level. This preference is enhanced if the ``effective'' neutrino mass is allowed to take on negative values \\cite{Elbers25}. However, since DESI's contribution to the neutrino mass constraint is geometric, relaxing the cosmological expansion history in the standard model by, for example, allowing for DDE significantly relaxes these constraints. The apparent preference for unphysical behavior from either dark energy or neutrinos suggests re-evaluating common assumptions about the standard cosmological model. Our purpose in this work is to explore the implications for one physical extension of the standard model, i.e. the possibility of nonzero spatial curvature ($\\Omega_k$). Indeed, the DESI collaboration reported constraints on a curved $\\Lambda$CDM + $\\Omega_k$, finding a $2\\sigma$ preference for a negatively curved universe with $\\Omega_k = 0.0023 \\pm 0.0011$ \\cite{BAODR2}. The implications of this preference for the local distance scale, compared to the flat $\\Lambda$CDM model preferred by CMB data from the Planck satellite \\cite{Planck18}, are shown in Figure~\\ref{fig:ppd}."}
{"input": "\\section{The Role of Curvature} \\label{sec:curvature} \\begin{figure} \\centering \\includegraphics[width=0.8\\linewidth]{Figures/degeneracy.pdf} \\caption{Degeneracy between $\\Omega_k$ and $H_0$. The constraint from CMB temperature and polarization spectra, shown here from Planck PR4 chains \\cite{NPIPE_Camspec}, follows the $H_0 (1 - 7 \\Omega_k)$ degeneracy at small values of the curvature, with a width set by uncertainty in $\\omega_m$ from the primary CMB \\cite{Lemos23}. With this same $\\omega_m$ constraint, DESI BAO places a strong bound on $H_0$, leading to an $\\Omega_k$ constraint mostly dictated by CMB uncertainties. } \\label{fig:degeneracy} \\end{figure} Baryon acoustic oscillations (BAO) probe the expansion history in the late ($z \\approx 0-2$) universe by providing a standard ruler with length given by the sound horizon at the end of the baryon drag epcoh $z_d \\approx 1100$ \\begin{equation} r_d = \\int_{z_d}^\\infty \\frac{dz\\ c_s(z)}{H(z)}, \\end{equation} where $c_s(z)$ is the sound speed and $H(z)$ is the Hubble parameter at redshift $z$, in the form of a peak in the 2-point function of galaxies. Converting into the observed angular and redshift coordinates of galaxy surveys, BAO then directly constrain the dimensionless quantities $D_M(z) / r_d$ and $H(z) r_d$, where $D_M$ and $H(z)$ are the transverse comoving distance and Hubble parameter, at the effective redshift $z$ of the galaxies. These measurements are closely related to the measurement of the angular acoustic scale of the CMB, which probe the ratio of the ratio of the transverse comoving distance and sound horizon at recoupling $\\theta_\\ast = D_{A,\\ast}/r_\\ast$."}
{"input": "parts per ten thousand by the Planck satellite \\cite{Planck18}. Within the standard model of cosmology, CMB data alone are sufficient to give reasonably tight predictions for BAO measurements at lower redshifts. This is because the primary CMB by itself constrains the physical baryon and cold dark matter densities $\\omega_b, \\omega_{cb} = \\Omega_b h^2, (\\Omega_c +\\Omega_b) h^2$, which together determine $r_d$, and the remaining degree of freedom, the present-day Hubble constant $H_0 = 100 \\ h\\ \\text{km/s/Mpc}$, is fixed by the constraint on the distance-to-last-scattering $D_M(z_\\ast)$ set by $\\theta_\\ast$, since the dark energy fraction $\\Omega_\\Lambda$ is fixed given any set of these three parameters. In this way of thinking, the roughly $1.5\\%$ shorter distances compared to Planck $\\Lambda$CDM, as measured by DESI BAO, can be roughly thought of as a small $1.5\\%$ Hubble tension at $z \\lesssim 1.5$. Indeed, the DESI collaboration show that primary CMB constraints on $\\omega_b, \\omega_{cb}$ and $\\theta_\\ast$ alone, as presented in ref.~\\cite{Lemos23}, are sufficient to reproduce most of the preference for DDE when combined with DESI BAO \\cite{BAODR2}. The former two densities are constrained by the primary CMB from Planck to roughly $0.7\\%$ and $0.84\\%$, respectively, meaning that absent changes to recombination physics the cosmological uncertainty on $r_d$ is at the less than $0.2\\%$ level. Spatial curvatures relaxes the tight relations set by CMB constraints on $\\omega_b$, $\\omega_c$ and $\\theta_\\ast$ by introducing a small nonlinearity into the relations between cosmological distances."}
{"input": "is so small compared to $\\Omega_{m,\\Lambda}$, it plays only a very small role in determining $H^2(z) \\sim \\rho(z)$ at any epoch. A given value of $\\Omega_k$ corresponds to a radius of curvature in comoving coordinates of \\begin{equation} R_k = \\frac{1}{\\sqrt{|\\Omega_k|}H_0} \\approx 21 H_0^{-1} \\left( \\frac{|\\Omega_k|}{0.0023} \\right)^{-\\frac12}. \\end{equation} In comparison, the comoving distances to redshifts $z=0.5, 1, 1100$ are roughly $0.4 H_0^{-1}, 0.8 H_0^{-1}$ and $3.1 H_0^{-1}$, respectively. The fractional difference between the comoving distance $\\chi$ and transverse comoving distance $D_M = R_k \\sinh(\\chi/R_k)$ is given by $\\chi/D_M \\approx 1 + \\frac16 R_k^{-2} \\chi^2$ to leading order and negligible for the redshifts probed by BAO for the values of curvature we consider. The correction at last scattering is roughly equal to $\\frac53 \\Omega_k$ for $\\Omega_m \\approx 0.3$, or $3.5\\%$ at $\\Omega_k = 0.0023$. All else being equal\\footnote{Here, for a given $\\Omega_k$ we keep the $H_0$ and $\\Omega_{m} h^2$ constant while adjusting $\\Omega_\\Lambda$.}, the comoving distance to last scattering $\\chi_\\ast$ is changed by a smaller relative amount, roughly $- \\Omega_k / 4$, leading to a total change $\\Delta D_\\ast/D_\\ast \\approx 1.4\\ \\Omega_k. $ We can thus think of curvature, at the values allowed by current data, as only modifying the distance to last scattering in $\\theta_\\ast$. This allows us to measure $\\Omega_k$: In flat $\\Lambda$CDM, the angular acoustic scale roughly constrains the combination $\\Omega_m h^3$, with $\\theta_\\ast$ scaling roughly as $\\omega_m^{0.14} h^{0.2}$ \\cite{Hu01,Percival02}, where $\\omega_m = \\Omega_m h^2$."}
{"input": "roughly $(1 - 1.4 \\Omega_k)$ to $\\theta_\\ast$, and modifies the best-fit combination given our choice of data \\begin{equation} \\Omega_m h^3 (1 - 7 \\Omega_k) = 0.09603 \\pm 0.00026. \\end{equation} However, the primary CMB independently constrains $\\omega_m$ to sub-percent accuracy, allowing us to constrain the combination $H_0 (1 - 7 \\Omega_k) $ to roughly $\\sigma(\\omega_m)/\\omega_m \\lesssim 1\\%$. At low redshifts, the effects of curvature are negligible, so BAO constrain $H_0$ through $H_0 r_d$---assuming that variations in $r_d$ are subdominant---and $\\Omega_m$ almost identically to the flat $\\Lambda$CDM case. Indeed the DESI data best constrain a combination close to $\\Omega_m^{0.2} H_0 = \\omega_m^{0.2} H_0^{0.6}$, at roughly the $0.3\\%$ level. Assuming again a CMB prior on $\\omega_m$, this is equivalent to a roughly $0.5\\%$ constraint on $H_0$, allowing CMB + BAO to break the $H_0-\\Omega_k$ degeneracy and constrain BAO. The factor of $7$ in the degeneracy gives us a curvature measurement of about $\\sigma(\\Omega_k) = 0.0012$. The slightly higher $H_0$ measured by BAO relative to the CMB is thus efficiently translated into a curvature of $\\Delta H_0 / (7 H_0) \\approx 0.002$. This degeneracy is illustrated in terms of the $H_0-\\Omega_k$ posterior in Figure~\\ref{fig:degeneracy}."}
{"input": "constraints. \\begin{figure} \\centering \\includegraphics[width=0.8\\linewidth]{Figures/distances.pdf} \\caption{Distances $D_H$, $D_M$ and $D_V$ fit to three ($\\Lambda$CDM, curved $\\Lambda$CDM and $w_0 w_a$CDM) cosmological models given DESI and CMB data, normalized to their values in the Planck best-fit $\\Lambda$CDM cosmology. The right extension shows these distances at high redshift, where $D_M/r_d$ in all models is forced to converge by the CMB constraint on $\\theta_\\ast$. } \\label{fig:distances} \\end{figure} Figure~\\ref{fig:distances} shows $D_{V,M,H}$ in different cosmological models fit to DESI BAO and the CMB at the low and high redshifts measured by the former and latter, respectively. In each of these models the best-fit parameters conspire to decrease the measured BAO scale by about $1.5\\%$ between the $z \\approx 0.5 - 1.5$, reflecting the low points in Figure~\\ref{fig:ppd}. However, one notable difference is that the best fit curved $\\Lambda$CDM model exhibits a fairly smooth dependence at low redshifts, amounting to a rescaling relative to the best-fit flat model, while the DDE $w_0 w_a$CDM model induces relatively large changes in the energy density and distances at low redshifts to fit the data."}
{"input": "\\section{Implications for Neutrino Mass} \\label{sec:neutrinos} In the standard model, the combination of CMB and BAO data allows us to geometrically measure the neutrino mass using the same degeneracy breaking as discussed above in Section~\\ref{sec:curvature}. We point the interested reader to ref.~\\cite{Weiner24}, as well as the DESI collaboration's supporting paper on neutrino mass constraints \\cite{Elbers25}, for a more detailed discussion; briefly: since neutrinos are still relativistic at recombination, the CMB power spectrum shape constraint on $\\omega_m$ is really a constraint on $\\omega_{cb} = (1 - f_\\nu) \\omega_m = (\\Omega_c + \\Omega_b) h^2$, where $f_\\nu$ is the neutrino mass fraction, in the presence of massive neutrinos. As in the case of $\\Omega_k$, geometric data in the CMB and BAO then allows us to distinguish between $\\omega_{cb}$ and $\\omega_m$, giving us a constraint on $M_{\\nu} \\approx (\\omega_m - \\omega_{cb})\\ 93\\ \\text{eV}$. For example, in the case of the angular acoustic scale, the addition of massive neutrinos decreases the distance to last scattering $D_\\ast \\sim \\Omega_m^{-0.1}/\\sqrt{\\omega_m}$ by a factor of roughly $(1 + 0.4 f_\\nu)$ compared to a neutrino-less universe with the same $H_0$ and baryon and dark matter densities \\cite{Percival02,Weiner24}, leading to $\\theta_\\ast \\sim \\omega_{cb}^{0.14} h^{0.2} (1 + 0.4 f_\\nu) $ and a best-constrained combination $\\Omega_m h^3 (1 + f_\\nu)$.\\footnote{Empirically, we find for our data including CMB lensing that the best-constrained combination is modified to $\\Omega_m h^3 (1 + 0.65 f_\\nu) = 0.09628 \\pm 0.00026$."}
{"input": "itself, but the measurement of the BAO scale at low redshift can as in the case of curvature. \\begin{figure} \\centering \\includegraphics[width=0.8\\linewidth]{Figures/mnu_constraints.png} \\caption{The 1D posterior for the sum of the neutrino masses assuming a positivity prior in a $\\Lambda$CDM (red) and curved $\\Lambda$CDM (black) universe. Neutrino mass constraints are significantly relaxed in the latter case, with both the normal and inverted hiearchies (black dotted lines) allowed at $95\\%$ confidence.} \\label{fig:mnu_constraints} \\end{figure} Allowing for nonzero spatial curvature relaxes this constraint because the curvature is determined through breaking the same degeneracy. Roughly speaking, the degeneracy direction is determined by their respective modifications to $\\theta_\\ast$: keeping $\\theta_\\ast$ constant requires $1.4 \\Omega_k = 0.4 f_\\nu$, i.e. the degeneracy has a steep slope of $\\Delta f_\\nu / \\Delta \\Omega_k = 3.5$. \\begin{figure} \\centering \\includegraphics[width=0.8\\linewidth]{Figures/mnu_contours.pdf} \\caption{Same as Fig.~\\ref{fig:mnu_constraints}, but for two-dimensional posteriors with other cosmological parameters.} \\label{fig:mnu_contours} \\end{figure} Figures~\\ref{fig:mnu_constraints} and \\ref{fig:mnu_contours} show the 1- and 2-D posteriors of the curved $\\Lambda$CDM model with a free neutrino mass. We adopt a positivity prior on the neutrino mass for brevity. Allowing for spatial curvature significantly relaxes the constraints on $M_\\nu$, such that both the normal and inverted hiearchies fall within the $95\\%$ confidence interval with this choice of prior, though the degeneracy between $\\Omega_k$ and $M_\\nu$ is not exact due to the BAO constraint also on $\\Omega_m$ and additional information in the full CMB, in particular measurements of structure growth from CMB lensing."}
{"input": "\\section{Detectability in Future Surveys} \\label{sec:detectability} \\begin{figure}[h] \\centering \\includegraphics[width=0.8\\linewidth]{Figures/once_and_future_surveys.pdf} \\caption{Detectability of different cosmological models in light of DESI DR2 (black points) and the planned Spec-S5 experiment (gray $1-2\\sigma$ bands). At the redshifts probed by Spec-S5, a curved model preferred by the DESI+CMB data predicts shorter distances at the roughly $0.5\\%$ level compared to $\\Lambda$CDM or DDE, which Spec-S5 will be able to constrain at high significance. } \\label{fig:specs5} \\end{figure} As shown in Figure~\\ref{fig:distances}, one of the distinguishing features of curvature as compared to other models that resolve the tension between DESI BAO and CMB data is that cosmological distances, and in particular the transverse comoving distance, can stay low relative to what is required to preserve $\\theta_\\ast$ until relatively high redshifts owing to the nonlinear relation between distance measures in cosmologies with spatial curvature. In order to investigate whether this signal can be meaningfully utilized in future cosmology experiments, let us consider the expected constraints from a proposed Stage-V spectroscopic instrument (Spec-S5) \\cite{SpecS5}. We estimate these constraints using a Fisher matrix calculation based on two galaxy samples in redshift bins $z \\in [2.1, 3.5],\\ [3.5, 4.5]$ over $11,000$ square degrees.\\footnote{We thank Martin White for providing the number densities and linear biases for these samples in ref.~\\cite{SpecS5}."}
{"input": "max} = 0.3\\ h\\text{Mpc}^{-1}$, though we find our forecasts to be relatively insensitive to these choices. Figure~\\ref{fig:specs5} shows the forecasted Spec-S5 constraints compared to the predictions of different cosmological models. Spec-S5 is expected to constrain the isotropic BAO scale $D_V/r_d$ over its combined redshift range at better than $0.1\\%$: notably, while both the DESI + CMB best-fit $\\Lambda$CDM and $w_0 w_a$CDM models would fit inside a $95\\%$ confidence interval in both redshift bins, the curved $\\Lambda$CDM curve would be detected at greater than $5\\sigma$ using the isotropic BAO. This points to the significant promise of high-redshift spectroscopic observations to pin down the curvature of the universe while distinguishing it from other forms of beyond-$\\Lambda$CDM physics. We note also that the constraints on the Alcock-Paczynski distortion $D_M/D_H$, while subdominant, can in addition measure the best-fit $w_0 w_a$CDM model at more than $2\\sigma$."}
{"input": "\\section{Conclusions} \\label{sec:discussion} Cosmological observations point to a universe close to $\\Lambda$CDM that is exceedingly flat. However, recent BAO data published by the DESI collaboration \\cite{BAODR2}, when combined with CMB data, point to a $2\\sigma$ hint of nonzero, negative spatial curvature, with a comoving curvature radius of about $21 H_0^{-1}$. A detection of negative spatial curvature would have significant implications for e.g. inflation, which cannot generically occur given positive curvature, and rule out scenarios such as slow-roll eternal inflation. In this note we have explored the consequences of such a small negative value of the curvature, or positive $\\Omega_k$, for the CMB and large-scale structure. Freeing $\\Omega_k$ at this level in cosmological analyses alleviates tension between the CMB and DESI BAO measurements by keeping the constraints from low-redshift BAO measurements and CMB power spectrum shape information essentially unchanged from flat $\\Lambda$CDM while adding a steep dependence to the observe angular acoustic scale $\\theta_\\ast$. This effect also significantly widens constraints on the sum of the neutrino masses $M_\\nu$, whose geometric measurement from these data rely on the same set of physical quantities, such that even a present day $\\Omega_k$ at two tenths of a percent are sufficient to widen $M_\\nu$ constraints from modestly disfavoring the normal hierarchy to accommodating both the normal and inverted hierarchies within $95\\%$ confidence."}
{"input": "the significant promise of such surveys in unveiling beyond-$\\Lambda$CDM physics. Since the role of curvature in fits to BAO + CMB data is essentially to allow modifications to the distance to last scattering $D_{M,\\ast}$ while keeping other cosmological parameters fixed, it is closely (inversely) related to alternative models that address tensions by changing the physics at recombination by changing the last-scattering sound horizon $r_\\ast$ or redshift $z_\\ast$. Ref.~\\cite{Hamidreza25} recently explored this possibility in the context of DESI DR2 for a phenomenological model modifying recombination, and many such models were constrained using CMB power spectra from ACT DR6 \\cite{ACT_extended}. Similarly to curvature, which will be probed by future high-redshift spectroscopic data, these modifications of recombination will be better constrained by future CMB data, including the Simons Observatory \\cite{SO} and CMB-S4 \\cite{CMBS4}. In the absence of new pre-recombination physics, these new observatories will significantly improve constraints on $\\omega_b, \\omega_{cb}$ which are currently a main limiting factor in constraining $\\Omega_k$. Together with expected improvements in measurements of the BAO scale (e.g. $H_0 r_d$), these future experiments will help home in on present tensions between the two types of data. While we have kept with the data used in the DESI DR2 analyses in ref.~\\cite{BAODR2}, it would be interesting to explore these avenues in light of the recent ACT DR6 CMB power spectra for this same reason \\cite{ACTDR6}."}
{"input": "on DDE using additional probes, in particular Type Ia supernovae \\cite{DESY5,Union3,Pantheon}. The DESI collaboration combines these data, particularly those from ref.~\\cite{DESY5}, with BAO and CMB data to yield a significant detection of DDE, and in particular DDE in the phantom regime, assuming a spatially flat universe. Recently, refs.~\\cite{Akrami25,Dinda25} conducted analyses of physical models of DDE with a free spatial curvature, finding that the data can be equally well fit by non-phantom dark energy in that scenario. This is because, in a cosmology with nonzero spatial curvature, the suppression of the distance scale at low redshifts compared to the CMB can be achieved via the nonlinear relation between different distances on large scales, rather than an unexpected increase in the late-universe energy density requiring dark energy to increase with time. Future observations, e.g. at high redshifts from Spec-S5, will be critical in differentiating between extensions of the standard model such as curvature or dynamical dark energy."}
{"input": "and networks lacking centralized infrastructure \\cite{8292633}. Connectivity in UAV-assisted systems can be further improved by increasing the number of UAVs deployed, as demonstrated and discussed in previous works \\cite{8292633, saifglobecom_E}. However, UAVs are inherently prone to failures due to their limited energy capacity, leading to network interruptions and outages that significantly impact the information flow from UEs to fusion centers. Furthermore, deploying a large number of UAVs in densely populated urban areas presents challenges, including site constraints and limited space \\cite{Rui_Zhang_UAV}. On the other hand, several works have investigated the deployment of more nodes (sensors and backhaul links) to maximize connectivity, e.g., \\cite{4786516, 4657335, H}. Therefore, it is imperative to study alternative solutions that can mitigate these challenges while maximizing the benefits of deploying additional UAVs, without additional system complexity or cost. %2) \\textit{RIS-assisted UAV Communications:} RIS consists of a large number of low-cost passive elements that can be dynamically adjusted to alter their phases, thus changing the impinging electromagnetic signals \\cite{4A, 6A, 10A}. By smartly controlling the behavior of the waves, RISs can improve the quality, coverage, and energy efficiency of wireless networks while improving connectivity via adding multiple cascaded channels \\cite{5A}, called RIS-aided links. Using RISs to improve connectivity of UAV networks has taken its shape in the recent literature. Recent works improve the connectivity of UAV networks via a single RIS \\cite{saifglobecom}, where the authors formulate the network connectivity problem as semidefinite programming (SDP) and solve it using the CVX toolbox \\cite{CON}."}
{"input": "the results of \\cite{saifglobecom} to multiple RISs and solve the problem via SDP and matrix perturbation. In \\cite{saifglobecom, saifglobecom_E}, each RIS assists to improve the connectivity of the UAVs networks by adding a single RIS-aided link that connects a UE and a UAV. These studies demonstrate good performance compared to traditional UAV communications without RIS assistance, which serve as baseline schemes in this paper. Furthermore, \\cite{aydin} uses the RIS to boost the strength of the signals for resilient wireless networks. RIS virtual partitioning is a RIS optimization technique that enhances communication performance \\cite{PLS, saifVTC}. The idea of RIS virtual partitioning is to divide RIS into several virtual sections, with the phase shifts of each partition configured to a specific direction. This allows a single RIS plate to simultaneously reflect one signal into multiple cascaded signals in different directions. Specifically, in \\cite{PLS}, the RIS is used to amplify the signal intended for legitimate UEs while attenuating the signal for illegitimate UEs, thereby enhancing the network secrecy through RIS partitioning. Additionally, the authors of \\cite{saifVTC, saifTCOM} consider optimizing RIS partitioning to maximize UAV connectivity for a simple model of one UE per RIS, limiting the ability of RIS to aid the links of multiple scheduled UEs in massive networks. %As such, multiple scheduled UEs can be connected to a UAV via a single RIS using the virtual partitioning technique. Non-orthogonal multiple access (NOMA) is a viable solution to enhance massive connectivity by scheduling multiple UEs to the same network resources \\cite{NOMA-1, NOMA-2}."}
{"input": "\\section{System Model} \\label{S} \\subsection{Network Model}\\label{NM} Consider the schematic shown in Fig. \\ref{fig1} with multiple single-antenna UAVs and UEs, and multiple RISs with $K$ passive reflecting elements. We represent the sets of UAVs, UEs, and RISs as $\\mathcal A$, $\\mathcal U$, and $\\mathcal R$, respectively. Each uniform planar array (UPA) RIS is equipped with $K=K_h \\times K_v$ elements where $K_h$ and $K_v$ separately denotes the sizes along the horizontal and vertical dimensions of RIS, respectively. The RISs are essential for maintaining connectivity between UEs and UAVs since the direct UE-UAV links might be obstructed. Assuming a dense urban scenario, where direct links between the UEs and some UAVs are blocked, RISs can significantly aid in establishing reliable communication with the blocked UAVs. In Fig.~\\ref{fig1}, for example, if UE$_{1}$ is not connected, it becomes completely blocked due to the absence of direct links to the UAVs, making the network unconnected (zero connectivity). In NOMA schemes, the disparity in power reception is a critical factor influencing the overall performance gain over OMA. Similar to \\cite{NOMA, optimal}, we overcome this by partitioning the RISs to serve multiple UEs, where each partition is configured to enhance the quality of its corresponding RIS-aided link. Consequently, each UAV receives signals with coherently aligned phases from its own RIS partition and non-coherently aligned phases from other RIS partitions. For practical implementation, we consider scheduling a small number of UEs per RIS using NOMA due to two reasons."}
{"input": "of successive interference cancellation (SIC), which grows nonlinearly with the number of UEs \\cite{NOMA}. Second, scheduling more UEs to RIS using NOMA does not substantially improve network connectivity, as each RIS would need to be divided into multiple partitions, resulting in weaker RIS-aided links. This observation is further supported by the numerical results presented in Section \\ref{NR}. Therefore, this study focuses on deploying multiple strong RIS-aided links to optimize connectivity and performance, while practically implementing NOMA. %For the purpose of practicality, we consider only two NOMA UEs per RIS in this paper due to the following two reasons. First, it is important to acknowledge that serving multiple NOMA UEs per RIS can pose significant challenges, as the complexity of the SIC process escalates exponentially with an increased number of UEs, as previously reported in the literature such as [39]. Second, %This improves the overall network connectivity performance of the system. \\begin{figure}[t!] \\begin{center} \\includegraphics[width=0.99\\linewidth, draft=false]{NOMA-RIS-SM-new2.pdf} \\caption{RIS-assisted NOMA UAV system with two RISs, each utilizes RIS partitioning technique to aid the signals of two UEs.} \\label{fig1} \\end{center} \\end{figure} To demonstrate the suitability of this study for massive connectivity, we consider a practical dense network scenario where the number of UEs greatly exceeds the number of available UAVs and RISs, i.e., $U>AR$. The UEs are grouped into NOMA clusters, denoted by $\\mathcal U_r, \\forall r \\in \\mathcal R$. $\\mathcal U_r$ also represents the number of partitions in RIS$_r$ and $U_r=|\\mathcal U_r|$."}
{"input": "the number of RISs $R$, and the terms RIS and cluster are used interchangeably throughout this paper. For simplicity, $\\mathcal U_r$ is the associated cluster of RIS$_r$. Through virtual partition representation, each RIS can connect $U_r$ UEs to one UAV. In this case, the partitions of the RISs need to be designed to simultaneously boost the signal dedicated to the first $R$ most reliable UAVs. The set of the possible transmitting UEs, denoted by $\\mathcal U_t$, is at most $\\mathcal U_rR$. Fig. \\ref{fig1} illustrates that each RIS is partitioned into two portions, where each portion is configured to passively beamform the signals of UE$_{u \\in \\mathcal U_t}$ to the corresponding UAVs (i.e., $\\mathcal U_{r=1}=\\{\\text{UE}_1, \\text{UE}_{2}\\}, \\mathcal U_{r=2}=\\{\\text{UE}_3, \\text{UE}_{4}\\}$ and $U_{r=1}=U_{r=2}=2$). Low-reliability UAVs are the most critical, as their failure—due to issues such as low battery, hardware malfunctions, or software problems—can cause severe connectivity degradation. Hence, it is essential to avoid relying on these UAVs. The UAV reliability metric, as defined in Section \\ref{NRE}, ensures that signals from UEs are transmitted to the most reliable UAVs, minimizing the risk of connectivity loss and enhancing network resilience. If RIS$_r$ is assigned to serve cluster $\\mathcal U_r$, the number of elements allocated to aid the signal of UE$_u$ (i.e., $u \\in \\mathcal U_r$) to UAV$_a$ is denoted by $K^a_{u,r}=\\lceil\\alpha^{a}_{u,r}K\\rceil$, where $\\alpha^{a}_{u,r} \\in [0, 1]$ is the RIS allocation factor such that $\\sum_{u \\in \\mathcal U_r} \\alpha^{a}_{u,r} \\leq 1$ and $\\sum_{u \\in \\mathcal U_r}K^{a}_{u,r}=K$ ensure the physically available number of RIS elements."}
{"input": "and the third term is the additive white Gaussian noise (AWGN) power density at UAV$_a$ with $w_a \\sim \\mathcal {CN}(0, \\sigma^2_{\\zeta_a})$. The first term of \\eref{YO} can be expressed as follows \\begin{align}\\label{Yu} &y_u= \\bigg(\\underbrace{\\sqrt{\\Tilde{G}_{u,a}}h_{u,a}}_\\text{direct link}+ \\underbrace{\\sqrt{\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}}\\mathbf h_{r,a} \\mathbf{\\Theta}_{r} \\mathbf g_{u,r}}_\\text{via RIS$_r$ aligned for UE$_u\\in \\mathcal U_r$} \\nonumber \\\\& + \\underbrace{\\sum_{r' \\in \\mathcal R, r' \\neq r}\\sqrt{\\Tilde{G}_{u,r'} \\Tilde{H}_{r',a}}\\mathbf h_{r',a} \\mathbf{\\Theta}_{r'} \\mathbf g_{u,r'}}_\\text{via RIS$_{r'}$ aligned for $\\mathcal U_{r'}$}\\bigg)\\sqrt{p}s_u, \\end{align} where $s_u$ is the signal of UE$_u$ and $p$ is the transmit power of UE$_u$, which is fixed for all UEs. $y_u$ in \\eref{Yu}, which can also be used for $y_{u' }$ by replacing $u'$ with $u$, has the following components \\begin{enumerate} \\item Signal received from the $\\text{UE}_u \\rightarrow \\text{UAV}_a$ direct link, $\\sqrt{\\Tilde{G}_{u,a}}h_{u,a}$; \\item Signal received from UE$_u$ to UAV$_a$ via RIS$_r$, $\\sqrt{\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}}\\mathbf h_{r,a} \\mathbf{\\Theta}_{r} \\mathbf g_{u,r}$, which can be written as \\begin{align}\\label{Yuu} \\nonumber & \\sqrt{\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}}\\mathbf h_{r,a} \\mathbf{\\Theta}_{r} \\mathbf g_{u,r}=\\underbrace{\\sqrt{\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}} \\sum_{k=1}^{K^{a}_{u,r}}h^{(k)}_{u,r,a}e^{j\\theta^{(k)}_{u,r}}}_\\text{aligned signal from portion $K^{a}_{u,r}$}\\\\& +\\underbrace{\\sqrt{\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}} \\sum_{u' \\in \\mathcal U_r, u' \\neq u}\\bigg( \\sum_{k'=1}^{K^{a}_{u',r}}h^{(k')}_{u,r,a}e^{j\\theta^{(k')}_{u',r}}\\bigg)}_\\text{non aligned signal from portion $K^{a}_{u',r}$}, \\end{align} where $\\theta^{(k)}_{u,r}$ is the $k$-th associated phase shift of RIS$_r$ with UE$_u$. The terms of \\eref{Yuu} can be written as $\\sqrt{\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}} \\sum_{k=1}^{K^{a}_{u,r}}h^{(k)}_{u,r,a}e^{j\\theta^{(k)}_{u,r}}=\\sqrt{\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}} \\alpha^{a}_{u,r}\\sum_{k=1}^{K}h^{(k)}_{u,r,a}e^{j\\theta^{(k)}_{u,r}}$ and $\\sqrt{\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}} \\sum_{k'=1}^{K^{a}_{u',r}}h^{(k')}_{u,r,a}e^{j\\theta^{(k')}_{u',r}}=\\sqrt{\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}} \\alpha^{a}_{u',r}\\sum_{u' \\in \\mathcal U_r, u' \\neq u}\\bigg(\\sum_{k=1}^{K}h^{(k)}_{u,r,a}e^{j\\theta^{(k)}_{u',r}}\\bigg)$.%, where $\\alpha_u$ and $\\alpha_{u'}$ are RIS$_r$ portions associated to UE$_u$ and UE$_{u'}$, respectively. %This approximation will be compared with the exact model in the prospective %numerical results section."}
{"input": "\\Tilde{H}_{r',a}}\\mathbf h_{r',a} \\mathbf{\\Theta}_{r'} \\mathbf g_{u,r'}$. \\end{enumerate} It is worth noting that in UAV communications, UAVs are deployed to be far away from each other to avoid any collision. Additionally, this paper considers narrow-beam RIS beamforming to UAVs. Consequently, the non-aligned signals from the other RISs (i.e., the second term in \\eref{YO}) are small, which can be neglected. Furthermore, due to the difficulty of deriving a closed-form solution of $\\alpha^{a}_{u,r}$ and $\\alpha^{a}_{u',r}$ from \\eref{exact}, we ignore the impact of non-aligned phases of the same RIS. Nevertheless, this assumption has been numerically evaluated in the results section; please refer to Section \\ref{NR}. In light of these discussions, \\eref{YO} can be rewritten as \\begin{align}\\label{Yfinal} \\nonumber y^{(r)}_a(\\boldsymbol{\\alpha}_r)=& \\sum_{u\\in \\mathcal U_r} \\bigg( \\bigg[\\sqrt{\\Tilde{G}_{u,a}}h_{u,a}+ \\alpha^{a}_{u,r}\\sqrt{\\Gamma_u}\\sum_{k=1}^{K} h^{(k)}_{u,r,a} e^{j\\theta^{(k)}_{u,r}}\\bigg] \\\\& \\times s_{u}\\sqrt{p} \\bigg) +\\zeta_a, \\end{align} where $\\boldsymbol{\\alpha}_r=[\\alpha^{a}_{u,r}, \\ldots, \\alpha^{a}_{U_r,r}]$ and $\\Gamma_u=\\Tilde{G}_{u,r} \\Tilde{H}_{r,a}$. Given the fixed transmit powers of UEs, distinguishing between the UEs receiving signals at the UAVs is critical. However, the related channel gain variations and the associated RIS partitions of each UE can achieve this difference. In this case, the stronger UE’s signal message gets decoded while the messages of the other UE is treated as interference. Specifically, at UAV$_a$, the received signal, $y^{(r)}_a(\\boldsymbol{\\alpha}_r)$, is decoded using the SIC scheme. In this iterative scheme, the signal from the UE with the strongest received power is decoded first while treating signals from other UEs as interference. Once decoded, this signal is subtracted from the total received signal, allowing the next strongest UE's signal to be extracted."}
{"input": "channel, $\\gamma^\\text{(UA)}_{u,a}=\\frac{p|\\sqrt{\\beta^\\text{UA}_{u,a}}h^\\text{UA}_{u,a}|^2}{N_0}$. \\item $\\gamma^\\text{UAV}_\\text{th}$ and $\\gamma^\\text{UE}_\\text{th}$ are the minimum SNR thresholds for the UAV-UAV and the UE-UAV communication links, respectively. \\end{itemize} Let $\\mathbf w \\in {[\\mathbb R^+]}^{E_\\text{org}}$ be the weight vector of the UE-UAV and UAV-UAV links, which is defined as $\\mathbf w= [ \\omega_1, \\omega_2, \\ldots, \\omega_{E_\\text{org}}]$. Accordingly, for $e_{l}$, the weight $\\omega_l$ is defined as \\begin{equation}\\label{wei} \\omega_l = \\begin{cases} \\gamma^\\text{(UAV)}_{v,v'} & \\text{for} ~ \\text{UAV}_v \\rightarrow \\text{UAV}_{v'}~ \\text{link},\\\\ \\gamma^\\text{(UA)}_{v,v'} & \\text{for}~ \\text{UE}_v \\rightarrow \\text{UAV}_{v'} ~ \\text{link}. \\end{cases} \\end{equation} Let $\\mathbf a_l$ be a vector of link $e_{l}$, where the $v$-th and the $v'$-th elements in $\\mathbf a_l$ are given by $a_{v,l}=1$ and $a_{v',l}=-1$, respectively, and zero otherwise. Let $\\mathbf A$ be the incidence matrix of $\\mathcal G_\\text{org}$ with the $l$-th column given by $\\mathbf a_l$. Hence, in $\\mathcal G_\\text{org}(\\mathcal V, \\mathcal E_\\text{org})$, the Laplacian matrix $\\mathbf L_\\text{org}$ is a $V \\times V$ matrix, defined as \\cite{4657335} \\begin{equation} \\label{lap} \\mathbf L_\\text{org}= \\mathbf A ~diag(\\mathbf w) ~\\mathbf A^T=\\sum^{E_\\text{org}}_{l=1} \\omega_l \\mathbf a_l \\mathbf a^T_l, \\end{equation} where the entries of $\\mathbf L_\\text{org}$ are given by \\begin{equation} L_\\text{org}(v,v') = \\begin{cases} \\sum_{s \\neq v}\\omega_{v,s} &\\text{if} ~v=v',\\\\ -\\omega_{l} &\\text{if}~ (v, v') \\in \\mathcal E_\\text{org}, \\\\ 0 & \\text{otherwise}. \\end{cases} \\end{equation} Similar to \\cite{new, 4657335, 4786516, 8292633, saifglobecom}, we choose a well-known metric, known as the \\textit{algebraic connectivity}, also called the Fiedler value, denoted by $\\lambda_2(\\mathbf L_\\text{org})$, to maximize the connectivity of the network."}
{"input": "larger set of edges denoted by $\\mathcal E_\\text{mod}$ with $\\mathcal E_\\text{mod}=\\mathcal E_\\text{org} \\cup \\mathcal E_\\text{new}$, where $\\mathcal E_\\text{new}$ is the new $\\text{UE}_u \\xrightarrow{\\text{RIS}_r} \\text{UAV}_{a}$ edges, $\\forall u\\in \\mathcal U_t, a \\in \\mathcal A, r\\in \\mathcal R$. For a new edge $e_{l}$, $ 1 \\leq l \\leq E_\\text{mod}$, that connects two vertices $(u, a') \\in \\mathcal V$ via RIS$_r$, we have \\begin{equation} e_l = \\begin{cases} 1 & \\text{if} ~ \\gamma^{(r)}_{u,a} \\geq \\gamma_\\text{th}^\\text{RIS} ~ \\text{for}~ \\text{UE}_u \\xrightarrow{\\text{RIS}_r} \\text{UAV}_{a}~ \\text{link},\\\\ 0 & \\text{otherwise}, \\end{cases} \\end{equation} where $\\gamma^\\text{RIS}_\\text{th}$ is the minimum SINR threshold of $\\text{UE}_u \\rightarrow \\text{UAV}_{a}$ via RIS$_r$. Essentially, with RIS deployment and partitioning, we either (i) add a new $\\text{UE}_u \\xrightarrow{\\text{RIS}_r} \\text{UAV}_{a}$ link if UE$_u$ is not directly connected to UAV$_a$ or (ii) tune the SINR of the $\\text{UE}_u \\rightarrow \\text{UAV}_{a}$ link using the SINR of the cascaded link of RIS$_r$. As a result, for $e_{l}$, the weight $\\omega_l$ is \\begin{equation}\\label{wei} \\omega_l = \\begin{cases} \\gamma^{(r)}_{u,a} & \\text{Given in \\eref{approNO} or} \\\\ \\gamma^{(r)}_{u,a} & \\text{Given in \\eref{approG}}. \\end{cases} \\end{equation} The network connectivity gain can be assessed by computing $\\lambda_2 (\\mathbf L_\\text{mod}) \\geq \\lambda_2 (\\mathbf L_\\text{org})$, where $\\mathbf L_\\text{mod}$ is the resulting Laplacian matrix of the new graph $\\mathcal G_\\text{mod}(\\mathcal V, \\mathcal E_\\text{mod})$. Adding more RIS-aided links can modify the entries of $\\mathbf L_\\text{org}$ and adds new non-zero elements in $\\mathbf A$, which generally increases $\\lambda_2(\\mathbf L_\\text{mod})$."}
{"input": "disconnected subgraphs. %Therefore, %The upper and lower bounds on the algebraic connectivity of a graph obtained by adding RIS-aided links connecting UEs to UAVs to a single connected graph is given in \\cite[Proposition~3]{saifglobecom_E}. \\subsection{Node Reliability}\\label{NRE} %\\ac{UAV-UAV links enhance UE-UAV connectivity by providing link redundancy and routing, enabling cooperation, and improving network flexibility. If UAV-UAV links are ignored, the model becomes simpler and it cannot represent the original problem, as it can no longer capture the full resilience and connectivity benefits provided by UAV cooperation.} Let $\\mathcal G^{-v}_\\text{org}$ be the remaining graph after removing vertex $v \\in \\mathcal V$ along with all its adjacent edges to other vertices in $\\mathcal G_\\text{org}$, i.e., $\\mathcal G^{-v}_\\text{org} \\subseteq \\mathcal G_\\text{org}$. We calculate the connectivity of the remaining graph based on the \\textit{Fiedler value} \\cite{new}, which is defined as $\\lambda_2(\\mathcal G^{-v}_\\text{org})$. A node that, when removed, significantly reduces the connectivity of the network is declared to be highly critical and thus not reliable. Therefore, we measure the reliability of the nodes based on their criticalities, which reflects the severity of the impact on the connectivity of the remaining graph, which is defined as \\begin{equation}\\label{eq1} C_v=\\lambda_2(\\mathcal G^{-v}_\\text{org}). \\end{equation} Equation \\eref{eq1} implies that highly critical nodes are not reliable. If $C_v > C_{v'}$, node $v$ has higher reliability than node $v'$. Since UAVs represent the backhaul core of the considered uplink system, they are the most critical nodes that can severely impact the network connectivity if they fail. Thus, we consider the reliability of the UAVs only."}
{"input": "1, ~~~~~~~~~~~~~~\\forall a \\in \\{1, 2, \\dots, A\\},\\\\ &\\text{C$_5$:} ~~~~\\sum_{r \\in \\mathcal R} \\sum_{a \\in \\mathcal A} x^{(r)}_{a} \\leq R, \\\\ &\\text{C$_6$:} ~~~~~\\sum_{a \\in \\mathcal A} x^{(r)}_{a} z^{(r)}_u \\leq 1, ~~~~~~~~~~~\\forall r \\in \\mathcal R, \\forall u \\in \\mathcal U_r,\\\\ &\\text{C$_7$:} ~~~~~\\gamma^{(r)}_{u,a}(\\mathbf Z, \\mathbf X, \\boldsymbol{\\alpha}_r)\\geq C_a\\gamma^\\text{RIS}_\\text{th}, ~~~~~~~~~\\forall (u,a,r),\\\\ &\\text{C$_8$:}~~~~~\\sum_{u=1}^{U_r}\\alpha^{a}_{u,r} z^{(r)}_ux^{(r)}_{a} \\leq 1, ~~~~~~\\forall r \\in \\mathcal R, \\forall u \\in \\mathcal U_r, \\\\ &\\text{C$_9$:}~~~~~ 0 \\preceq \\boldsymbol{\\alpha}_r \\preceq 1, ~~~~~~~~~~~~~~\\forall r \\in \\{1, 2, \\dots, R\\},\\\\ & ~~~~~~~~~~ x^{(r)}_{k}, z^{(r)}_u \\in \\{0,1\\}, ~\\forall u \\in \\mathcal U_r, ~~r \\in \\mathcal R, a \\in \\mathcal A, \\end{align} \\end{subequations} where $\\preceq$ is the pairwise inequality and $\\boldsymbol{\\alpha}=[\\boldsymbol{\\alpha}_r, \\ldots, \\boldsymbol{\\alpha}_R]$. In $\\mathcal P$, \\text{C$_1$} shows that the transmitting UE$_u$ is connected to only one RIS; \\text{C$_2$} ensures that at most $U_r$ connections exist between the UEs and each RIS. Thus, \\text{C$_1$} and \\text{C$_2$} make \\text{C$_3$} with at most $U_rR$ links are created between the transmitting UEs and the RISs. \\text{C$_4$} implies that UAV$_a$ is connected to only one RIS, and accordingly, \\text{C$_5$} implies that at most $R$ reflected links should be created for the UAVs via the RISs. \\text{C$_6$} means that a complete path can be created from a selected UE to the suitable UAV through one RIS. \\text{C$_7$} states that the SINR of a typical $\\text{UE}_u \\xrightarrow{\\text{RIS}_r} \\text{UAV}_{a}$ link should be greater than or equal to the minimum SINR threshold of that link times the reliability value of the corresponding UAV, constituting the QoS constraint on UAV$_a$ based on its reliability."}
{"input": "for a given value of $l$. For simplicity, we use the term $\\omega^o_l$ instead of $\\gamma^{(r)}_{u,a}(\\boldsymbol{\\alpha}_r)$ unless when it creates confusion. In this paper, we are interested in maximizing network connectivity by solving $\\mathcal P$ under the max-sum link quality, where we maximize the weights of the added $\\text{UE}_u \\xrightarrow{\\text{RIS}_r} \\text{UAV}_{a}$ links. By using \\eref{L'}, we get an equivalent objective function of $\\mathcal P$ as follows $\\lambda_2(\\mathbf L_\\text{mod} (\\boldsymbol{\\alpha}, \\mathbf Z, \\mathbf X))= \\lambda_2( \\mathbf L_\\text{org}+ \\sum_{\\substack{r\\in \\mathcal R}} \\sum_{\\substack{a\\in \\mathcal A}}\\sum_{u\\in \\mathcal U_r}z^{(r)}_ux^{(r)}_{a} \\omega^o_{u,a}(\\boldsymbol{\\alpha}) \\mathbf a_{u,a} \\mathbf a^T_{u,a})$. Since $\\mathbf L_\\text{org}$ of the original network graph is fixed and network connectivity is a monotonically increasing function of the added links and their weights \\cite{new}, \\cite{saifglobecom_E}, we can maximize the network connectivity by selecting the $\\text{UE}_u \\xrightarrow{\\text{RIS}_r} \\text{UAV}_{a}$ links and maximizing their sum SINR. Subsequently, $\\mathcal P$, that aims at the joint optimization of RIS-aided link selection (i.e., UE-RIS-UAV clustering) and RIS partitioning to maximize the connectivity via maximizing the sum SINR of the new $\\text{UE}_u \\xrightarrow{\\text{RIS}_r} \\text{UAV}_{a}$ links, can be given by \\begin{subequations} \\nonumber \\label{eq10} \\begin{align} & \\mathcal P^{\\text{sum}}: \\max_{\\boldsymbol{\\alpha}, \\mathbf Z, \\mathbf X} ~~ \\lambda_2\\bigg(\\mathbf L_\\text{org} + \\sum_{\\substack{r\\in \\mathcal R \\\\ u\\in \\mathcal U_r}} \\sum_{\\substack{a\\in \\mathcal A}} z^{(r)}_ux^{(r)}_{a} \\omega^o_{u,a}(\\boldsymbol{\\alpha}) \\mathbf a_{u,a} \\mathbf a^T_{u,a}\\bigg) \\label{eq10a}\\\\ &~~~~~~~{\\rm s.~t.\\ } ~~~~~ \\text{C$_1$} - \\text{C$_9$}, \\end{align} \\end{subequations} which uses all constraints of $\\mathcal P$. In this paper, we propose a novel optimization technique to maximize network connectivity."}
{"input": "subproblems: \\begin{itemize} \\item Given the initial RIS partitioning $\\boldsymbol{\\alpha^0}$, $\\mathcal P^\\text{sum}$ can be solved with the UE NOMA and UAV clustering variables $\\mathbf Z$ and $\\mathbf X$, which is still MILNP. Thus, we propose an innovative procedure of two types of clustering: RIS-UAV clustering, denoted as $\\mathcal C$ and UE-$\\mathcal C$ clustering. The clustering procedure is presented in \\sref{Link}, and its pseudo code is provided in Algorithm \\ref{Algorithm1}. \\item Given the formed clustering $\\mathbf Z$ and $\\mathbf X$, we focus on RIS partitioning $\\boldsymbol{\\alpha}$, which is derived in a closed-form in Proposition $1$. A detailed step-by-step closed-form solution for the RIS partitioning is presented in \\sref{RIS}. \\end{itemize} The proposed iterative solution have two procedures, summarized in Algorithm \\ref{alg_all}, that run as follows: (i) RIS-aided link selection and (ii) RIS partitioning design. These two procedures are executed seamlessly until convergence in a centralized manner at the BS. The BS collects the CSI of the system to perform the RIS-aided link selection and adjusts the phase shifts of the RIS elements. Then, it optimizes the RIS partitions. In the next time slot, these two procedures are re-executed for different network topologies. \\begin{algorithm}[t!"}
{"input": "\\section{RIS-aided Link Selection: UE-RIS-UAV Clustering} \\label{Link} Given RIS partitioning $\\boldsymbol{\\alpha^0}$, the subproblem for updating $\\mathbf Z$ and $\\mathbf X$ is given by \\begin{subequations} \\nonumber \\label{eq10} \\begin{align} &\\mathcal P^\\text{sum}_1: \\max_{\\mathbf Z, \\mathbf X} ~~ \\lambda_2\\bigg(\\mathbf L_\\text{org} + \\sum_{\\substack{r\\in \\mathcal R}} \\sum_{\\substack{u \\in \\mathcal U_r}} \\sum_{a\\in \\mathcal A} z_u^{(r)} x_a^{(r)} \\omega^o_{u,a} \\mathbf a_{u,a} \\mathbf a^T_{u,a}\\bigg) \\label{eq10a}\\\\ &~~~~~~~{\\rm s.~t.\\ } ~~~~~ \\text{C$_1$} - \\text{C$_7$}. \\end{align} \\end{subequations} This section solves $\\mathcal P^\\text{sum}_1$ to optimize $\\mathbf Z$ and $\\mathbf X$ through an innovative clustering solution. Specifically, we propose two procedures of clustering: RIS-UAV clustering and NOMA UE clustering with size $U_r$, which are presented in Algorithm $\\ref{Algorithm1}$ and presented as follows. %\\subsection{UE NOMA Clustering} \\label{Clustering} \\textbf{UAV-RIS clustering:} First, we assign the most reliable UAVs to the RISs $\\mathcal R$, where each RIS cluster has at most one reliable UAV. We sort the UAVs in an ascending order based on their reliability $C_a, \\forall a \\in \\mathcal A$. This sorted UAV index set is denoted as $\\Tilde{\\mathcal A}$. Afterwards, the first $R$ UAVs in the set $\\Tilde{\\mathcal A}$ are associated with the closest RISs. The formed UAV-RIS clusters are denoted as $\\mathcal {C}=\\{\\mathcal C_1, \\ldots, \\mathcal C_R\\}$, where each cluster refers to one RIS and one reliable UAV, i.e., $x^{(r)}_a=1, \\forall (r,a)$. For simplicity, each cluster is indexed by RIS$_r, \\forall r$, and we omit the index $a$ hereafter unless when it creates confusion."}
{"input": "possible transmitting UEs are associated with one cluster. First, we sort the UEs in an ascending order based on the RSS of their direct links to the UAVs. This sorted UE index set is denoted as $\\Tilde{\\mathcal U}$. Such sorted set is crucial to maximize network connectivity and ensure that UEs with weak connections remain connected, thereby avoiding zero connectivity. Specifically, UEs with weak or no direct links to UAVs are prioritized to be clustered to the RISs. As such, we provide alternative transmission paths for UEs with weak channels to UAVs via RISs, enhancing their ability to transmit signals effectively. This sorted UE index set plays a crucial role, where it is employed to partition the set of the first $RU_r$ weakly connected UEs into $U_r$ subsets labeled as $\\mathtt U_i=\\Tilde{\\mathcal U}\\{(i-1)R+1: iR\\}, i \\in [1, U_r]$. In Line $16$, we initialize the clusters. Specifically, $\\mathcal C_r$ is initialized using the $r$-th element extracted from the first subset of $\\mathtt U_1$, $\\forall r\\in \\{1, \\ldots, R\\}$, i.e., $\\mathcal C_r=\\mathtt U_1\\{ r\\}$. Following, the UEs are iteratively admitted to NOMA clusters as illustrated in the subsequent process: (1) Line $18$ updates the cluster by temporarily admitting the waited UEs and (2) Line $19$ calls the utility matrix procedure to compute the utility matrix $\\mathbf O_i$ for the $i$-th round of UE admission. The utility matrix function $\\mathbf O_i$ is calculated using nested ``for loops'' between Lines $27$ and $33$."}
{"input": "UAV-RIS Clustering $(\\mathcal R, \\mathcal A)$} \\State Compute the distances between RISs and UAVs \\State $\\mathcal A_0 \\leftarrow{}~ \\{a\\in \\mathcal A ~|~ d_{r,a} \\leq R^\\text{RA}_\\text{RIS}, \\forall r\\}$ \\State $\\Tilde{\\mathcal A} \\leftarrow{}~ \\text{SortAscend}(C_a, \\forall a \\in \\mathcal A_0)$ //\\textit{UAV ordering based on their reliability} \\State $\\Tilde{\\mathcal A}_0= \\Tilde{\\mathcal A}\\{1:R\\}$ \\For{$r=1: R$} \\State $\\mathcal C_r \\leftarrow{} \\bigg(\\Tilde{\\mathcal A}_0(r), \\text{Closest RIS}\\bigg)$ \\State $\\mathcal C \\leftarrow \\mathcal C \\cup \\mathcal C_r$ \\EndFor %\\algrule \\State \\textbf{Step 2: UE NOMA Clustering $(\\mathcal C, \\mathcal U)$} \\State Compute the distances between UEs and RISs \\State $\\mathcal U_0 \\leftarrow{}~ \\{u\\in \\mathcal U ~|~ d_{u,r} \\leq R^\\text{UR}_\\text{RIS}, \\forall r\\}$ \\State $\\Tilde{\\mathcal U} \\leftarrow{}~ \\text{SortDescend}(\\text{RSS}, \\forall u \\in \\mathcal U)$ //\\textit{UE ordering based on their RSS to the UAVs} \\State $\\mathtt U_i \\leftarrow \\Tilde{\\mathcal U}\\{(i-1)R+1: iR\\}, i \\in [1, U_r]$ //Form UE NOMA clusters \\State $\\mathcal C_r \\leftarrow \\mathtt U_1\\{r\\}$, $\\forall r=\\{1, \\ldots, R\\}$ //Initialize UE NOMA clusters \\For{$i=1:U_r-1$} \\State $\\mathcal I_i \\leftarrow \\mathtt U_{i+1}$ \\State $\\mathbf O_i \\leftarrow \\text{Utility Matrix UE}(\\mathcal C_r, \\mathcal I_i)$ //Create utility matrix for UE NOMA clustering \\State $\\mathbf B^i_\\text{sum} \\leftarrow \\mathbf O_i$ \\State $\\mathcal C_r \\leftarrow \\mathcal C_r \\cup \\mathcal I_i\\{u\\}$, $y_u^{(r)}=1, \\forall (r,u)$~//Update clusters \\State $z^{(r)}_{u} \\leftarrow 1$, $\\forall u \\in \\mathcal C_r$ \\EndFor \\State \\textbf{return} $\\mathbf Z$, $\\mathbf X$, and $\\mathcal C_r, \\forall r$ \\algrule \\State $\\textbf{Utility Matrix UE}(\\mathcal C_r, \\mathcal I_i)$ \\State $\\mathbf O_i \\leftarrow \\boldsymbol{0}^{R\\times I_i}$ \\For{$r=1:r$} \\For{$u=1:I_i$} \\State $\\mathcal Q_r \\leftarrow \\mathcal C_r \\cup \\mathcal I_i\\{u\\}$ //\\textit{Temp."}
{"input": "\\section{Optimal RIS Partitioning} \\label{RIS} %In NOMA schemes, UAVs utilize SIC to decode signals transmitted by UEs in descending order of received signal power. Traditionally, the grant-based NOMA scheme ensures that the UE with the highest channel gain is assigned the highest transmit power, enabling other cluster members to create interference and improve their SINRs. %This section proposes two novel transmission schemes at the UEs' side while fine-tuning the received power at the UAVs through the optimization of RIS partitions. %\\subsection{RIS Partitioning} We maximize the sum SINR of all the RIS-aided links within NOMA cluster under the given SINR constraint and RIS elements allocation. Therefore, given $\\mathbf Z$ and $\\mathbf X$, the problem of optimal RIS partitioning to maximize the sum SINR of the RIS-aided links within the NOMA cluster can be formulated as \\begin{subequations} \\nonumber \\label{sum-rate} \\begin{align} &\\mathcal P^\\text{sum}_2: ~\\max_{\\boldsymbol{\\alpha_r}} ~~~~ \\sum_{u=1}^{U_r}\\gamma^{(r)}_{u,a}(\\alpha^{a}_{u,r}) \\label{eq10a}\\\\ &~~~~~~~{\\rm s.~t.\\ }\\\\ &\\text{C$_1^2$:} ~~~~~\\gamma^{(r)}_{u,a}(\\alpha^{a}_{u,r})\\geq C_a\\gamma^\\text{RIS}_\\text{th}, ~~~~~~~u \\in \\{1, \\ldots, U_r\\},\\\\ & \\text{C$_2^2$:} ~~~~~ 0 \\leq \\alpha^{a}_{u,r} \\leq 1, ~~~~~~~~~~~~~~~~u \\in \\{1,\\ldots, U_r\\},\\\\ &\\text{C$_3^2$:}~~~~~\\sum_{u=1}^{U_r}\\alpha^{a}_{u,r} \\leq 1. ~~~~~~~ \\end{align} \\end{subequations} The closed form solution $\\alpha^{a^*}_{u,r}, u \\in \\{1, \\ldots, U_r\\}$ of the partitions of RIS$_r$ is provided in the following Proposition. \\begin{proposition} \\label{pro1} The optimal RIS$_r$ partitioning that provides the maximized network connectivity via maximizing the sum SINR for the RIS$_r$ cluster is given by \\begin{align}\\label{closed} \\alpha^{a*}_{u,r} = \\begin{cases} \\sqrt{\\left [\\frac{C_a \\gamma^\\text{RIS}_\\text{th}\\sum\\limits_{\\substack{u\\in \\mathcal U_r \\\\ u'\\neq u}}\\bigg(\\Tilde{\\gamma}_{u'}+(\\alpha^{a^*}_{u',r})^2 B+1\\bigg)-\\Tilde{\\gamma}_u}{A} \\right]^+} & \\text{if}~ u \\neq 1,\\\\ 1-\\sum_{u'=u+1}^{U_r}\\alpha^{a^*}_{u',r} & \\text{if}~ u=1, \\end{cases} \\end{align} where $A=K^2m\\hat{\\gamma}_{u}$, $B=K^2m\\hat{\\gamma}_{u'}$, and $[x]^+=\\max\\{x,0\\}$."}
{"input": "maximize the sum SINR of the set RIS-aided links $\\text{UE}_u \\xrightarrow{\\text{RIS}_r} \\text{UAV}_{a}$ of RIS$_r$ given \\text{C$_2^2$} and \\text{C$_3^2$} within the constraint of the minimum SINR requirement of each UE. Our allocation scheme is to allocate RIS elements to all UEs except for UE with the highest direct UE-UAV link quality. The SINR of the RIS-aided links of those UEs are just equal to their minimum required SINR. All the remaining RIS elements will be allocated to the UE with the highest direct UE-UAV link quality. %In fact, the UE with the maximum channel gain contributes the most to the sum-rate of a NOMA cluster; thus, it should be allocated the maximum number of elements. For simplicity of the analysis, we explain the steps of the closed-form solution through an example of $4$ UEs (i.e., $U_r=4$) with a reverse chronological order, e.g., \\cite{NOMA-1, optimal}. Suppose that $\\Tilde{\\gamma}_1> \\Tilde{\\gamma}_2>\\Tilde{\\gamma}_3 > \\Tilde{\\gamma}_4$, i.e., UE$_1$ is the UE with the highest channel gain, where its SINR is given in \\eqref{approG}. \\\\ \\textbf{UE$_4$:} If $\\Tilde{\\gamma}_4 \\geq C_a \\gamma^\\text{RIS}_\\text{th}$, $\\alpha^{a^*}_{4,r} =0$, otherwise $\\Tilde{\\gamma}_4+(\\alpha^{a}_{4,r})^2 K^2m\\hat{\\gamma}_4=C_a \\gamma^\\text{RIS}_\\text{th}$. By performing some manipulations, we have $\\alpha^{a*}_{4,r}= \\sqrt{\\frac{C_a \\gamma^\\text{RIS}_\\text{th}-\\Tilde{\\gamma}_4}{K^2m\\hat{\\gamma}_4}}$.\\\\ \\textbf{UE$_3$:} If $\\Tilde{\\gamma}_3 \\geq C_a \\gamma^\\text{RIS}_\\text{th}$, $\\alpha^{a^*}_{3,r} =0$, otherwise $\\frac{\\Tilde{\\gamma}_3+(\\alpha^{a}_{3,r})^2 K^2m\\hat{\\gamma}_3}{\\Tilde{\\gamma}_{4}+(\\alpha^{a^*}_{4,r})^2 K^2m\\hat{\\gamma}_{4}+1}=C_a \\gamma^\\text{RIS}_\\text{th}$. By performing some manipulations, we have $\\alpha^{a*}_{3,r}= \\sqrt{\\frac{C_a \\gamma^\\text{RIS}_\\text{th}\\bigg[ \\Tilde{\\gamma}_{4}+(\\alpha^{a^*}_{4,r})^2 K^2m\\hat{\\gamma}_{4}+1\\bigg]-\\Tilde{\\gamma}_3}{K^2m\\hat{\\gamma}_3}}$. \\\\ \\textbf{UE$_2$:} If $\\Tilde{\\gamma}_2 \\geq C_a \\gamma^\\text{RIS}_\\text{th}$, $\\alpha^{a^*}_{2,r} =0$, otherwise $\\frac{\\Tilde{\\gamma}_2+(\\alpha^{a}_{2,r})^2 K^2m\\hat{\\gamma}_2}{\\Tilde{\\gamma}_{3}+(\\alpha^{a^*}_{3,r})^2 K^2m\\hat{\\gamma}_{3}+\\Tilde{\\gamma}_{4}+(\\alpha^{a^*}_{4,r})^2 K^2m\\hat{\\gamma}_{4}+1}=C_a \\gamma^\\text{RIS}_\\text{th}$. By performing some manipulations, we have $\\alpha^{a*}_{2,r}= \\sqrt{\\frac{C_a \\gamma^\\text{RIS}_\\text{th}\\sum_{u=3,4}\\bigg[ \\Tilde{\\gamma}_{u}+(\\alpha^{a^*}_{u,r})^2 K^2m\\hat{\\gamma}_{u}+1\\bigg]-\\Tilde{\\gamma}_2}{K^2m\\hat{\\gamma}_2}}$.\\\\ \\textbf{UE$_1$:} If $\\Tilde{\\gamma}_1 \\geq C_a \\gamma^\\text{RIS}_\\text{th}$, $\\alpha^{a^*}_{1,r} =0$, otherwise $\\alpha^{a^*}_{1,r}=1-\\alpha^{a^*}_{2,r}-\\alpha^{a^*}_{3,r}-\\alpha^{a^*}_{4,r}$."}
{"input": "enhance connectivity once the network is well-connected, especially when the connections between the nodes are already dense. %%%%%% \\begin{figure}[t!] \\begin{center} \\includegraphics[width=0.99\\linewidth, draft=false]{Fig5} \\caption{Network connectivity versus number of UAVs $A$ for $U=15$, $U_r=3$, $K=200$, and $R=3$.} \\label{fig5} \\end{center} \\end{figure} \\begin{figure}[t!] \\begin{center} \\includegraphics[width=0.99\\linewidth, draft=false]{Fig6} \\caption{Network connectivity versus number of UEs $U$ for $A=8$, $U_r=3$, $K=200$, and $R=3$.} \\label{fig6} \\end{center} \\end{figure} In Figs. \\ref{fig5} and \\ref{fig6}, we show the connectivity performance versus the number of UAVs $A$ and the number of UEs $U$, respectively. The numbers of RIS elements, RIS-aided links, and RISs are fixed to $200$, $3$, and $3$, respectively, while the number of UAVs $A$ is varied in the range of $[6, 14]$ with $U=15$ in Fig.~\\ref{fig5} and the number of UEs $U$ is varied in the range of $[10, 20]$ with $A=8$ in Fig.~\\ref{fig6}. From both figures, we can see that RIS-aided NOMA consistently outperforms the other schemes. As the number of UAVs increases in Fig. \\ref{fig5}, the connectivity of all schemes significantly improves. This is because most of $\\text{UAV}_a \\rightarrow\\text{UAV}_{a'}$ link connections satisfy the $\\gamma^{\\text{UAV}}_0$, and adding more UAV nodes creates additional UAV-UAV and UE-UAV edges in $\\mathcal G_\\text{org}$. The enhanced connectivity as a result of the increased number of UAVs facilitates more robust and reliable network connections. On the other hand, the connectivity performance of Fig. \\ref{fig6} is slightly degraded for all the schemes, indicating that adding more UEs does not necessarily add more edges to the graph, as there are no direct connections between the UEs. \\begin{figure}[t!"}
{"input": "significantly reduces network connectivity. Consequently, when $\\mathtt A=10$, the traditional UAV network becomes disconnected, thus has zero connectivity. The designed network, resulting from our proposed scheme, is more resilient against UAV failure, achieving zero connectivity at $\\mathtt A=16$, thanks to NOMA UE clustering, RIS deployment and partitioning. We observe the same behavior for the well-connected network, but with a slower decrease in network connectivity to zero, i.e., at $\\mathtt A=16$ for traditional UAV network and $\\mathtt A>18$ for RIS-aided NOMA. \\begin{figure}[t!] \\begin{center} \\includegraphics[width=0.99\\linewidth, draft=false]{Fig9} \\caption{Network connectivity versus $\\sigma^2_e$ for $A=8$, $U_r=2$, $U=15$, $K=200$, and $R=3$.} \\label{fig8} \\end{center} \\end{figure} Next, we discuss the impact of imperfect CSI on the performance of the proposed RIS-aided NOMA scheme. We consider the imperfect CSI model in \\cite{Imp3, Imp4} as follows. Generally, the channel gain for the $i-j$ link can be modeled as $g_{i,j}=\\sqrt{\\beta_{i,j}}h_{i,j}$, where $h_{i,j}$ represents the small-scale fading and is expressed as $h_{i,j}=\\hat{h}_{i,j}+h_{e, ij}$; where $\\hat{h}_{i,j}$ is the estimated channel gain that also follows the Nakagami-$f$ distribution with the corresponding parameters of the links UE-RIS, RIS-UAV, and UE-UAV, $h_{e, ij} \\sim \\mathcal {CN}(0, \\sigma^2_{e, ij})$ denotes the estimation error, and $\\beta_{i,j}$ is the large-scale model. Specifically, we denote the end-to-end CSI error of $\\text{UE}_u \\rightarrow \\text{UAV}_a$ and $\\text{UE}_u \\xrightarrow{\\text{RIS}_r} \\text{UAV}_{a}$ links by $\\sigma^2_{e, ua}$ and $\\sigma^2_{e, ura}$, respectively. Referring to Section II, $\\beta_{i,j}=\\beta^\\text{UK}_{u,a}$ for the $\\text{UE}_u \\rightarrow \\text{UAV}_a$ link, $\\beta_{i,j}=\\Tilde{G}_{u,r}$ for $\\text{UE}_u \\rightarrow \\text{RIS}_r$ link, and $\\beta_{i,j}=\\Tilde{H}_{r,a}$ for the $\\text{RIS}_r \\rightarrow \\text{UAV}_a$ link."}
{"input": "\\rightarrow \\text{RIS}_r$ link with the $k$-th RIS element, and $\\hat{h}_{i,j}=h_{r,a}^{(k)}$ for $\\text{RIS}_r \\rightarrow \\text{UAV}_a$ link with the $k$-th RIS element. Therefore, the SINR at UAV$_a$ can be expressed as $\\gamma^{(r)}_{u,a}(\\boldsymbol{\\alpha}_r)= \\frac{\\Tilde{\\gamma}_u+(\\alpha^{a}_{u,r})^2 K^2m\\hat{\\gamma}_u}{\\sum\\limits_{\\substack{u\\in \\mathcal U_r \\\\ u'\\neq u}}\\bigg(\\Tilde{\\gamma}_{u'}+(\\alpha^{a}_{u',r})^2 K^2m\\hat{\\gamma}_{u'}\\bigg)+1+\\sigma^2_{e, ua}\\Tilde{\\gamma}_u+\\sigma^2_{e, ura}\\hat{\\gamma}_u}$ Fig.~\\ref{fig8} illustrates the impact of imperfect CSI coefficients $\\sigma^2_e$ on network connectivity. The scenario considered includes $A=8$, $U_r=2$, $U=15$, $K=200$, $R=3$, and $\\beta_0=-20$ dB, with the imperfection model and related values taken from the literature \\cite{Imp1, Imp2}. We assume $\\sigma^2_{e, ua}=\\sigma^2_{e, ura}=\\sigma^2_{e}$, and present the network connectivity for both perfect CSI ($\\sigma^2_{e}=0$) and imperfect CSI as a function of $\\sigma^2_{e}$. As can be seen in the figure, imperfect CSI introduces some performance degradation compared to the perfect CSI case, however, the overall impact remains moderate until $\\sigma^2_e >10^0$, where a more noticeable impact is observed. In particular, the influence of imperfect CSI mainly affects the SINR of the RIS-aided links, which contributes to degradation of network connectivity. However, since network connectivity is influenced not only by the SINRs of the RIS-assisted links but also by the overall graph structure — including NOMA cluster formation, the reliable scheduling of UAVs, and the addition of RIS-aided links — the impact of CSI errors on connectivity remains moderate. \\begin{table*}[t!] \\renewcommand{\\arraystretch}{0.9} \\caption{Execution time (in seconds) and performance of the simulated schemes for varying numbers of nodes.} \\label{table_2} \\centering \\begin{tabular}{|c||cc|cc|cc|cc|} \\hline \\multirow{3}{*}{\\textbf{Schemes}} & \\multicolumn{8}{c|}{\\textbf{Number of Nodes (UEs \\& UAVs)}} \\\\ \\cline{2-9} & \\multicolumn{2}{c|}{\\textbf{25}} & \\multicolumn{2}{c|}{\\textbf{35}} & \\multicolumn{2}{c|}{\\textbf{45}} & \\multicolumn{2}{c|}{\\textbf{55}} \\\\ \\cline{2-9} & \\textbf{Time} & \\textbf{Perf."}
{"input": "& \\textbf{Time} & \\textbf{Perf.} & \\textbf{Time} & \\textbf{Perf.} & \\textbf{Time} & \\textbf{Perf.} \\\\ \\hline \\hline RIS-aided NOMA & $0.148626$ & $30.0889$ & $0.203271$ & $68.9734$ & $0.223271$ & $88.9734$ & $0.253271$ & $100.4974$ \\\\ Convex Optimization & $2.570838$ & $25.6707$ & $5.901637$ & $60.4974$ & $10.329478$ & $78.4974$ & $15.329478$ & $86.4974$ \\\\ Single Large RIS & $0.030709$ & $22.3508$ & $0.064330$ & $58.5885$ & $0.038614$ & $72.4974$ & $0.048614$ & $78.4974$ \\\\ Traditional UAV & $0.030534$ & $19.5680$ & $0.034065$ & $55.6054$ & $0.032308$ & $65.6054$ & $0.034308$ & $75.6054$ \\\\ \\hline \\end{tabular} \\end{table*} Finally, \\tref{table_2} studies the run time (in seconds for one algorithm iteration) of MATLAB for all the schemes and their corresponding connectivity performance. The simulations to calculate the running time are carried in MATLAB on a macOS 12.5.1 iMac Apple M1 chip and 8 GB memory. We consider a network setup of $3$ RISs with different number of nodes (UEs and UAVs). \\tref{table_2} shows that the convex scheme requires high computation time than all the other solutions. This is due to the fact that this scheme solves the SDP optimization of all candidate RIS-aided links in the network. On the other hand, our proposed scheme is efficient in terms of low computation time and good network connectivity performance. Thus, our proposed scheme can be executed quickly, making it a preferred method for application in UAV networks."}
{"input": "\\section{Introduction} \\label{sec-Introduction} The operation which associates to any pair of categories $\\A$ and $\\B$ a category $(\\A,\\B)$ which came to be known by the name \"comma category\" was introduced by Lawvere in his thesis \\cite{Law0} for the purpose of a foundational clarification, in particular of the notion of {\\it adjointness}. Since then the comma construction turned out to be fundamental in many areas of category theory from computing Kan extensions to the general calculus of adjoints and limits. Based on his observation from 1963 that cartesian closed categories serve as a common abstraction of type theory and propositional logic, Lawvere used the comma construction in his paper \\cite{Law3} in which he summed up a stage of the development of the relationship between category theory and proof theory based on the strategy to interpret proofs themselves as structures, following idea which goes back to Kreisel and Kleene. In that paper he introduced a notion of a \\emph{hyperdoctrine} consisting of a category $\\T$ of \\emph{types} and a functor \\[\\Pe \\maps \\T^{op} \\to \\Cats \\] which served as a main device in his subsequent influential paper \\cite{Law4} to to find an analogue in category theory to the comprehension scheme of set theory which says, essentially, that given a property, there is a set consisting exactly of the elements having that property. Lawvere translated this statement into an adjunction \\[\\xymatrix{(\\Set,X) \\ar@<-1.2ex>[r]_{}^{\\perp} & \\Pe(X) \\ar@<-1.2ex>[l]_{}} \\] where the comma category on the left consists of all functions into the fixed set $X$."}
{"input": "the adjunction to the case of the presheaf hyperdoctrine \\[\\xymatrix{(\\Cat,\\X) \\ar@<-1.2ex>[r]_{}^{\\perp} & \\Pe(\\X) \\ar@<-1.2ex>[l]_{}} \\] where $\\X$ is a category and $\\Pe(\\X)$ is a category of presheaves on $\\X$ and a functor from right to left is given by a Grothendieck construction assigning a (discrete) fibration to any presheaf. A further step was done by Gray in \\cite{Gray2} based on his reformulation \\cite{Gray1} of Grothendieck's theory of (co)fibrations \\cite{Gr2} in order to discuss what it would mean for an adjunction \\[\\xymatrix{(\\Cat,\\X) \\ar@<-1.2ex>[r]_{}^-{\\perp} & \\Cats[\\X^{op},\\Cat] \\ar@<-1.2ex>[l]_{}} \\] to be an instance of the comprehension scheme. Gray's answer is that it is not, and the reason is that in this context the comprehension scheme is equivalent to asserting \\[{\\displaystyle \\lim_{\\to }}(\\tau_{\\B}) = \\B \\] where $\\tau_{\\B} \\maps \\B \\to \\bullet$ is a constant functor into the terminal category $\\bullet$ in $\\Cat$, and this assertion is false unless $\\B$ is discrete. Gray realized that the difficulty lied in applying the above notion of a colimit in a traditional 1-dimensional sense which is not suitable for $\\Cat$ being intrinsically a 2-category. Therefore he started developing the theory of functors, comma categories, adjointness and limits in a 2-dimensional context culminating in a milestone monograph \\cite{Gray} which meant to be the first part of a projected four volumes, but its subsequent parts never appeared (as well as \"The calculus of comma categories\" which he promised in the bibliography of \\cite{Gray2})."}
{"input": "The purpose of category theory is to try to describe certain general aspects of the structure of mathematics. Since category theory is also part of mathematics, this categorical type of description should apply to it as well as to other parts of mathematics. \\end{quote} Already until the publication of \\cite{Gray} there has been a considerable amount of work on various aspects of this study by various authors, most notably by B\\' enabou \\cite{Be1}, Eilenberg and Kelly \\cite{EK} and \\cite{Kelly4}, Linton \\cite{Lin} and Street \\cite{Street1} and \\cite{Street2} which was directly concerned with, or at least, relevant to it. The theory of adjoint squares which is the subject of the last chapter of \\cite{Gray} was previously studied by Palmquist \\cite{Palm} and Maranda \\cite{Mara} in the context of categories enriched over the closed monoidal category in the sense of \\cite{EK}. One of the purposes of this paper is to give some conceptual clarifications of the theory of adjoint squares using a theory of 2-adjunctions and 2-comonads. Particularly, there exists a strict 2-adjunction \\[\\xymatrix{\\Cat^{2}_{c} \\ar@<-1.2ex>[r]_{D}^-{\\perp} & \\Cat \\ar@<-1.2ex>[l]_{I}} \\] where $\\Cat^{2}_{c}$ is the 2-category whose objects are functors and 1-cells are colax squares filled by upwards pointing natural transformations with an obvious notion of 2-cells between the latter. The 2-functor $D \\maps \\Cat^{2}_{c} \\to \\Cat$ sends any functor $U \\maps \\A \\to \\X$ to its comma category $(\\X,U)$ and $I \\maps \\Cat \\to \\Cat^{2}_{c}$ is an obvious 2-functor which sends any category $\\X$ to identity functor $I_{\\X} \\maps \\X \\to \\X$."}
{"input": "of formal category theory it is very important that this 2-adjunction does not require any further restrictions on categories or functors whatsoever. This point of view is very much in the spirit of B\\' enabou's philosophy of regarding functors as \\emph{generalized fibrations}. As any other, this 2-adjunction generates a 2-comonad $\\D$ on $\\Cat^{2}_{c}$ and the main contribution of this paper is a description of colax $\\D$-coalgebras and investigation of its properties. In the light of the aforementioned line of thought it should not come as a surprise that these complex mathematical structures are lurking behind many important constructions in formal category theory, most notably in the \\emph{formal theory of monads} which Street developed in \\cite{Street1} followed by its sequel with Lack in \\cite{LSt} and based on B\\' enabou's realization that the definition of a monad can be made in an arbitrary bicategory (which was actually the main reason why B\\' enabou insisted on incorporating \\emph{morphisms of bicategories} or \\emph{lax functors} as the central part of the theory of bicategories). Although Lack and Street claim in Remark 1.1. of \\cite{LSt} that it would be possible to develop the whole formal theory of monads in the context of general bicategories, the extension of their theory from 2-categories is not starightforward as it seems. The reason lies in the concluding remark behind Proposition I.6."}
{"input": "case of bicategories. The important attempt to develop such theory based on generalized operads was done by Chikhladze in \\cite{Chik} by means of \\emph{generalized multicategories} or \\emph{$T$-monoids} who defined as monads within a Kleisli bicategory. Motivated by investigation on functoriality of the Kleisli construction Chikhladze developed a lax version of the formal theory of monads, and studied its connection to bicategories. The theory of generalized multicategories itself has its early origins in the work of Burroni \\cite{Burr} but the more contemporary approaches mostly use the theory developed by Hermida in \\cite{Her1} (for the excellent overview of the state of affairs in the filed the diligent reader may consult Leinster's book \\cite{Le}). The point of departure from bicategories in the development of the theory was Cruttwell and Shulman's paper \\cite{CS} who recognized that generalized multicategories have been defined in numerous contexts throughout the literature as the \\emph{lax algebras} or \\emph{Kleisli monoids} relative to a pseudomonad on a bicategory but they questioned the meanings of these constructions from author to author. They proposed a unified framework for generalized multicategories by means of \\emph{virtual double categories} contenting themselves by working with monads on double categories and related structures rather than bicategories. An important role in their theory is played by normal oplax $T$-algebras where $T$ is a normal monad on a virtual equipment. In the statement of Theorem 9.13."}
{"input": "normal oplax T-algebras, with a remark immediately after the theorem that the restriction to normal oplax algebras in its final statement cannot be dispensed with either. Based on this result Shulman introduced in \\cite{Shu} so called \\emph{twisted $G$-actions} in order to define 2-categories with contravariance based on his axiomatic treatment of duality involutions of categories. The purpose was to treat both covariant and contravariant morphisms inside 2-categories and it is the author's opinion that this is one of the most important approaches to deal with such questions in categorical structures. However it is also a conviction of the author that the main reason why there is a proliferation of such unifying theories is caused by insufficient understanding of bicategories, in particular of adjunctions in bicategories and consequently higher adjunctions in Gray-categories and other higher categorical structures. This paper tends to bridge this gap by starting a program which might be called \\emph{formal theory of adjunctions} by paraphrasing Street's formal approach to monads. One of the most important discoveries of the author during the last few years is that the associated split fibration 2-monad $\\F$ whose pseudo-algebras are Grothendieck fibrations extends to the 2-category $\\Cat^{2}_{c}$ from the beginning of this paper and moreover it turns out that this 2-monad is admissible in the sense of Bunge and Funk \\cite{BF}. The comma constructions from the beginning plays a crucial role in several 2-(co)monads on the 2-category $\\Cat^{2}_{c}$ and its cousins."}
{"input": "of the study of this paper was investigated by Pavlovi\\' c in \\cite{Pav}. He studied three comonads derived from the comma construction and its induced coalgebras which correspond to the three main concepts of his paper: cofree equivalences, dualities and $\\ast$-autonomous categories.He showed that the comonad which yields the $\\ast$-autonomous categories is the Chu construction and then he proceeded to show that it is induced by the right adjoint to the inclusion of $\\ast$-autonomous categories among autonomous categories, with lax structure-preserving morphisms. Moreover, Pavlovi\\' c showed that this inclusion turns out to be comonadic: autonomous categories are exactly the Chu-coalgebras. This paper was one of the main motivations for the author to start an investigation of the comma 2-comonad in foundations. The other important source of inspiration for the author was Maltsiniotis paper \\cite{Maltsiniotis} whose impetus for his work was the observation that the class of fibrations shares many formal properties with the class of $\\W$-smooth functors where $\\W$ is a basic localizer in the sense of Grothendieck \\cite{Gr}. The crucial observation which we will address in the sequels ton this paper is his observation that the class of functors which have a right adjoint shares many properties with the class of $W$-aspheric functors. However this paper is not so ambitious as its introduction seem to suggest."}
{"input": "identities \\[\\xymatrix@!=0.5pc{&& D \\ar@2[dll]_-{\\n D} \\ar@{=}[drr]^-{} && \\\\ DID \\ar@2[rrrr]_-{D\\del} &&&& D} \\hspace{2cm} \\xymatrix@!=0.5pc{&& I \\ar@2[dll]_-{I\\n} \\ar@{=}[drr]^-{} && \\\\ IDI \\ar@2[rrrr]_-{\\del I} &&&& I }\\] says that for any two objects $G$ in $\\Cat^{2}_{c}$ and $\\E$ in $\\Cat$ the following diagrams \\[\\xymatrix@!=0.5pc{&& (\\X,G) \\ar@2[dll]_-{\\n_{(\\X,G)}} \\ar@{=}[drr]^-{} && \\\\ (\\X,G)^{2} \\ar@2[rrrr]_-{D(\\del_{G})} &&&& (\\X,G)} \\hspace{2cm} \\xymatrix@!=0.5pc{&& I_{\\E} \\ar@2[dll]_-{(\\n_{\\E},\\n_{\\E})} \\ar@{=}[drr]^-{} && \\\\ I_{\\E^{2}} \\ar@2[rrrr]_-{\\del_{I_{\\E}}} &&&& I_{\\E} }\\] commute in $\\Cat^{2}_{c}$ and $\\Cat$ respectively. From the universal property of $(\\X,G)$ \\[\\xymatrix@!=1.5pc@R1pc@C1pc{(\\X,G)^{2} \\ar[rrr]^{\\de_{0}} \\ar[ddd]_{\\de_{1}} \\ar[dr]^{D(\\de_{1},\\delta_{G},\\de_{0})} &&& (\\X,G) \\ar@{==}[ddd]^{} \\ar[dr]^{\\de_{0}} &\\\\ & (\\X,G) \\ar[rrr]^{\\de_{0}} \\ar[ddd]_{\\de_{1}} &&& \\A \\ar[ddd]^{G} \\\\ && \\ultwocell<\\omit>{\\delta_{I_{(\\X,G)}}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,} && \\\\ (\\X,G) \\ar[dr]_{\\de_{1}} \\ar@{==}[rrr]^{} &&& (\\X,G) \\ar@{-->}[dr]_(0.4){\\de_{1}} \\ultwocell<\\omit>{\\delta_{G}\\,\\,\\,\\,\\,} & \\uultwocell<\\omit>{\\delta_{G}} \\\\ & \\X \\ar@{=}[rrr]_{} &&& \\X }\\] for any object $[(X,f,A),(x,a),(X',f',A')]$ in $(\\X,G)^{2}$ we obtain an object of $(\\X,G)$ \\begin{equation}\\label{Daction} \\begin{array}{c} D(\\del_{G})[(X,f,A),(x,a),(X',f',A')] = \\\\ (\\de_{1}(X,f,A),\\delta_{G}(X',f',A')\\de_{1}(x,a),\\de_{0}(X',f',A'))=(X,f'x,A') \\end{array} \\end{equation} together with an action on morphisms in $(\\X,G)^{2}$ described by the following diagram \\begin{equation}\\label{Dmor} \\begin{array}{c} \\hspace{-1cm}\\xymatrix@!=0.1pc{X \\ar[rrr]^{h} \\ar[ddd]_{f} \\ar[dr]^{x} &&& Y \\ar@{-->}[ddd]^{g} \\ar[dr]^{y} &&& &&&\\\\ & X' \\ar[rrr]^{k} \\ar[ddd]_{f'} &&& Y' \\ar[ddd]^{g'} &&&& X \\ar[rrr]^{h} \\ar[ddd]_{f'x=G(a)f} &&& Y \\ar[ddd]^{g'y=G(e)g} \\\\ &&&&&&&&&&&\\\\ G(A) \\ar[dr]_{G(a)} \\ar@{-->}[rrr]^{G(m)} &&& G(E) \\ar@{-->}[dr]_(0.3){G(e)} && \\ar@{|->}[r]^{D(\\del_{G})} &&&&&& \\\\ & G(A') \\ar[rrr]_{G(n)} &&& G(E') &&&& G(A') \\ar[rrr]_{G(n)} &&& G(E') } \\end{array} \\end{equation} sending $[(h,m),(k,n)] \\maps [(X,f,A),(x,a),(X',f',A')] \\to [(Y,g,E),(y,e),(Y',g',E')]$ in $(\\X,G)^{2}$ to its diagonal $(h,n) \\maps (X,f'x,A') \\to (Y,g'y,E')$."}
{"input": "of (\\ref{DnIDA}) and (\\ref{Dthetachi}) \\begin{equation}\\label{DIDthetaAxihyperactionA} \\begin{array}{c} \\xymatrix@!=0.6pc@R1pc@C1.25pc{H(A) \\ar[rrrrrrrr]^-{\\theta^{100}_{H(A)}\\theta^{000}_{A}} \\ar[ddddddddd]_{\\chi_{A}} \\ar@{-->}[dddrrr]_(0.6){1_{H(A)}} \\ar[ddrrrr]^-{1_{H(A)}} &&&&&&&& CCH(A) \\ar[ddrrrr]^-{C(\\omega_{Q(A)}C(\\chi_{A}))} \\ar@{-->}[dddl]_{\\omega_{KH(A)}\\eta_{H(A)}} \\ar@{-->}[ddddddddd]^(0.6){\\eta_{H(A)}} &&&& \\\\ &&&&&&&&&& \\\\ &&&& H(A) \\ar[rrrrrrrr]^(0.6){\\theta^{000}_{Q(A)}\\theta^{001}_{A}} \\ar[ddddddddd]_{\\chi_{A}} \\ar@{-->}[ddr]^(0.75){1_{H(A)}} &&&&&&&& CHQ(A) \\ar[ddddddddd]^{\\eta_{H(A)}} \\ar@{-->}[ddlll]_{\\omega_{Q(A)}C(\\chi_{Q(A)})} \\\\ &&& H(A) \\ar@{-->}[dddd]_{\\chi_{A}} \\ar@{-->}[drr]_(0.4){1_{H(A)}} \\ar@{-->}[rrrr]^-{\\theta^{101}_{H(A)}\\theta^{000}_{A}} &&&& HKH(A) \\ar@{-->}[dddd]_{\\chi_{KH(A)}} \\ar@{-->}[drr]^(0.5){H(\\varepsilon_{Q(A)}K(\\chi_{Q(A)}))} &&&&& \\\\ &&&&& H(A) \\ar@{-->}[dddd]_{\\chi_{A}} \\ar@{-->}[rrrr]_-{\\theta^{001}_{Q(A)}\\theta^{001}_{A}} &&&& HQQ(A) \\ar@{-->}[dddd]^{\\chi_{QQ(A)}} &&&\\\\ &&&&&&&&&&&& \\\\ &&&&&&&&&&&& \\\\ &&& GQ(A) \\ar@{-->}[drr]_(0.4){G(1_{Q(A)})} \\ar@{-->}[rrrr]^-{G(\\theta^{111}_{H(A)}\\theta^{010}_{A})} &&&& GKH(A) \\ar@{-->}[drr]^{G(\\varepsilon_{A}K(\\chi_{A}))} &&&&&&\\\\ &&&&& GQ(A) \\ar@{-->}[rrrr]_-{G(\\theta^{011}_{Q(A)}\\theta^{011}_{A})} &&&& GQQQ(A) &&& \\\\ GQ(A) \\ar@{-->}[rrrrrrrr]^{G(\\theta^{110}_{H(A)}\\theta^{010}_{A})} \\ar[ddrrrr]_-{G(1_{Q(A)})} \\ar@{-->}[uurrr]^{G(1_{Q(A)})} &&&&&&&& GKCH(A) \\ar@{-->}[ddrrrr]_-{GK(\\omega_{Q(A)}C(\\chi_{A}))} \\ar@{-->}[uul]^(0.8){G(\\varepsilon_{KH(A)}K(\\eta_{H(A)}))} &&&& \\\\ &&&&&&&&&&& \\\\ &&&& GQ(A) \\ar[rrrrrrrr]_{G(\\theta^{010}_{Q(A)}\\theta^{011}_{A})} \\ar@{-->}[uuur]_-{G(1_{Q(A)})} &&&&&&&& GKHQ(A) \\ar@{-->}[uuulll]_(0.8){G(\\varepsilon_{Q(A)}K(\\chi_{Q(A)}))} } \\end{array} \\end{equation} \\end{itemize} The identity (\\ref{DIDcoassociativity0}) says that (\\ref{DthetachiDnIDA}) and (\\ref{DIDthetaAxihyperactionA}) are equal. This gives us the identities: \\begin{equation}\\label{thetaA000} C(\\theta^{000}_{A}) \\theta^{000}_{A} = \\theta^{100}_{H(A)} \\theta^{000}_{A}, \\end{equation} \\begin{equation}\\label{thetaA001} C(\\theta^{001}_{A}) \\theta^{000}_{A} = \\theta^{000}_{Q(A)} \\theta^{001}_{A}, \\end{equation} \\begin{equation}\\label{thetaA010} H(\\theta^{010}_{A}) \\theta^{001}_{A} = \\theta^{101}_{H(A)} \\theta^{000}_{A}, \\end{equation} \\begin{equation}\\label{thetaA011} H(\\theta^{011}_{A}) \\theta^{001}_{A} = \\theta^{001}_{Q(A)} \\theta^{001}_{A}, \\end{equation} \\begin{equation}\\label{thetaA100} Q(\\theta^{010}_{A}) \\theta^{011}_{A} = \\theta^{111}_{H(A)} \\theta^{010}_{A}, \\end{equation} \\begin{equation}\\label{thetaA101} Q(\\theta^{011}_{A}) \\theta^{011}_{A} = \\theta^{011}_{Q(A)} \\theta^{011}_{A}, \\end{equation} \\begin{equation}\\label{thetaA110} K(\\theta^{000}_{A})\\theta^{010}_{A} = \\theta^{110}_{H(A)} \\theta^{010}_{A}, \\end{equation} \\begin{equation}\\label{thetaA111} K(\\theta^{001}_{A}) \\theta^{010}_{A} = \\theta^{010}_{Q(A)} \\theta^{011}_{A}. \\end{equation} In order to summarize what all these equations mean let us recall definitions of the Eilenberg-Moore and Kleisli categories of coalgebras over a comonad: \\begin{definition}\\label{EM} Let $(C,\\tau,\\zeta)$ be a comonad on a category $\\X$. We say that a pair $(X,x)$ where $x \\maps X \\to C(X)$ is a $C$-coalgebra if it satisfies the following two axioms: \\[\\xymatrix@!=3pc{X \\ar[r]^-{x} \\ar[d]_-{x} & C(X) \\ar[d]^{\\tau_{X}} \\\\ C(X) \\ar[r]_-{C(x)} & CC(X) } \\hspace{1cm} \\xymatrix@!"}
{"input": "\\xymatrix@!=1pc{\\Cat \\times \\Cat \\ar[rr]^-{\\A r} \\ar[dr]_-{Pr_{1}} && \\Cat \\times \\Cat \\ar[dl]^{Pr_{1}} \\\\ & \\Cat & } \\end{array} \\end{equation} which is defined for any pair $(\\X,\\Y)$ of categories by $\\A r(\\X,\\Y):= (\\X, \\X + \\Y)$ where \"+\" denotes a coproduct in $\\Cat$. For any pair of functors $F \\maps \\X \\to \\X'$ and $G \\maps \\Y \\to \\Y'$ we have $\\A r(F,G):= (F, F + G)$ with obvious action on pairs of natural transformations. Then $\\A r^{2}(\\X,\\Y):= \\A r(\\X, \\X + \\Y):= (\\X, \\X + \\X + \\Y)$ and there exist natural 2-transformations \\begin{equation} \\begin{array}{c}\\label{MN} \\M \\maps \\A r^{2} \\Rightarrow \\A r \\\\ \\N \\maps \\I_{\\Cat \\times \\Cat} \\Rightarrow \\A r \\end{array} \\end{equation} with one component $(I_{\\X}, \\bigtriangledown_{\\X} + I_{\\Y}) \\maps (\\X, \\X + \\X + \\Y) \\to (\\X, \\X + \\Y)$ and the other one $(I_{\\X}, \\iota_{\\Y}) \\maps (\\X, \\Y) \\to (\\X, \\X + \\Y)$ indexed by $(\\X,\\Y)$ respectively, as in the following diagrams \\[\\hspace{-.5cm} \\xymatrix@!=4pc{(\\X, \\Y) \\ar[r]^-{(I_{\\X}, \\iota_{\\Y})} \\ar[d]_-{(F, G)} & (\\X, \\X + \\Y) \\ar[d]^{(F,F + G)} \\\\ (\\X', \\Y') \\ar[r]_-{(I_{\\X'}, \\iota_{\\Y'})} & (\\X', \\X' + \\Y')} \\hspace{.5cm} \\xymatrix@!=4pc{(\\X, \\X + \\X +\\Y) \\ar[rr]^-{(I_{\\X}, I_{\\X} + I_{\\X} + I_{\\Y)}} \\ar[d]_-{(F, F + F + G)} && (\\X, \\X + \\Y) \\ar[d]^{(F,F + G)} \\\\ (\\X', \\X' + \\X' + \\Y') \\ar[rr]_-{(I_{\\X'}, I_{\\X'} + I_{\\X'} + I_{\\Y'})} && (\\X', \\X' + \\Y')}\\] where $\\iota_{\\Y}$ is a canonical inclusion."}
{"input": "(eds) Seminar on Triples and Categorical Homology Theory, Lecture Notes in Mathematics, vol 80 (1969), 119-140. \\bibitem[B\\' enabou, 1967]{Be1} J. B\\' enabou, Introduction to bicategories, Reports of the Midwest Category Seminar, Lecture Notes in Math. 47 (1967), 1-77. \\bibitem[B\\' enabou, 1973]{Be2} J. B\\'enabou, Les distributeurs, Institut de mathematiques pure and appliqu\\' ee, Rapport No. 3 (1973),Universit\\' e catholique de Louvain. \\bibitem[Blackwell,Kelly, Power,1989]{BKP} R. Blackwell, G.M. Kelly, A.J. Power, Two-dimensional monad theory, Journal of Pure and Applied Algebra Volume 59, Issue 1 (1989), 1-41. \\bibitem[Bourke,2014]{Bo} J. Bourke, Two-dimensional monadicity, Advances in Mathematics 252 (2014), 708-747. \\bibitem[Bourke, Lobbia,2023]{BL} J. Bourke, G. Lobbia, A skew approach to enrichment for Gray-categories, Advances in Mathematics Volume 434 (2023), 109327. \\bibitem[Borceaux, Bourn,2004]{BB} F. Borceaux, D. Bourn, Mal'cev, Protomodular, Homological and Semi-abelian categories, Mathematics and its Applications vol. ~566, Kluwer Academic Publishers (2004). \\bibitem[Bunge,1974]{Bu} M. Bunge, Coherent Extensions and Relational Algebras, Transactions of the American Mathematical Society, Vol. 197 (1974), 355-390. \\bibitem[Bunge, Carboni,, 1995]{BC} M. Bunge, A. Carboni, The symmetric topos, Journal of Pure and Applied Algebra 105 (1995), 233-249. \\bibitem[Bunge, Funk, 1999]{BF} M. Bunge, J. Funk, On a bicomma object condition for KZ-doctrines, Journal of Pure and Applied Algebra 143 (1999), 69-105. \\bibitem[Burroni, 1971]{Burr} A. Burroni, $T$-catégories (cat\\' egories dans un triple), Cahiers de topologie et g\\' eom\\' etrie diff\\' erentielle cat\\' egoriques Volume 12 (1971) no. 3, pp. 215-321. \\bibitem[Casta\\~{n}o Iglesias, G\\' omez-Torrecillas, Nastasescu, 1998]{CIGTN} F. Casta\\~{n}o Iglesias, J. G\\' omez-Torrecillas, C. Nastasescu, Frobenius functors: applications, Communications in Algebra vol.~27, Issue 10 (1998), 4879-4900."}
{"input": "Rev\\^etements Etale et Groupe Fondamental, SGA 1, Lecture Notes in Math. 224, Springer-Verlag, Berlin, 1971. \\bibitem[Grothendieck, 2022]{Gr} A. Grothendieck, \\' A la poursuite des champs Volume I, \\' edit\\' e par G. Maltsiniotis, Documents Math\\' ematiques Volume 20, Soci\\' et\\' e Math\\' ematique de France, 2022. \\bibitem[Hermida, Jacobs, 1998]{HJ} C. Hermida, B. Jacobs, Structural Induction and Coinduction in a Fibrational Setting, Information and Computation 145 (1998), 107-152. \\bibitem[Hermida, 2000]{Her1} C. Hermida, Representable multicategorires, Advances in Mathematics 151 (2000), 164-225. \\bibitem[Hermida, 2001]{Her2} C. Hermida, From coherent structures to universal properties, Journal of Pure and Applied Algebra 165 (2001), 7-61. \\bibitem[Hyland, Power, 2002]{HP} M. Hyland, J. Power, Pseudo-commutative monads and pseudo-closed 2-categories, Journal of Pure and Applied Algebra 175 (2002), 141-185. \\bibitem[Jacobs,1993]{Jac} B. Jacobs, Comprehension categories and the semantics of type dependency, Theoretical Computer Science Volume 107, Issue 2 (1993), 169-207. \\bibitem[Johnstone,1993]{J1} P.T. Johnstone, Fibrations and Partial Products in a 2-Category, Applied Categorical Structures 1 (1993), 141-179. \\bibitem[Johnstone,1995]{J2} P.T. Johnstone, Connected limits, familial representability and Artin glueing, Mathematical Structures in Computer Science vol. 5 (1995), 441-459. \\bibitem[Johnstone,2011]{J3} P.T. Johnstone, Remarks on punctual local connectedness, Theory and Applications of Categories, Vol. ~25, No. 3 (2011), 51-63. \\bibitem[Kelly,1974]{Kelly4} G.M. Kelly, Coherence theorems for lax algebras and for distributive laws, Category Seminar Sydney 1972/73, Lecture Notes in Mathematics, 420,(1974), 281-375. \\bibitem[Kelly, Lawvere,1989]{KL} G.M. Kelly, F.W. Lawvere, On the complete lattice of essential localizations, Bulletin de la Soci\\' et\\' e Math\\' ematique de Belgique, S\\' erie A, v.ol.~ 41, no 2 (1989), 289-319. \\bibitem[Kock,1995]{Kock} A."}
{"input": "\\section{Introduction} Quantum circuit \\cite{Nielsen_Chuang_2010, Kitaev_Yu_Shen_2002} is a universal model for quantum computation in which quantum information is processed via the application of a series of unitary operations called quantum logic gates. %The goal of quantum logic gates is to implement the target global unitary operation on all the qubits in the register, according to the quantum program. Similarly to a classical computer, whose computation can be described using the classical circuit model, every global quantum operation on a qubit register can be realized using a universal finite set of elementary operations. A set of such quantum logic gates is referred to as the universal gate set or, in the context of quantum hardware, the native gate set. Contrary to the classical case, the finite length quantum circuits built out of a finite discrete set $\\mathcal{S}$ of quantum gates can be used to implement arbitrary multi-qubit (global) unitary operations only approximately, up to some error $\\epsilon$ (in a suitable metric). The number of elementary gates needed to implement a target unitary operation $U$ with precision $\\epsilon$ using gates from $\\mathcal{S}$ is a measure of the complexity of $U$ with respect to $\\mathcal{S}$ \\cite{Nielsen_Chuang_2010, Kitaev_Yu_Shen_2002, aaronson2016complexityquantumstatestransformations}. For a universal gate set $\\mathcal{S}$ and any finite $\\epsilon$, the complexity of any $U$ is finite and thus can be upper bounded by the shortest circuit length, $\\ell (\\mathcal{S}, \\epsilon)$, so that any $U$ can be $\\epsilon$-approximated by a quantum circuit built out of $\\mathcal{S}$ of length at most $\\ell (\\mathcal{S}, \\epsilon)$."}
{"input": "be understood as an absolute measure of the efficiency of $\\mathcal{S}$ at the scale of $\\epsilon$-approximations. Since the implementation of quantum gates is always flawed, for reasonably small nonzero $\\epsilon$, this number fully characterizes the efficiency of $\\mathcal{S}$. Quantum compilation \\cite{ge2024quantumcircuitsynthesiscompilation,H_ner_2018, Nielsen_Chuang_2010} is a process whose main objective is to approximate the target quantum circuit from the high-level hardware-agnostic representation used by quantum programmers to the form expressible by the native gate set executable on a specific quantum computer. Another task handled by the compiler is circuit optimization, which, loosely speaking, involves reducing the resources of quantum circuits, such as the depth of the circuit or the number of specific gates used. In the case of the current noisy intermediate-scale quantum (NISQ) machines, which do not enjoy quantum error correction, the reduction of the circuit depth and the number of costly gates (such as the noisy entangling gates) is of utmost practical importance \\cite{Gheorghiu_2023, Preskill2018quantumcomputingin, Noh_2020}. On the other hand, in the fault-tolerant regime, due to the Eastin-Knill theorem \\cite{PhysRevLett.102.110502, Woods_2020, Faist_2020}, the bottleneck is typically determined by the number of resource-costly non-traversal gates used \\cite{gottesman2005quantumerrorcorrectionfaulttolerance, doi:10.1073/pnas.2026250118,Eastin_2009}. For example, in the case of Clifford+T gate sets, the focus is usually on the reduction of the T-count i.e. the number of non-transversal T gates (also known as the $P(\\pi/4)$ or $\\pi/8$ gates \\footnote{To avoid confusion with the $T$ symbol occurring in $T$-QCO, we refer to the T gate as $P(\\pi/4)$ gate."}
{"input": "number of qubits needed to perform the computations \\cite{PRXQuantum.2.020341, Gheorghiu_2022, Ruiz2025, 10.5555/2685179.2685180,vandaele2024lowertcountfasteralgorithms,zhou2024algorithmicfaulttolerancefast, heyfron2018efficientquantumcompilerreduces}. However, the compilation process is fundamentally limited by the efficiency of the used gate set $\\mathcal{S}$. Aside from the applications in the description of information processing occurring in quantum computers, quantum circuits can be used to describe the discrete unitary dynamics of general discrete quantum systems \\cite{Tokusumi_2018, Fisher_2023, Claeys_2022}. Such an approach has been recently proposed to gain insight into the physics of black hole interiors, and interesting results regarding the saturation and recurrence of the complexity of such systems have been obtained \\cite{Hayden_2007, PhysRevX.14.041068}. Such behaviour also depends on the efficiency of gate sets $\\mathcal{S}$ used to model the system. Although it is conjectured that the generic universal gate sets $\\mathcal{S}$ have, so called spectral gap, which implies the optimal asymptotic efficiency $\\ell(\\mathcal{S}, \\epsilon) = \\Theta\\left(\\mathrm{log}(1/\\epsilon)\\right)$, the quantitative methods to bound and compare the efficiency of various gate sets $\\mathcal{S}$ are not well-developed. %Suppose we want to realize the operation $U$ from $\\mathbf{U}(d)$ using the gates from $\\mathcal{S}$, up to a finite precision $\\epsilon$. The smallest $\\ell$ of a quantum circuit needed to achieve this task is called the complexity of $U$. Crucially, the complexity is defined relative to a chosen $\\mathcal{S}$. %The gate set $\\mathcal{S}$ is universal if the complexity of all quantum operations in $\\mathbf{U}(d)$ is finite. Equivalently, quantum circuits $\\mathcal{S}_{\\leq \\ell}$ form an $\\epsilon$-net in $U$ in $\\mathbf{U}(d)$ for some finite circuit length $\\ell$, i.e."}
{"input": "(in a chosen metric) from some operation implemented by a quantum circuit from $\\mathcal{S}_{\\leq \\ell}$. %The smallest length of circuits needed to form an $\\epsilon$-net, denoted $\\ell(\\mathcal{S}, \\epsilon)$, is an absolute measure of optimality of the set of gates $\\mathcal{S}$ at the scale of $\\epsilon$ approximations. In this work, we introduce and study the relative measure of the efficiency of finite discrete universal gate sets $\\mathcal{S}$ that we call the quantum circuit overhead (QCO) and the related notion of $T$-Quantum Circuit Overhead ($T$-QCO). The notion of overhead is based on the comparison of the efficiency $\\ell(\\mathcal{S}, \\epsilon)$ among the gate sets $\\mathcal{S}$ having the same number of elements, where the optimal efficiency is denoted $\\ell_{\\mathrm{opt}}(|\\mathcal{S}|, \\epsilon)$. Crucially, both overheads can be upper-bounded by essentially calculable quantities, namely $Q$ and $Q_T$, respectively, which can be obtained from numerical simulations. To demonstrate the feasibility of our method and its applications, we perform extensive numerical experiments in which we calculate $Q/Q_T$, focusing on the comparison between the two scenarios for single-qubit gate sets: \\begin{enumerate} \\item A Haar-random set $\\mathcal{S}$ with a fixed number of elements (of infinite or fixed finite order), \\item A set $\\mathcal{S}$ composed of a finite group (such as Clifford or Hurwitz group) completed with a single Haar-random gate (of infinite or fixed finite order), making the set universal. \\end{enumerate} In the second scenario, we compare such random ensembles with some ``special'' choices, e.g. the $P(\\pi/4)$ gate in the case of the Clifford group, gaining insight into their efficiency."}
{"input": "inclusion of the finite order cases is motivated by the fault-tolerance considerations and the analysis of the so-called Super-Golden Gates \\cite{sarnak2015}. Surprisingly, our results suggest that the $P(\\pi/4)$ gate is a highly non-optimal choice among all gates of order $8$ in terms of $Q_T$. We also identified the best possible gates of orders 8 and 2 in the Clifford and Hurwitz group cases, respectively. %We argue that it makes sense to compare $\\ell(\\mathcal{S}, \\epsilon)$ within the sets of gates with the same number of elements, where for the optimal gate set we have $\\ell_{\\mathrm{opt}}(|\\mathcal{S}|, \\epsilon)$. %To bound such overhead, we need to be able to upper bound $\\ell(\\mathcal{S}, \\epsilon)$ and lower bound $\\ell_{\\mathrm{opt}}(|\\mathcal{S}|, \\epsilon)$. %such that for any element $U \\in \\mathbf{U}(d)$, $d_{op}(U, \\tilde{U}) \\leq \\epsilon$ for some $\\tilde{U} \\in \\bigcup_{1 \\leq \\ell \\leq \\ell_{\\epsilon}} \\mathcal{S}_{\\ell}$ \\footnote{If $e \\in \\mathbf{U}(d)$, then $\\mathcal{S}_{\\ell_1} \\subseteq \\mathcal{S}_{\\ell_2}$ for $\\ell_1 \\leq \\ell_2$ so that $\\bigcup_{1 \\leq \\ell \\leq \\ell_{\\epsilon}} \\mathcal{S}_{\\ell} = \\mathcal{S}_{\\ell_{\\epsilon}}$.}. %Universality of a set $\\mathcal{S}$ is equivalent to the contition that for any $\\epsilon$, the set forms an $\\epsilon$-net. %If $\\mathcal{S}\\subset U(d)$ is $\\delta$-approximate $t$-design then $\\S_l$ is $\\delta^l$-approxiamte $t$-design In order to upper bound the overhead, we need to be able to upper bound $\\ell(\\mathcal{S}, \\epsilon)$ and lower bound $\\ell_{\\mathrm{opt}}(|\\mathcal{S}|, \\epsilon)$."}
{"input": "t,t},\\,\\,\\,\\,\\,T_{\\nu,t}\\coloneqq \\int_{\\mathbf{U}(d)} d\\nu(U) U^{ t,t} \\mathrm{,} \\end{equation} are so called $t$-moment operators, $U^{t,t}:=U^{\\otimes t}\\otimes \\bar{U}^{\\otimes t}$, and we require $\\delta(\\nu,t) < 1$ (see Appendix \\ref{app:t_designs} for more information). A finite gate set $\\mathcal{S}$ can be associated with a discrete probability measure supported on its elements (e.g. the uniform measure $\\nu_{\\mathcal{S}}$). The spectral gap of $\\mathcal{S}$ is then $1- \\delta(\\nu_\\mathcal{S})$, where $\\delta(\\nu_\\mathcal{S})$ is the supremum of $\\delta(\\nu_\\mathcal{S},t)$ over all scales $t$, so that the spectral gap property reads $\\delta(\\nu_{\\mathcal{S}}) < 1$. The quantitative version of the statement about the efficiency of gate sets $\\mathcal{S}$ with a spectral gap is a non-constructive SKL theorem \\cite{Harrow_2002,S_owik_2023} %Essentially the same theorem was proved differently in \\cite{S_owik_2023} \\footnote{This theorem follows from the proof of Theorem 3 if the bound $(1-x)^r \\leq e^{-rx}$ is not used.} and it states that if $\\delta(\\nu_\\mathcal{S}) >0$, then for any precision $\\epsilon$ every operation $U$ from $\\mathbf{U}(d)$ can be approximated by a sequence of gates from $\\mathcal{S}$ of the length \\begin{equation} \\frac{d^2-1}{\\log\\left(1/\\delta(\\nu_\\mathcal{S})\\right)}\\log\\left(\\frac{2}{ A_v \\epsilon}\\right ) \\mathrm{.} \\end{equation} Notice that although the scaling is optimal, the pre-factor may be arbitrarily large. Moreover, in our setting, the pre-factor is bounded from below via $\\delta(\\nu_\\mathcal{S})\\geq \\delta_{\\mathrm{opt}}(\\mathcal{S})$, where \\begin{equation} \\label{eq:efficient_gates} \\delta_{\\mathrm{opt}}(\\mathcal{S}) \\coloneqq \\frac{2\\sqrt{|\\mathcal{S}| - 1}}{|\\mathcal{S}|} \\mathrm{.} \\end{equation} \\cite{kesten59} (see Appendix \\ref{app:kesten} for more detailed explaination). We say a gate set $\\mathcal{S}$ is efficient if $\\delta(\\nu_\\mathcal{S})= \\delta_{\\mathrm{opt}}(\\mathcal{S})$ and refer to $\\delta_{\\mathrm{opt}}(\\mathcal{S})$ as the optimal value. Note that the optimal value depends only on the number of gates $|\\mathcal{S}|$."}
{"input": "which have a spectral gap is a well-known mathematical problem, dating back to Kazhdan's observation that certain finitely generated discrete subgroups have the spectral gap (Kazhdan's property (T)). %\\color{red} %..LPS... \\cite{lps86, lps87} \\cite{gamburd1999} %\\color{black} %Bourgain and Gamburd proved the existence of spectral gap for universal sets $\\mathcal{S} \\subset \\mathrm{SU}(d)$ consisting of algebraic elements \\cite{Bourgain2007OnTS, bourgain2011spectralgaptheoremsud} which was later generalized to any compact simple Lie group by Benoist and de Saxcé \\cite{benoist2014spectralgaptheoremsimple}. Nowadays, it is conjectured %that any universal set $\\mathcal{S} \\subset{G}$ has a spectral gap for any compact semisimple group $G$. %\\color{red} %[TODO] Algebraic elements vs algebraic entries. More examples (etc. - read Kuperberg)? Explain that algebraic generators are not interesting in practice. %\\color{black} %\\color{red} %origins: How they studied equidistributivity. LPS conjecture on a gap. %\\color{black} The study of $\\delta(\\nu_\\mathcal{S})$ for generic $\\mathcal{S}$ is a hard problem as $\\delta(\\nu_\\mathcal{S})$ can not be directly calculated. However, some properties of $\\delta(\\nu_\\mathcal{S})$ are known. For example, it is known that $\\delta(\\nu_\\mathcal{S}) < 1$ for the universal sets $\\mathcal{S}$ consisting of algebraic elements \\cite{Bourgain2007OnTS, bourgain2011spectralgaptheoremsud}. This result was later generalized to any compact, simple Lie group \\cite{benoist2014spectralgaptheoremsimple}. Moreover, it has been conjectured (and is now commonly believed) that $\\delta(\\nu_\\mathcal{S}) < 1$ for any universal $\\mathcal{S}$ and there are known examples of efficient single-qubit gate sets $\\mathcal{S}$ with $|\\mathcal{S}|= p -1$ for $p \\equiv 1 \\, \\mathrm{mod} \\, 4$ \\cite{lps86, lps87}. Finally, some commonly used one-qubit gate sets are known to be efficient \\cite{bocharov2013, selinger2015, sarnak2015, kliuch2016}."}
{"input": "of efficient many-qubit gates remains an open problem. Fortunately, one can still obtain useful non-constructive SKL theorems using the knowledge of $\\delta(\\nu_\\mathcal{S},t)$. Such a finite-scale approach was studied in \\cite{Dolgopyat2002, varju13, 9614165, słowik2025fundamentalsolutionsheatequation} and is sufficient in practice, as it corresponds to studying efficiency at a certain finite precision $\\epsilon$. The approach from \\cite{9614165, słowik2025fundamentalsolutionsheatequation} utilizes the relation between $\\epsilon$-nets and $\\delta$-approximate $t$-designs. A finite subset of channels $\\mathcal{E}$ from $\\mathbf{U}(d)$ is an $\\epsilon$-net if for every channel $\\mathbf{U}$ from $\\mathbf{U}(d)$, there exists a channel $\\mathbf{V}$ from $\\mathcal{E}$, such that $d(\\mathbf{U},\\mathbf{V}) \\leq \\epsilon$. In other words, $\\mathcal{E}$ contains all the possible channels up to the error $\\epsilon$. It is intuitively clear that $\\epsilon$-nets formed by quantum circuits built from $\\mathcal{S}$ and $\\delta$-approximate $ t$-designs supported on them are related. However, the quantitative relations between them were not known until recently. Such bounds for the group $\\mathbf{U}(d)$ were first rigorously studied in \\cite{9614165}, where the authors show \\footnote{The result is more general as it does not assume that the measure is uniform.} that a discrete set is an $\\epsilon$-net if it is a support of a $\\delta$-approximate $t$-design with the parameters obeying the following scalings \\begin{equation} \\label{eq:t_eps} t(\\epsilon)\\gtrsim \\frac{d^{5/2}}{\\epsilon}, \\quad \\delta(\\epsilon) \\lesssim \\left(\\frac{\\epsilon^{3/2}}{d}\\right)^{d^2} \\end{equation} (see \\cite{9614165} for precise formulas). A more recent study improves the second scaling to $\\delta(\\epsilon) \\lesssim (\\epsilon/d^{1/2})^{d^2}$ \\cite{słowik2025fundamentalsolutionsheatequation}."}
{"input": "\\quad \\delta(\\epsilon) \\lesssim\\left(\\frac{\\epsilon}{d^{1/2}}\\right)^{d^2} %\\end{equation} % (see \\cite{SRS_2024} for explicit formulas), and from \\cite{9614165} it is known that $t\\simeq \\frac{d^{2}}{\\epsilon}$ is tight. From the point of view of nonabelian Fourier analysis on groups, such reciprocal relation between $t$ and $\\epsilon$ can be intuitively understood as the relation between distances on the group and its corresponding ``frequency'' space, so that smaller $\\epsilon$ corresponds to faster varying functions. The quantitative version of such SKL theorem was proved in \\cite{9614165} and states \\footnote{Original Proposition 2 in \\cite{9614165} has $1- \\delta(\\nu_{\\mathcal{S}},t)$ instead of $\\log\\left(1/\\delta(\\nu_\\mathcal{S},t)\\right)$ due to unnecessary bounding. } that for a fixed precision $\\epsilon$, every operation $U$ from $\\mathbf{U}(d)$ can be $\\epsilon$-approximated by sequences of gates from $\\mathcal{S}$ of the length $\\ell_{\\delta}(\\mathcal{S}, \\epsilon)$ \\begin{equation} \\label{eq:ub} \\ell(\\mathcal{S}, \\epsilon) \\leq \\ell_{\\delta}(\\mathcal{S}, \\epsilon) \\sim \\frac{d^2-1}{\\log\\left(1/\\delta(\\nu_\\mathcal{S},t(\\epsilon))\\right)}\\log\\left(\\frac{1}{\\epsilon}\\right ) \\mathrm{,} \\end{equation} where $t(\\epsilon)$ is the bound of type (\\ref{eq:t_eps}) stemming from the $\\epsilon$-net $t$-design correspondence. Thus, we can say that $\\delta(\\nu_\\mathcal{S}, t(\\epsilon))$ upper bounds the efficiency of $\\mathcal{S}$ on the level of $\\epsilon$-approximations. Moreover, for not too large values of $t$ and $d$, the value of $\\delta(\\nu_\\mathcal{S},t)$ can be calculated using supercomputing clusters. %A similar result was proved earlier in \\cite{varju13}, however with significantly worse scaling $t(\\epsilon)$. The bound (\\ref{eq:ub}) can be improved by using the tighter bound (\\ref{eq:impr_t_eps}). %Theorem \\ref{th:sklosh} follows from Theorem \\ref{th:tdepsc} and a well-known fact that if $\\mathcal{S}$ is a $\\delta$-approximate $t$-design then $\\mathcal{S}_{\\ell}$ is a $\\delta^{\\ell}$-approximate $t$-design."}
{"input": "analysis suggesting fast stabilization of the distribution with growing $t$. Our numerical experiments further validate this observation and extend it to all types of ensembles of gate sets studied in this paper. Hence, although the bounds (\\ref{eq:t_eps}) provide some theoretical guarantees on the scales $t$ needed to gain insight into the $\\epsilon$-scale efficiency (via (\\ref{eq:ub})), our results suggest that in practice, it suffices to compute $\\delta(\\nu_\\mathcal{S},t)$ for $t$ much smaller than the bounds $t(\\epsilon)$. Although from (\\ref{eq:ub}) it seems like $\\delta(\\nu_\\mathcal{S}, t)$ is a good measure of the efficiency of $\\mathcal{S}$, the value of $\\delta(\\nu_\\mathcal{S}, t)$ is sensitive to the number of gates $|\\mathcal{S}|$. In particular, as the number of gates $|\\mathcal{S}|$ goes to infinity, the optimal value (\\ref{eq:efficient_gates}), which lower bounds the supremum of $\\delta(\\nu_\\mathcal{S}, t)$ over $t$, goes to $0$. Since the implementation of gate sets $\\mathcal{S}$ with large $|\\mathcal{S}|$ is very costly in practice, e.g. due to the necessary calibrations of quantum hardware, it makes sense to compare the gate sets $\\mathcal{S}$ of fixed $|\\mathcal{S}|$. This motivates us to introduce the notion of the overhead of quantum circuits. \\label{sec:qco} %In this section we introduce the core concept of this paper - Quantum Circuit Overhead and the derived concept of the T-Quantum Circuit Overhead."}
{"input": "same-size ensembles of type $\\mathcal{S}_{\\mu, n, r}$ containing the corresponding number of gates $n=|C|$ (see Fig.~\\ref{fig:cliff_Q_vs_QT_t500_all4} for Clifford group and Fig.~\\ref{fig:hurwitz_Q_vs_QT_t500_all4} for Hurwitz group) and with the values of $Q_T$ for gate sets of type $C_T$ with ``special'' choices of $T$. Finally, we identify the choices of $T$ giving the best values of $Q_T$, among all gates of order 8 (for the Clifford group) and 2 (for the Hurwitz group). Additionally, we check the tightness of the bound (\\ref{eq:efficient_gates}) in the case of ensembles of type $C_{\\mu,r}$ with finite $r$ by calculating the distributions of singular values of the corresponding $t$-moment operator (see Appendix \\ref{app:kesten} and Fig.~\\ref{fig:cliff_t500_r8_spec} and Fig.~\\ref{fig:hurwitz_t500_r2_spec} for more details). %We consider two universal sets $\\mathcal{S}_{12}, \\mathcal{S}_{24} \\subset \\mathbf{U}(2)$ of one-qubit Super Golden Gates \\cite{Parzanchevski_2018}. %Each such set consists of a finite subgroup $C_n \\subset \\mathbf{U}(2)$ with $n$ elements, extended by a tailored $T$-gate, denoted $T_n$ so that $\\mathcal{S}_n=C_n \\cup T_n$. In our case, these finite groups are - the Hurwitz group $C_{12}$ and the Clifford group $C_{24}$ for one qubit. %For each such universal set $\\mathcal{S}_n$, we construct a random ensemble of $\\approx10^4$ derived universal gate sets in which the $T_n$-gate is substituted with a Haar-random gate. This way, we obtain a histogram of $Q_T$ for a fixed $t$. We increase the value of $t$ until the histogram stabilizes and mark two special values on the histograms - the optimal value of $Q_T$ and the value of $Q_T$ for the super-golden gates $\\mathcal{S}_n$."}
{"input": "not depend on the scale $t$ and lower bounds the histogram in $t \\to \\infty$ limit. Additionally, for each $\\mathcal{S}_n$, we compare such histograms with analogous histograms for the same-size ensembles of purely Haar-random universal gate sets containing the same number of gates. \\subsection{Clifford group} The one-qubit Clifford subgroup $\\mathcal{C} \\subset \\mathbf{U}(2)$ has 24 elements and is generated by \\begin{equation} \\mathcal{C}=\\left\\langle \\begin{pmatrix} 1 & 0 \\\\ 0 &i \\end{pmatrix}, \\begin{pmatrix} 1 & 1 \\\\ -1 & 1 \\end{pmatrix} \\right\\rangle, \\end{equation} up to normalization. The special choices of $T$ gates include the $P(\\pi/4)$ gate (of rank 8) and the so-called Super-Golden gate \\cite{Parzanchevski_2018} (of rank 2), denoted $T_{24}$ \\begin{equation} P(\\pi/4)= \\begin{pmatrix} 1 & 0 \\\\ 0 & 1+i \\end{pmatrix}, \\quad T_{24}= \\begin{pmatrix} -1-\\sqrt{2} & 2-\\sqrt{2}+i \\\\ 2-\\sqrt{2}-i & 1+\\sqrt{2} \\end{pmatrix} \\mathrm{,} \\end{equation} up to normalization. The value for the gate set $\\mathcal{C}_{P(\\pi/4)}$ is way outside the range of Fig.~\\ref{fig:cliff_QT_t5-50-500_sg} and Fig.~\\ref{fig:cliff_QT_t5-50-500_r8_sg}, with $Q_T \\approx 52$ for $t=500$. \\begin{figure} \\centering \\includegraphics[width=0.5\\textwidth]{cliff_QT_t5-50-500_sg.pdf} \\caption{ The histograms of $Q_T$ probability density for an ensemble of type $\\mathcal{C}_{\\mu, \\infty}$ with increasing $t$. The red line denotes the corresponding optimal value. The black line corresponds to a Super-Golden gate set $\\mathcal{C}_{T_{24}}$.} \\label{fig:cliff_QT_t5-50-500_sg} \\end{figure} %\\begin{figure}[H] % \\centering % \\includegraphics[width=0.5\\textwidth]{Figures/old/Clifford_with_sg_spec.pdf} % \\caption{The distribution of the singular values of the $t$-moment operator for a gate set $\\mathcal{C}_T$ with $t=100$. The Kesten-McKay measure is denoted by a dotted black line\\todo{Check t; Make it consistent with the new plots; what with super-golden?; correct; is it T or Super-Golden?"}
{"input": "%\\begin{figure}[H] % \\centering % \\includegraphics[width=0.5\\textwidth]{Figures/super-golden/hurwitz_Q_vs_QT_t100.pdf} %\\caption{The histograms of $Q_T$ probability density for an ensemble of type $\\mathcal{H}_{\\mu, \\infty}$ (bottom) vs the corresponding histogram of $Q$ for an ensemble of type $\\mathcal{S}_{\\mu, 12}$ for $t=100$. Both ensembles contained $\\approx 10^4$ gate sets. The red line denotes the corresponding optimal value.} % \\label{fig:hurwitz_Q_vs_QT_t100} %\\end{figure} %\\begin{figure}[H] % \\centering % \\includegraphics[width=0.5\\textwidth]{Figures/old/Hurwitz_with_sg_spec.pdf} % \\caption{The distribution of the singular values of the $t$-moment operator for a gate set $\\mathcal{H}_{T_{12}}$ with $t=100$. The Kesten-McKay measure is denoted by a dotted black line\\todo{Check t; Make it consistent with the new plots; what with super-golden?; correct; is T here a Super-Golden?}} % \\label{fig:Hurwitz_with_sg_spec} %\\end{figure} \\begin{figure} \\centering \\includegraphics[width=0.5\\textwidth]{hurwitz_QT_t5-50-500_r2_sg.pdf} \\caption{The histograms of $Q_T$ probability density for an ensemble of type $\\mathcal{H}_{\\mu,2}$ with increasing $t$. The red line denotes the corresponding optimal value. The black line corresponds to a Super-Golden gate set $\\mathcal{H}_{T_{12}}$.} \\label{fig:hurwitz_QT_t5-50-500_r2_sg} \\end{figure} \\begin{figure} \\centering \\includegraphics[width=0.5\\textwidth]{hurwitz_Q_vs_QT_t500_all4.pdf} \\caption{ The histograms of $Q_T$ probability density for ensembles of type $\\mathcal{H}_{\\mu, r}$ (bottom) vs the histogram of $Q$ for the corresponding ensembles of type $\\mathcal{S}_{\\mu, 12, r}$ (top) for $t=500$. The red line denotes the corresponding optimal value. Note that the scales on the Y-axis differ.} \\label{fig:hurwitz_Q_vs_QT_t500_all4} \\end{figure} %\\begin{figure}[H] % \\centering % \\includegraphics[width=0.5\\textwidth]{Figures/arXiv/hurwitz_Q_vs_QT_t500_r2.pdf} % \\caption{The histograms of $Q_T$ probability density for an ensemble of type $\\mathcal{H}_{\\mu, 2}$ (bottom) vs the corresponding histogram of $Q$ for an ensemble of type $\\mathcal{S}_{\\mu, 12}$ for $t=500$. Both ensembles contained $\\approx 10^4$ gate sets. The red line denotes the corresponding optimal value."}
{"input": "\\section{Unitary channels and the projective group} \\label{app:unitary} %Indeed, suppose an agent is given one of the two quantum channels - either $\\mathbf{U}$ (with probability p) or $\\mathbf{V}$ (with probability 1-p). The agent aims to determine which channel was provided by preparing a quantum state $\\rho$, passing it through the channel and measuring the output state. The maximal probability of the agent being successful is %\\begin{equation} % p_{\\mathrm{succ}}=\\frac{1}{2} + \\frac{1}{2}||(p\\, \\mathbf{U}, (1-p)\\,\\mathbf{V}||_{\\diamond}\\mathrm{.} %\\end{equation} %an agent is given the unknown quantum channel - either $\\mathcal{U}$ (with probability p) or $\\mathcal{V}$ (with probability 1-p). The task is to tell which channel is in the box. The agent can prepare the state $\\rho$, pass it through the box, and measure the output. The maximal probability of the agent's guess being correct is %\\begin{equation} % p_{\\mathrm{succ}}=\\frac{1}{2} \\left( 1-d_{\\diamond}\\left(p\\, \\mathcal{U}, (1-p)\\,\\mathcal{V}\\right)\\right)\\mathrm{.} %\\end{equation} %where The unitary channel $\\mathbf{U}$ acting on a Hilbert space $\\mathcal{H} \\cong \\mathbb{C}^d$ is the CPTP map defined via $\\mathbf{U}(\\rho) = U \\rho U^{\\dagger}$, for any quantum state $\\rho: \\mathcal{H} \\rightarrow \\mathcal{H}$ and some fixed unitary representative $U$ from $\\mathrm{U}(d)$. Since two unitaries $U, V$ which differ by a phase $U=Ve^{i \\phi}$ define the same unitary channel, the group of all unitary channels $\\mathbf{U}(d)$ can be identified with the projective unitary group $\\mathrm{PU}(d)=\\mathrm{U}(d)/U(1)$, where the canonical projection $\\pi: \\mathrm{U}(d) \\rightarrow \\mathbf{U}(d)$ is mapping the unitaries to the corresponding unitary channels $U \\mapsto \\mathbf{U}$. % (e.g."}
{"input": "\\section{Approximate t-designs and $\\epsilon$-nets} \\label{app:t_designs} The balanced polynomials of degree $t$ are homogeneous polynomials with degree $t$ in using matrix elements $u_{i,j}$ and degree $t$ in $\\overline{u}_{i,j}$. Notice that such polynomials are well-defined on $\\mathbf{U}(d)$ as they are not sensitive to the global phase factors. We denote the space of all such polynomials of degree $t$ by $\\mathcal{H}_t$. The space $\\mathcal{H}_t$ is spanned by the entries of $U^{t,t}:=U^{\\otimes t}\\otimes \\bar{U}^{\\otimes t}$ thus in general, each polynomial $f_t(U) \\in \\mathcal{H}_t$ can be expressed as $$f_t(U)=\\mathrm{Tr}\\left(A \\left(U^{\\otimes t}\\otimes \\bar{U}^{\\otimes t}\\right)\\right)$$ for some matrix $A$. Let $\\mu$ be the normalized Haar measure on $\\mathbf{U}(d)$, $\\mu(\\mathbf{U}(d))=1$. The Haar measure provides us with a notion of a uniform density on $\\mathbf{U}(d)$. %This measure can be defined as the pushforward of the Haar measure $\\mu_U$ on $\\mathrm{U}(d)$, $\\mu(X)=\\mu_U(\\pi^{-1}(X))$.%We do not discriminate between the unitary channels and unitaries and between Haar measures $\\mu$ and $\\mu_U$ unless it can lead to confusion. A $t$-design is a probability measure $\\nu$ on $\\mathbf{U}(d)$ which yields the same averaging outcome as the Haar measure average for all polynomials $f_t(U) \\in \\mathcal{H}_t$ \\begin{equation} \\int_{\\mathbf{U}(d)}d\\nu(U)f_t(U)=\\int_{\\mathbf{U}(d)}d\\mu(U)f_t(U) \\mathrm{.} \\label{eq:tde} \\end{equation} The case in which the measure $\\nu$ is supported on a finite number of points $\\{\\nu_i, U_i\\}$ is of utmost practical importance. In such a case, the left-hand side integral of (\\ref{eq:tde}) can be written as a sum %It turns out that there exist a finite discrete subsets $\\mathcal{E} \\subset \\mathbf{U}(d)$, such that averaging functions $f_t(U)$ over $\\mathcal{E}$ and $\\mu$ yields the same result."}
{"input": "\\sum_{U_i\\in\\mathcal{S}}\\nu_i f_t(U_i)=\\int_{\\mathbf{U}(d)} d\\mu(U)f_t(U) \\mathrm{,} \\label{eq:td} \\end{equation} where $\\mathcal{S}$ denotes a finite set supporting the measure $\\nu$. We are mostly interested in a case of uniform $t$-designs, i.e., the ones for which all $\\nu_i = 1/|\\mathcal{S}|$, and denote such a measure as $\\nu_{\\mathcal{S}}$. Hence, by $\\mathcal{S} \\subset \\mathbf{U}(d)$ being a $t$-design, we understand that the corresponding uniform discrete probability measure $\\nu_{\\mathcal{S}}$ is a $t$-design. Using the $t$-moment operators, the deviation from $\\nu$ being a $t$-design (\\ref{eq:tde}) can be measured as the difference in the operator norm $\\delta(\\nu,t)$ (see (\\ref{eq: t_moment}) and the formula above). This way, we can consider the cases where the condition (\\ref{eq:tde}) is satisfied only approximately, which leads to the definition of a $\\delta$-approximate $t$-design. We say that $\\nu$ is a $\\delta$-approximate $t$-design if $\\delta(\\nu,t) < 1$. In particular, the value $\\delta(\\nu,t)=0$ corresponds to (an ideal) $t$-design. %It is intuitively clear that $\\delta$-approximate $t$-designs and $\\epsilon$-nets are related, however the quantitative relations between them were not known until recently. We are interested in the explicit bounds $t(\\epsilon)$ and $\\delta(\\epsilon)$ which guarantee that a $\\delta$-approximate $t$-design is an $\\epsilon$-net, for a fixed $\\mathbf{U}(d)$. %Such bounds for the group $\\mathbf{U}(d)$ were first rigorously studied in \\cite{9614165}, where the authors show \\footnote{The result is more general as it does not assume that the measure is uniform.} that a discrete set is an $\\epsilon$-net if it is a support of a $\\delta$-approximate $t$-design with %\\begin{equation} %\\label{eq:t_eps} % t(\\epsilon)\\gtrsim \\frac{d^{5/2}}{\\epsilon}, \\quad \\delta(\\epsilon) \\lesssim \\left(\\frac{\\epsilon^{3/2}}{d}\\right)^{d^2} %\\end{equation} % (see \\cite{9614165} for explicit formulas)."}
{"input": "\\section{Optimal spectral gap and Kesten-McKay measure} \\label{app:kesten} Below, we discuss the applicability of the optimal value (\\ref{eq:efficient_gates}) and the related measure in various settings considered in this paper. For a symmetric (i.e., inverse-closed) gate set $\\mathcal{S}$, the $t$-moment operator (\\ref{eq: t_moment}) is a bounded self-adjoint operator with a well-defined spectrum. Its spectral measure $\\sigma_{\\mathcal{S},t}$ is compactly supported and hence, determined by its moments $\\sigma_{\\mathcal{S},t}^{(m)}$. The asymptotic behavior of such moments, i.e., the limit $ \\lim_{t \\to \\infty}\\sigma_{\\mathcal{S},t}^{(m)}$ is determined by the number of length $m$ spellings of identity and was provided in \\cite{kesten59} in the case of $\\mathcal{S}$ generating a free group. Moreover, it was shown in \\cite{kesten59}, that in this case there exists a measure $\\sigma_{\\mathcal{S}}$, such that $\\sigma_{\\mathcal{S}}^{(m)}=\\lim_{t \\to \\infty}\\sigma_{\\mathcal{S},t}^{(m)}$, known as the Kesten-McKay or Plancherel measure \\begin{equation} d \\sigma_{\\mathcal{S}}(x) = \\frac{|\\mathcal{S}| \\sqrt{\\delta^2_{\\mathrm{opt}}(\\mathcal{S})-x^2}}{2 \\pi (1-x^2)} \\mathbf{1}_{[-\\delta_{\\mathrm{opt}(\\mathcal{S})},\\, \\delta_{\\mathrm{opt}(\\mathcal{S}) }]} dx \\mathrm{,} \\end{equation} where $\\delta_{\\mathrm{opt}}(\\mathcal{S})$ is the optimal value (\\ref{eq:efficient_gates}). This implies that $\\sigma_{\\mathcal{S},t}$ converge weakly to $\\sigma_{\\mathcal{S}}$ in the limit $t \\to \\infty$ (see \\cite{Dulian_2024} for details). Furthermore, analogous results can be obtained for any (i.e., not necessarily inverse-closed) finite $\\mathcal{S}$, for which $\\mathcal{S} \\cup \\mathcal{S}^{-1}$ generates a free group \\cite{Dulian_2024}. However, since in this setting the $t$-moment operator does not need to be self-adjoint, by the Kesten-McKay measure we understand the spectral measure of $\\sqrt{T_{\\nu_{\\mathcal{S}},t} T_{\\nu_{\\mathcal{S}},t}^*}$ as $t \\to \\infty$, or equivalently the measure describing the singular values of $T_{\\nu_{\\mathcal{S}},t}$ as $t \\to \\infty$, given by \\begin{equation} \\frac{|\\mathcal{S}| \\sqrt{\\delta^2_{\\mathrm{opt}}(\\mathcal{S})-x^2}}{\\pi (1-x^2)} \\mathbf{1}_{[0,\\, \\delta_{\\mathrm{opt}(\\mathcal{S}) }]} dx \\mathrm{."}
{"input": "a Kesten-McKay measure can be applied in the setting of Haar random gate sets $\\mathcal{S}$, since then $\\mathcal{S} \\cup \\mathcal{S}^{-1}$ generates a free group with probability 1. Crucially, the Kesten-McKay measure can also be applied in the setting of $T$-QCO (\\ref{eq:S_ansatz}), when the additional gate $T$ is of infinite order (e.g. Haar random). This follows from the fact that in this case the derived gate set construction (\\ref{eq:derived}), which is used to upper bound the $T$-QCO (\\ref{eq:Q_T}), does not change the number of spellings of identity, compared to the free group case. For a Haar-random gate $T$ of fixed finite order, the number of spellings of identity is increased, which implies that the (even) spectral measure moments are larger than the moments of the Kesten-McKay measure. As a consequence, the support of the Kesten-McKay measure is contained in the support of such a spectral measure and the bound (\\ref{eq:efficient_gates}) can be applied. However, it was not clear how tight such a bound is with respect to the actual cut-off of the bulk spectrum. To verify it, we checked the distribution of the singular values of $t$-moments for (derived) ensembles of type $C_{\\mu, r}$ with finite $r$. The resulting distributions are close to the Kesten-McKay distribution, with the support of the latter contained in that of the former quite tightly (see Fig.~\\ref{fig:cliff_t500_r8_spec} and Fig.~\\ref{fig:hurwitz_t500_r2_spec}). Thus, the optimal value (\\ref{eq:efficient_gates}) is relevant in all cases considered in this paper."}
{"input": "be a chosen finite gate set from %$\\mathbf{U}(d)$. %Then, quantum circuits built out of $\\ell$ gates from $\\mathcal{S}$ correspond to words of length $\\ell$ over an alphabet $\\mathcal{S}$. We denote such set as %\\begin{equation} % \\mathcal{S}_{\\ell} \\coloneqq \\{U_{a_{1}}U_{a_{2}}\\cdots U_{a_{\\ell}}:\\, a_{i}\\in\\left\\{ 1,\\ldots,k\\right\\}\\} \\mathrm{,} %\\end{equation} %and the set of all words with length at most $\\ell$ by $\\mathcal{S}_{\\leq \\ell}$. \\begin{figure} \\centering \\includegraphics[width=0.5\\textwidth]{cliff_t500_r8_spec.pdf} \\caption{The probability density of the singular values of the $t$-moment operator for a derived ensemble of type $\\mathcal{C}_{\\mu,8}$ with $\\approx 20$ gate sets for $t=500$. The dotted black line denotes the Kesten-McKay measure, and the red line denotes the corresponding optimal value. } \\label{fig:cliff_t500_r8_spec} \\end{figure} \\begin{figure} \\centering \\includegraphics[width=0.5\\textwidth]{hurwitz_t500_r2_spec.pdf} \\caption{The probability density of the singular values of the $t$-moment operator for a derived ensemble of type $\\mathcal{H}_{\\mu,2}$ with $\\approx 20$ gate sets for $t=500$. The dotted black line denotes the Kesten-McKay measure, and the red line denotes the corresponding optimal value. } \\label{fig:hurwitz_t500_r2_spec} \\end{figure} %\\begin{theorem}[] %\\label{th:tdepsc} % Let $\\mathcal{E} \\subset \\mathbf{U}(d)$ be a finite subset. If $\\mathcal{E}$ is a $\\delta$-approximate $t$-design with $t \\geq 5\\frac{d^{5/2}}{\\epsilon} \\tau(\\epsilon, n) \\simeq \\frac{d^{5/2}}{\\epsilon}$ and $\\delta \\leq \\frac{1}{32} \\left(\\frac{\\epsilod^{3/2}}{4 C \\mathrm{log}^{1/2}(6C / \\epsilon)n}\\right)^{d^2-1} \\simeq \\left(\\frac{\\epsilod^{3/2}}{n}\\right)^{d^2-1} $ then $\\mathcal{E}$ forms an $\\epsilon$-net in $\\mathbf{U}(d)$ %\\footnote{The original statement is stronger since it is given wrt. to the diamond norm $d_{\\diamond}$ on the space of unitary channels, which can be identified with $\\mathbf{U}(d)$. The result follows since $d_{\\mathrm{proj}}(U,V)$ is upper-bounded by $ d_{\\diamond}(U,V)$, in fact $d_{\\mathrm{proj}}(U,V) \\leq d_{\\diamond}(U,V) \\leq 2 \\cdot d_{\\mathrm{proj}}(U,V) $ for any $U,V \\in \\mathbf{U}(d)$."}
{"input": "\\section{$T$-Quantum Circuit Overhead} \\label{app:T_QCO} The useful property of a derived set $\\mathcal{S}_{T}$ (\\ref{eq:derived}) is that the $T$-complexity of a fixed unitary with respect to $\\mathcal{S}_{T}$ is equal to its complexity (for the same precision). This allows us to lower bound the optimal $T$-complexity by $\\ell_{ \\mathrm{opt}}(|\\mathcal{S}_T|,\\epsilon)$. Moreover, for every unitary $U$ constructible using $\\mathcal{S}$ with a non-zero $T$-complexity for precision $\\epsilon$, there exists a unitary $U_T$ constructible using $\\mathcal{S}_T$ with the same $T$-complexity for the same precision (and vice-versa). Indeed, each such unitary $U$ can be $\\epsilon$-approximated by the reduced word over $\\mathcal{S}$ of the form \\begin{equation} \\label{eq:rwT} U \\approx_{\\epsilon} c_{i_1} T^{k_1} c_{i_2} T^{k_2} \\ldots c_{i_{p}} T^{k_p} c_{i_{p+1}} \\mathrm{,} \\end{equation} with $T$-count $\\sum_{i=1}^p k_i$ equal to said $T$-complexity, where $c_{i_j}$ are the elements from $C$ and $c_{i_1}$ and $c_{i_{p+1}}$ may be missing. Suppose $c_{i_1}$ is present and $c_{i_{p+1}}$ is missing. Denoting $g_{j} \\coloneqq d_{j}T d_{j}^{\\dagger}$, where $d_j \\coloneqq c_{i_1} c_{i_2} \\ldots c_{i_j}$, we have \\begin{equation} U \\approx_{\\epsilon} g_1^{k_1} g_2^{k_2} \\ldots g_p^{k_p} d_{p+1} \\end{equation} and $U_T=U d_{p+1}^{\\dagger}$ is $\\epsilon$-approximated by the word over $S_T$ of the form $g_1^{k_1} g_2^{k_2} \\ldots g_p^{k_p}$. It is easy to see that such a form needs to have the lowest possible $T$-count, so that $U$ and $U_T$ have the same $T$-complexity. Indeed, otherwise $U$ could be $\\epsilon$-approximated by a word with the $T$-count smaller than that of (\\ref{eq:rwT}). Similarly, for other cases and vice versa. Hence, the supremum of $T$-complexities over all operations $U$ in $\\mathbf{U}(d)$ is the same for $\\mathcal{S}$ and $\\mathcal{S}_T$ and equals $\\ell(\\mathcal{S}_T,\\epsilon)$."}
{"input": "\\section{Numerical experiments - methods} \\label{app:num_expl} % \\color{red} % Describe here briefly the numerics methodology [PIOTR] - e.g. adjoints, irrep decomposition, hear; we need to stress that doing such experiments is non-trivial. %\\color{black} In order to obtain the value of $Q(\\mathcal S, \\epsilon)$, one needs to computate the norm $\\delta(\\nu_\\mathcal{S},t) =\\left\\|T_{\\nu_\\mathcal{S},t}-T_{\\mu,t}\\right\\|_{\\infty}$ (see \\eqref{eq: t_moment} and equation above). In a naive approach, one could compute $U^{t, t} = U^{\\otimes t}\\otimes\\bar{U}^{\\otimes t}$ for each $U$ in $\\mathcal{S}$, but performing such calculation is exponentialy hard in $t$.\\\\ This problem can be avoided by noticing that the mapping $U \\mapsto U^{t,t}$ is a representation of the $SU(d)$ group onto $\\mathbb{C}^{2dt}$. Every representation of $SU(d)$ can be expressed as a block diagonal matrix, where each block is some irreducible representation (irrep) of $SU(d)$ \\cite{barut}. In our case, it reads \\begin{gather} U^{t,t} = \\blockdiagonal{\\pi_{\\lambda_1}(U)}{\\pi_{\\lambda_2}(U)}{\\pi_{\\lambda_k}(U)}, \\end{gather} where $\\pi_\\lambda$ is an irrep with label $\\lambda$ (more on that later). It follows that the $t$-moment operators are block diagonal as well, and their blocks are given by $T_{\\nu, \\lambda} = \\int_G d\\nu(U) \\pi_\\lambda(U)$. Furthermore, by the orthogonality of irreps \\cite{barut}, the Haar measure blocks $T_{\\mu, \\lambda}$ are equal to zero for all irreps $\\pi_\\lambda$, except the trivial one $\\pi_0(U)=1$. In summary, the value of $\\delta(\\nu_\\mathcal{S}, t)$ can be computed as \\begin{gather} \\max_\\lambda \\|T_{\\nu_\\mathcal{S}, \\lambda} - T_{\\mu, \\lambda}\\|_\\infty =\\max_{\\lambda \\neq 0} \\|T_{\\nu_\\mathcal{S}, \\lambda}\\|_\\infty, \\end{gather} where maximization is performed over all unique irreps appearing in the decomposition of $U^{t,t}$."}
{"input": "quantum number $s \\le t$. For $d\\ge2$, the irreps are labeled by the $d-1$-dimensional generalizations of a spin number (e.g. the Young tableaus), and thus, more complicated conditions are required \\cite{Dulian_2023, Dulian_2024, benkart1994, barut}. In either case, the dimensions of $\\pi_\\lambda$ are $\\mathcal O (t^{d(d-1)/2})$ and thus the norms $\\|T_{\\nu_\\mathcal{S}, \\lambda}\\|_\\infty$ can be computed efficiently. % \\begin{equation} % \\begin{tabular}{|c|c|c|} % \\hline % \\hspace{5pt} & \\hspace{5pt} & \\hspace{5pt} \\\\\\hline % \\end{tabular},\\quad % \\begin{tabular}{|c|c|} % \\hline % \\hspace{5pt} & \\hspace{5pt} \\\\\\hline % \\hspace{5pt} \\\\\\cline{1-1} % \\end{tabular},\\quad % \\begin{tabular}{|c|c|c|c|} % \\hline % \\hspace{5pt} & \\hspace{5pt} & \\hspace{5pt} & \\hspace{5pt} \\\\\\hline % \\hspace{5pt} & \\\\\\cline{1-2} % \\end{tabular},\\quad % \\begin{tabular}{|c|c|c|} % \\hline % \\hspace{5pt} & \\hspace{5pt} & \\hspace{5pt} \\\\\\hline % \\hspace{5pt} & \\hspace{5pt} & \\hspace{5pt} \\\\\\cline{1-3} % \\end{tabular}."}
{"input": "the unique solution to $G(\\gamma) = 0$; see \\eqref{eq:Groot} for the definition of the function $G(\\gamma)$. Conditional on deep but standard conjectures, it was previously shown that $h_3(d) \\ll_\\epsilon |d|^{1/4 + \\epsilon}$, see the work of Wong \\cite{Wong} and of Shankar--Tsimerman \\cite{ShankarTsi}. For more general number fields, pointwise bounds are known in a limited number of settings \\cite{BSTTTZ, EV, KlunersWang, LOZ, PTW2, Wang-Nilpotent, Wang}. Our proof strategy synthesizes ideas from many of the previous works in the area, in particular we rely on ideas of Ellenberg--Venkatesh \\cite{EV}, Heath-Brown--Pierce \\cite{HBP}, Frei--Widmer \\cite{FW}, Koymans--Thorner \\cite{KT} and Helfgott--Venkatesh \\cite{HV}. The starting point of the field is (independent) insights of Michel and Soundararajan that $h(d)/h_\\ell(d)$ can be lower bounded in the presence of sufficiently many small split primes. This was first set on firm ground in a very influential lemma of Ellenberg--Venkatesh \\cite[Lemma 3]{EV}. In order to progress further, it was soon realized that further improvements to the aforementioned ``Ellenberg--Venkatesh lemma'' were pivotal. This is typically done by allowing for more small split primes, but this comes at the price that one needs to keep track of relations (inside the class group) between the resulting small split primes. For imaginary quadratics, this was codified in Heath-Brown--Pierce \\cite[Proposition 2.1]{HBP} by using a Cauchy--Schwarz type argument, while this was done in a slightly different form for general number fields in Frei--Widmer \\cite[Proposition 2.1]{FW}. A key insight of Koymans--Thorner \\cite{KT} is that both approaches may be fruitfully combined to give a very flexible Ellenberg--Venkatesh lemma."}
{"input": "the state of the art being the results in Lemke Oliver--Smith \\cite{LOS}. For imaginary quadratic fields, the strongest currently available average bounds are due to Heath-Brown--Pierce \\cite[Theorem 1.1]{HBP}. Our next result extends their work to real quadratic fields. \\begin{theorem} \\label{t2} Let $\\ell \\geq 5$ be a prime and let $\\epsilon > 0$. Then there exists $C > 0$ such that for all $X \\geq 1$ there holds $$ \\sum_{\\substack{0 < d < X \\\\ d \\textup{ squarefree}}} h_\\ell(d) \\leq C X^{\\frac{3}{2} - \\frac{3}{2\\ell + 2} + \\epsilon}. $$ \\end{theorem} Theorem \\ref{t2} will fall as a consequence of Theorem \\ref{tEV} and a straightforward adaptation of \\cite[Proposition 2.3]{HBP}. This argument will be carried out in Section \\ref{sBonusT}. %Similarly to \\cite[Corollary 1.2]{KT}, Theorem \\ref{t2} can be used to improve the best known bounds on the number of $D_p$-extensions of $\\Q$. \\subsection*{Acknowledgements} PK gratefully acknowledges the support of the Dutch Research Council (NWO) through the Veni grant ``New methods in arithmetic statistics''."}
{"input": "\\section{Bounding torsion in real quadratic fields} \\label{sKT} Let $K$ be a number field. Let $\\mathcal{P}_K^{(1)}$ be the prime ideals of $K$ with inertia and residue field degree $1$. We define $$ \\pi_K^{(1)}(Z) := \\{\\mathfrak{p} \\in \\mathcal{P}_K^{(1)} : N_{K/\\Q}(\\mathfrak{p}) \\leq Z\\}. $$ If $K = \\Q(\\sqrt{d})$ is a real quadratic number field for some squarefree $d > 0$, we write $O_d$ for its ring of integers, $\\sigma$ for a generator of $\\Gal(K/\\Q)$ and $\\pi_K^{(1)}(Z) = \\pi_d(Z)$. We introduce for every prime number $\\ell$ and every real number $Z > 0$ the set $$ S_\\ell(d, Z) := \\left\\{\\beta = u + v \\sqrt{d} \\in O_d : \\begin{array}{l} |u| \\leq 2e^{3/2} Z^\\ell, \\quad |v| \\leq 2e^{3/2} Z^\\ell d^{-1/2} \\\\ \\beta O_d = (\\mathfrak{p}_1 \\sigma(\\mathfrak{p}_2))^{\\ell} \\text{ for some distinct } \\mathfrak{p}_1, \\mathfrak{p}_2 \\in \\pi_d(Z) \\end{array} \\right\\}. $$ We will frequently use the abbreviation $\\mathrm{Cl}_d := \\mathrm{Cl}(\\Q(\\sqrt{d}))$. Our next theorem is inspired by the argument from \\cite[Theorem 3.3]{KT}. \\begin{theorem} \\label{tEV} Let $\\ell \\geq 2$ and $Z > 0$. Then we have for all squarefree $d > 1$ with $|\\pi_d(Z)| > 0$ $$ |\\mathrm{Cl}_d[\\ell]| \\ll \\frac{d^{1/2} \\log d}{|\\pi_d(Z)|} + \\frac{d^{1/2} \\log d}{|\\pi_d(Z)|^2} \\cdot |S_\\ell(d, Z)|. $$ The implied constant is absolute. \\end{theorem} \\begin{proof} Let $R_d$ be the regulator of $\\Q(\\sqrt{d})$, and define $A := \\mathrm{Cl}_d/\\mathrm{Cl}_d[\\ell]$. By work of Louboutin \\cite{Louboutin} we have $$ |\\mathrm{Cl}_d[\\ell]| \\cdot |A| \\cdot R_d = |\\mathrm{Cl}_d| \\cdot R_d \\ll d^{1/2} \\log d."}
{"input": "100$ (see \\cite[Eq. (1.1)]{Regulator}), which we henceforth assume. Define $n := \\lfloor R_d \\rfloor \\geq 2$. We now subdivide $\\mathcal{F}$ into $n$ intervals of equal length, i.e.~we define for each $i \\in \\{0, \\dots, n - 1\\}$ the set $$ \\mathcal{F}_i = \\left\\{\\lambda \\mathbf{u} : \\frac{i}{n} \\leq \\lambda < \\frac{i + 1}{n}\\right\\}. $$ We fix a complete set of representatives $\\mathfrak{b}_1, \\dots, \\mathfrak{b}_h$ for $\\mathrm{Cl}_d$. Then for each prime ideal $\\mathfrak{p} \\in \\pi_d(Z)$, there exists a unique class $\\mathfrak{b}_j$ such that $\\mathfrak{b}_j \\mathfrak{p}^\\ell$ is principal. We denote by $a$ the image of $\\mathfrak{b}_j$ inside $A$. We also fix a choice of $\\alpha$ with $\\mathfrak{b}_j \\mathfrak{p}^\\ell = (\\alpha)$ and we let $i$ be the unique integer such that $\\mathbf{v}_\\alpha \\in \\mathcal{F}_i$. Then we have constructed a map $$ \\psi: \\pi_d(Z) \\rightarrow A \\times \\{0, \\dots, n - 1\\}, \\quad \\quad \\psi(\\mathfrak{p}) = (a, i). $$ The Cauchy--Schwarz inequality states $$ |\\pi_d(Z)| = \\sum_{(a, i)} |\\psi^{-1}(a, i)| \\leq \\left(\\sum_{\\substack{(a, i) \\\\ \\psi^{-1}(a, i) \\neq \\varnothing}} 1\\right)^{1/2} \\left(\\sum_{\\substack{(a, i) \\\\ \\psi^{-1}(a, i) \\neq \\varnothing}} |\\psi^{-1}(a, i)|^2\\right)^{1/2}. $$ Hence we have $$ |A| \\cdot R_d \\geq |A| \\cdot n \\geq \\sum_{\\substack{(a, i) \\\\ \\psi^{-1}(a, i) \\neq \\varnothing}} 1 \\geq \\frac{|\\pi_d(Z)|^2}{\\sum\\limits_{\\substack{(a, i) \\\\ \\psi^{-1}(a, i) \\neq \\varnothing}} |\\psi^{-1}(a, i)|^2}. $$ Going back to equation \\eqref{eClaim}, we thus have to show that $$ \\sum_{\\substack{(a, i) \\\\ \\psi^{-1}(a, i) \\neq \\varnothing}} |\\psi^{-1}(a, i)|^2 \\leq |\\pi_d(Z)| + |S_\\ell(d, Z)|. $$ Let us now split the contribution of $|\\psi^{-1}(a, i)|^2 = |\\psi^{-1}(a, i) \\times \\psi^{-1}(a, i)|$ into two pieces."}
{"input": "Firstly, the diagonal elements $(\\mathfrak{p}, \\mathfrak{p})$ contribute exactly $|\\pi_d(Z)|$. Secondly, for off-diagonal elements $(\\mathfrak{p}_1, \\mathfrak{p}_2)$ with $\\mathfrak{p}_1 \\neq \\mathfrak{p}_2$, we can do the following. Since $\\mathfrak{p}_1$ and $\\mathfrak{p}_2$ have the same class $a$ in $\\mathrm{Cl}_d/\\mathrm{Cl}_d[\\ell]$, it follows that $\\mathfrak{p}_1/\\mathfrak{p}_2$ is $\\ell$-torsion. Hence $\\mathfrak{p}_1^\\ell$ and $\\mathfrak{p}_2^\\ell$ are equivalent in the class group. Hence there is a class $\\mathfrak{b}_j$ and elements $\\alpha_1, \\alpha_2$ with $\\mathfrak{b}_j \\mathfrak{p}_1^\\ell = (\\alpha_1)$ and $\\mathfrak{b}_j \\mathfrak{p}_2^\\ell = (\\alpha_2)$ and moreover $\\mathbf{v}_{\\alpha_1}, \\mathbf{v}_{\\alpha_2} \\in \\mathcal{F}_i$. We now define $$ \\beta := \\frac{\\epsilon_{\\alpha_1} \\alpha_1 \\sigma\\left(\\epsilon_{\\alpha_2} \\alpha_2\\right)}{N_{K/\\Q}(\\mathfrak{b}_j)} = u + v \\sqrt{d} \\in O_d, $$ where we recall that $\\sigma$ is the unique non-trivial element of $\\Gal(\\Q(\\sqrt{d})/\\Q)$. We expand \\begin{align*} \\varphi(\\epsilon_{\\alpha_1} \\alpha_1) &= \\mathbf{v}_{\\alpha_1} + y_{\\alpha_1} \\mathbf{p} \\\\ \\varphi(\\epsilon_{\\alpha_2} \\alpha_2) &= \\mathbf{v}_{\\alpha_2} + y_{\\alpha_2} \\mathbf{p} \\\\ \\varphi(\\sigma(\\epsilon_{\\alpha_2} \\alpha_2)) &= -\\mathbf{v}_{\\alpha_2} + y_{\\alpha_2} \\mathbf{p} \\\\ \\varphi(N_{K/\\Q}(\\mathfrak{b}_j)) &= \\log(N_{K/\\Q}(\\mathfrak{b}_j)) \\mathbf{p}. \\end{align*} By \\eqref{eBasisChange}, we observe that $y_{\\alpha_i} = \\log(|N_{K/\\Q}(\\alpha_i)|)/2$. % as $y_{\\alpha_i}$ is the projection of $\\varphi(\\alpha_i)$ on the line generated by the vector $\\mathbf{p}$. From this, we conclude that $$ \\varphi(\\beta) = \\mathbf{v}_{\\alpha_1} - \\mathbf{v}_{\\alpha_2} + \\frac{\\ell \\log(N_{K/\\Q}(\\mathfrak{p}_1 \\mathfrak{p}_2))}{2} \\mathbf{p}. $$ Since $\\mathbf{v}_{\\alpha_1}, \\mathbf{v}_{\\alpha_2} \\in \\mathcal{F}_i$, it follows that $\\mathbf{v}_{\\alpha_1} - \\mathbf{v}_{\\alpha_2} = \\delta \\mathbf{u}$ with $|\\delta| \\leq \\frac{1}{n}$. By \\eqref{eBasisChange}, this gives $$ \\left|\\log |\\beta|_{v_1} - \\log |\\beta|_{v_2}\\right| = 2R_d |\\delta| \\leq \\frac{2R_d}{n} \\leq 3 $$ since $n = \\lfloor R_d \\rfloor$ and $R_d \\geq 2$."}
{"input": "\\section{Integral points on elliptic curves with bounded height} \\label{sHV} Let $D$ be a non-zero integer and let $E_D: y^2 = x^3 + D$. We write $$ E_D(\\Z) \\coloneqq \\{(x, y) \\in \\Z^2 : y^2 = x^3 + D\\}. $$ Write $\\overline{\\Q}$ for the algebraic closure of $\\Q$. Denote the canonical height by \\[ \\hat{h}: E(\\overline{\\Q})\\rightarrow \\R_{\\geq 0}, \\qquad \\hat{h}(P) \\coloneqq \\frac{1}{2} \\lim_{n\\rightarrow\\infty} \\frac{h(x(2^nP))}{4^n}, \\] where $h$ denotes the absolute logarithmic height on $\\overline{\\Q}$. To obtain an upper bound for the number of integral points on $E_D$ with bounded height, we apply \\cite[Theorem 3.8]{HV}, which works in a much more general setting. For the convenience of the reader, we give a more direct proof specialized to our case based on \\cite{Helfgott}. Define for $0 < t < 1$ and $0 < \\gamma < 1$ the functions \\begin{equation} \\label{eq:deffg} f(t) \\coloneqq \\frac{1 + t}{2t} \\log\\frac{1 + t}{2t} - \\frac{1 - t}{2t} \\log\\frac{1 - t}{2t}\\quad\\text{and}\\quad g(\\gamma) \\coloneqq f\\left(\\frac{1}{2} \\sqrt{(1 + \\gamma) (3 - \\gamma)}\\right). \\end{equation} \\begin{theorem} \\label{theorem:intbound} There exists $C > 0$ such that the following holds. Let $\\epsilon > 0$ and $0 < \\gamma < 1$. Let $D$ be an integer such that the sixth-power free part of $D$ is at least $\\exp(C/\\epsilon)$. Then we have for all $Z \\geq |D|^{\\frac{1}{6}}$ \\begin{align*} \\#\\left\\{(x, y)\\in E_D(\\Z): |x|\\leq Z^2\\right\\} &\\leq \\#\\left\\{P\\in E_D(\\Z): \\hat{h}(P) \\leq \\log Z + C\\right\\}\\\\ &\\ll_{\\epsilon, \\gamma} Z^{\\gamma + \\epsilon} \\times \\exp\\left(\\left(g(\\gamma) + \\epsilon\\right) \\rank_{\\Z} E_D(\\Q)\\right). \\end{align*} \\end{theorem} We give the following estimate for the canonical height."}
{"input": "+ \\hat{h}(P_2)+ \\max_{i\\in \\{1, 2\\}} \\hat{h}(P_i) - \\log m +C. \\] \\end{lemma} \\begin{proof} From the addition formula for points in $E_D(\\Q)$, we have \\[ x(P_1 + P_2) = \\left(\\frac{y_1 - y_2}{x_1 - x_2}\\right)^2 - x_1 - x_2 = \\frac{x_1^2x_2 + x_1x_2^2 - 2y_1y_2 + 2D}{(x_1 - x_2)^2}. \\] To bound $\\hat{h}(P_1 + P_2)$, we apply Lemma~\\ref{lemma:heightest} with $a = x_1^2x_2 + x_1x_2^2 - 2y_1y_2 + 2D$ and $b = (x_1 - x_2)^2$, and we compute \\begin{align*} \\max\\{|a^3|,|b^3D|\\} &=\\max\\{|x_1^2x_2+x_1x_2^2-2y_1y_2+2D|,|D|^{\\frac{1}{3}}(x_1-x_2)^2\\}^3\\\\ &\\ll \\max\\{|x_1^2x_2|,|x_1x_2^2|,|y_1y_2|,|D|,|D|^{\\frac{1}{3}}x_1^2,|D|^{\\frac{1}{3}}x_2^2\\}^3\\\\ &\\ll \\max_{\\{i,j\\}=\\{1,2\\}}\\max\\{|x_i|^3,|D|\\}^2 \\max\\{|x_j|^3,|D|\\}. \\end{align*} Since $(x_1, y_1) \\equiv (x_2, y_2) \\bmod m$, we see that $m \\mid \\gcd(y_1 - y_2, x_1 - x_2)$, and therefore we plainly have $m^2 \\mid b$. But we also have $$ a = (y_1 - y_2)^2 - (x_1 + x_2) (x_1 - x_2)^2 \\equiv 0 \\bmod m^2. $$ We conclude that $m^2 \\mid \\gcd(a, b)$. Let $g \\coloneqq \\gcd(x_1^3, D) = \\gcd(x_2^3, D)$. One checks that $g^3 \\mid (x_i^3)^2 x_j^3 = (x_i^2 x_j)^3$, so $g \\mid x_i^2x_j$. Moreover, we have $g \\mid y_1^2$ and $g \\mid y_2^2$, hence $g^2 \\mid y_1^2 y_2^2$ and $g \\mid y_1y_2$. We deduce that $g \\mid x_1^2x_2 + x_1x_2^2 - 2y_1y_2 + 2D$. Moreover, we have $g^3 \\mid (x_1 - x_2)^6 D$ by expanding $(x_1 - x_2)^6$ with the binomial theorem and using the previously obtained divisibilities. Since $m$ and $g$ are coprime, we can combine our divisibility conditions into $(m^2g)^3 \\mid \\gcd(a^3, b^3D)$."}
{"input": "\\left\\{2\\hat{h}(P_i) + \\hat{h}(P_j) + \\frac{1}{2}\\log g\\right\\} - \\frac{1}{2} \\log g - \\log m + C \\\\ &\\leq \\max_{\\{i, j\\} = \\{1, 2\\}} \\left\\{2\\hat{h}(P_i) + \\hat{h}(P_j)\\right\\} - \\log m + C \\end{align*} as required. \\end{proof} \\begin{prop}[{\\cite{KL}}] \\label{prop:spherepack} Let $A(n,\\theta)$ be the maximal number of points in $\\R^n$ such that the angle between any two points and the origin is at least $\\theta$. Let $f$ be the function as defined in \\eqref{eq:deffg}. Then for $0<\\theta<\\pi/2$, we have \\[ \\log A(n,\\theta) \\leq n\\left( f(\\sin\\theta) + o(1)\\right), \\] where the convergence as $n \\rightarrow \\infty$ is uniform for $\\theta$ within any closed subinterval of $(0,\\pi/2)$. \\end{prop} It follows from \\cite{Silvermanlowerbounds} that there exist absolute constants $c_1, c_2 > 0$ satisfying \\[ \\hat{h}(P) \\geq c_1\\log |D'| - c_2, \\] where $D'$ is the sixth-power free part of $D$ and $P$ is any non-torsion in $E_D(\\Q)$. We take $\\gamma$ and $\\epsilon > 0$ as in Theorem \\ref{theorem:intbound}. Note that we may assume without loss of generality that $\\epsilon \\leq 1/4$. Let $c_1\\log |D'| - c_2\\leq B\\leq \\log Z$ be a number and choose a prime $p \\nmid D$ such that $B(\\gamma-\\epsilon)\\leq\\log p\\leq B(\\gamma+\\epsilon)$, which is possible as long as $\\epsilon\\log |D'|$ is sufficiently large, so that $\\epsilon B$ is sufficiently large. Suppose $P_1$ and $P_2$ satisfy the assumptions of Lemma~\\ref{lemma:gapprinciple}, and are such that \\begin{align} \\label{eDyadic} (1 - \\epsilon) B < \\hat{h}(P) \\leq B."}
{"input": "\\section{Proof of main theorem} \\label{sMainT} The goal of this section is to prove Theorem~\\ref{theorem:main}. We begin by recalling the corresponding version of Theorem~\\ref{tEV} for imaginary quadratic fields from \\cite[Proposition 2.1]{HBP}. For a more general statement we refer the reader to \\cite[Theorem 3.3]{KT}. For $d < 0$, define \\[ S_\\ell(d, Z) \\coloneqq \\left\\{\\begin{array}{l} u + v \\sqrt{d} \\in O_d \\\\ u \\neq 0, v \\neq 0 \\end{array} : (p_1p_2)^\\ell = u^2 - dv^2\\text{ for some primes } p_1, p_2 \\leq Z\\right\\}. \\] \\begin{theorem} \\label{theorem:neg} There exists $C > 0$ such that for all primes $\\ell$, for all $Z > 0$ and for all squarefree $d < -1$ with $|\\pi_d(Z)| > 0$ $$ |\\mathrm{Cl}_d[\\ell]| \\leq \\frac{C |d|^{1/2} \\log |d|}{|\\pi_d(Z)|} + \\frac{C |d|^{1/2} \\log |d|}{|\\pi_d(Z)|^2} \\cdot |S_\\ell(d, Z)|. $$ \\end{theorem} \\begin{proof} This is similar to \\cite[Proposition 2.1]{HBP}, although it does not explicitly follow from the work there. Therefore we have opted to prove this from scratch. Consider the exact sequence $$ 0 \\rightarrow \\mathrm{Cl}_d[\\ell] \\rightarrow \\mathrm{Cl}_d \\xrightarrow{\\cdot \\ell} \\ell \\mathrm{Cl}_d \\rightarrow 0. $$ By the class number formula and work of Louboutin \\cite{Louboutin}, we deduce that \\begin{align} \\label{eBS} |\\mathrm{Cl}_d[\\ell]| = \\frac{|\\mathrm{Cl}_d|}{|\\ell \\mathrm{Cl}_d|} \\ll \\frac{|d|^{1/2} \\log |d|}{|\\ell \\mathrm{Cl}_d|}. \\end{align} We now give a lower bound for $|\\ell \\mathrm{Cl}_d|$. Consider the map $\\psi: \\pi_d(Z) \\rightarrow \\ell \\mathrm{Cl}_d$, which sends a prime ideal $\\mathfrak{p}$ to the class $[\\mathfrak{p}^\\ell]$."}
{"input": "\\in \\Z_{\\neq 0}$. Then there exist primes $p_1, p_2$ such that $4(p_1p_2)^3 = u'^2 - dv'^2$. This corresponds to a point $(4p_1p_2, 4u') \\in E_{16dv'^2}(\\Z)$, with $p_1p_2 \\leq Z^2$ and $|d| v'^2 \\leq 16 e^3 Z^6$ (this bound is automatic for $d < 0$ and guaranteed by Theorem \\ref{tEV} for $d > 0$). We apply Theorem~\\ref{theorem:intbound} to bound the points on each $E_{16dv'^2}$, which yields the estimate \\begin{align} \\label{eS3dZ} |S_3(d, Z)| \\ll_{\\epsilon, \\gamma} Z^{\\gamma + \\epsilon} \\times \\sum_{0 < |v'| \\leq 4 e^{3/2} Z^3 |d|^{-1/2}}\\exp\\left(\\left(g(\\gamma)+\\epsilon\\right) \\rank_{\\Z} E_{16dv'^2}(\\Q)\\right). \\end{align} We now draw upon an upper bound for the rank from \\cite[Proposition 2]{Fouvry}, namely \\[ \\rank_{\\Z} E_{16dv'^2}(\\Q) \\leq A + B\\omega(2dv'^2) + \\frac{2\\log |\\mathrm{Cl}_d[3]|}{\\log 3}, \\] where $A$ and $B$ are absolute constants. Putting this upper bound into equation \\eqref{eS3dZ} yields the estimate \\[ |S_3(d, Z)| \\ll_{\\epsilon, \\gamma} Z^{\\gamma + 2\\epsilon} \\times Z^3 |d|^{-\\frac{1}{2}}\\times |\\mathrm{Cl}_d[3]|^{\\frac{2(g(\\gamma)+\\epsilon)}{\\log 3}}. \\] Using the trivial bound \\eqref{ePointwise}, we certainly have $ |\\mathrm{Cl}_d[3]|^{\\epsilon}\\ll d^{\\epsilon}$. Substituting this into \\eqref{eq:cl1}, we obtain \\begin{equation} \\label{eq:cl2} |\\mathrm{Cl}_d[3]| \\ll_{\\epsilon, \\gamma} Z^{20 \\epsilon} \\left(\\frac{|d|^{1/2}}{Z} + Z^{1 + \\gamma} \\times |\\mathrm{Cl}_d[3]|^{\\frac{2g(\\gamma)}{\\log 3}}\\right). \\end{equation} To minimize this bound, take \\[ Z = \\left(|d|^{\\frac{1}{2}}|\\mathrm{Cl}_d[3]|^{-\\frac{2g(\\gamma)}{\\log 3}}\\right)^{\\frac{1}{2 + \\gamma}} + |d|^{\\frac{1}{6}}, \\] where the term $|d|^{\\frac{1}{6}}$ ensures that $Z$ satisfies \\eqref{eq:lbZ}. With this choice of $Z$ and rescaling $\\epsilon$, the bound \\eqref{eq:cl2} becomes \\[ |\\mathrm{Cl}_d[3]| \\ll_{\\epsilon, \\gamma} |d|^{\\frac{1}{2}(1 - {\\frac{1}{2+\\gamma})+\\epsilon}} |\\mathrm{Cl}_d[3]|^{\\frac{2g(\\gamma)}{(2 + \\gamma)\\log 3}} + |d|^{\\frac{1}{6}(1 + \\gamma) + \\epsilon} \\times |\\mathrm{Cl}_d[3]|^{\\frac{2g(\\gamma)}{\\log 3}}."}
{"input": "\\section{Introduction} \\label{intro} Transformer-based Large Language Models (LLMs) have demonstrated remarkable generalization across diverse natural language processing (NLP) tasks by leveraging broad, heterogeneous datasets~\\cite{transformer}. However, deploying these models for domain-specific applications remains challenging. Foundation models are typically large, often with hundreds of billions to trillions of parameters~\\cite{borealis2024highlevel, b1, llama3}, requiring substantial computational resources~\\cite{upstagesolar, llama3, nvidia70}. Additionally, general-purpose LLMs often fail to capture fine-grained domain knowledge due to their generic training data~\\cite{polycoder, codegen}. This challenge has driven the demand for \\textit{domain-specialized, resource-efficient models} that retain strong task performance while reducing computational costs~\\cite{csr, mcq, problemstatementpaper}. Recent advances in model optimization techniques, such as pruning and parameter-efficient fine-tuning, have enabled the adaptation of LLMs to specialized domains with reduced resource overhead~\\cite{llmpruner}. However, these approaches rely heavily on the availability of high-quality domain-specific datasets, which are often scarce or expensive to curate. The bottleneck in domain adaptation thus lies not only in optimizing model architectures but also in constructing an \\textit{effective domain-specific dataset} from limited initial supervision. Given the wide range of potential application domains, manually curating datasets for every scenario is impractical, raising the critical question: \\textit{How can we efficiently construct a domain-specific training dataset using minimal human supervision while maintaining adaptability and scalability?} Recent work has shown that large-scale instruction tuning may not always be necessary for effective fine-tuning."}
{"input": "\\textbf{FineScope}, a novel self-data cultivation framework that learns computation-efficient domain-specific language models by automating domain-specific dataset construction. FineScope utilizes a \\textit{Sparse Autoencoder (SAE)} to learn a compressed representation of domain-relevant features~\\cite{anthropy, kissane2024interpreting, yan2024encourage}. Unlike typical SAEs which model all neuron activities, FineScope learns our SAE using only top-$k$ activations, which selects \\textit{semantically similar} data points from a large, general-purpose corpus, forming a domain-optimized dataset. It allows FineScope to learn latent domain structures rather than static embedding similarity, making it more scalable and adaptable across diverse application domains. The curated dataset is then used to fine-tune a \\textit{pruned domain-specialized LLM}, ensuring both computational efficiency and high task relevance. To aid the fine-tuning process of the pruned models, the FineScope further refines the curated dataset using a \\textit{self-data distillation} technique~\\cite{sdft}, allowing the models with fewer parameters to learn the target domain knowledge seamlessly. We evaluate FineScope across multiple domain-specific NLP tasks and demonstrate that it achieves strong domain adaptation while significantly reducing the computational cost. Our results show that models fine-tuned with SAE-curated datasets consistently outperform conventional fine-tuning approaches, achieving higher accuracy with smaller model sizes. Furthermore, FineScope enables fine-grained multi-domain specialization, allowing LLMs to adapt to different subdomains without retraining from scratch. For example, testing on the Math dataset showed an average performance improvement of 11.45 across all models. Our key contributions are as follows: \\begin{itemize} \\item We introduce \\textbf{FineScope}, a novel framework for \\textbf{self-supervised domain dataset curation}, enabling scalable domain adaptation from a minimal seed dataset."}
{"input": "can not provide high accuracy for all these domains at the same time. So, the need for domain-specific LLMs is highly important in recent times. % changed % On the other hand, with the GPU capacity increasing significantly, researchers have been developing heavier LLMs with immense number of parameters for better performance and scalability like \\cite{upstagesolar,llama3,nvidia70} etc.. The increasing number of parameters in LLMs, making it difficult to access and it limits the research as everyone does not have the access of high-end GPUs. Moreover, applying these LLMs onto a certain task can also cause higher inaccuracy for a certain domain due to its heterogeneous training data and settings. Due to this challenge, there have been many works on the specialization of domain based tasks. % problem paper statement for reference : our analysis reveals that although LLMs have made remarkable advancements in generating code for open-domain applications, their performance degrades sharply when applied to specific domains. For ChatGPT, the CodeBLEU score drops by 51.48\\% on average. We particularly notice that this is often caused by a lack of domain knowledge, particularly the misuse of third-party libraries. Particularly, the challenges we are considering in this work are as follows : \\textbf{Challenge-1:} Even though in recent years, there have been many advancements in curation of domain-specific datasets, these datasets still lack granularity required for deep domain knowledge."}
{"input": "like maths, physics, chemistry, biology. LLMs lack these detailed domain knowledge due to imbalance in datasets. This imbalance leads LLMs to develop biases toward specific subjects, resulting in strong performance in some areas while lacking accuracy in others. Additionally, curating these datasets is challenging due to the abundance of available datasets and the difficulty of filtering out irrelevant data points for a given domain. \\\\ \\textbf{Challenge-2:} With the GPU capacity increasing significantly, researchers have been developing heavier LLMs with immense number of parameters for better performance and scalability like \\cite{upstagesolar,llama3,nvidia70} etc. However, it is getting harder to train and experiment with these models effectively. Therefore, addressing these challenges is essential to accelerate the advancement of LLMs and AI as a whole. The increasing number of parameters in LLMs, making it difficult to access and it limits the research due to extensive computational cost. Moreover, applying these LLMs onto a certain task can also cause higher inaccuracy for a certain domain due to its heterogeneous training data and settings. Due to this challenge, there have been many works on the specialization of domain based tasks. \\\\ \\textbf{Challenge-3:} With the help of scaling laws, development of LLMs has been majorly effected~\\cite{borealis2024highlevel}. This also caused LLMs to be of hundreds of billions~\\cite{b1,llama3} and trillions parameters : a force behind advancement of Artificial Intelligence (AI). However, recently, there is a growing interest to make LLMs efficient and smaller in size and number of parameters. One common approach for this is pruning LLMs."}
{"input": "\\section{Related Work} \\label{RW} \\subsection{Domain-Specific Language Models} Domain-adapted LLMs have been developed to enhance performance in specialized fields by training on domain-specific datasets. Notable examples include PharmaGPT-13B/70B~\\cite{pharmagpt} for biopharmaceutical and chemical applications, SaulLM-54B/141B~\\cite{saullm} for legal tasks, Shai-10B~\\cite{shai} for asset management, BloombergGPT~\\cite{bloomberg} for financial analytics, and MedPalm~\\cite{medpalm} for medical question answering. Other models such as ClimateBERT~\\cite{climatebert}, ChatLaw~\\cite{chatlaw}, and FinGPT~\\cite{fingpt} focus on climate science, legal text processing, and financial analysis, respectively. However, building such models heavily depends on access to high-quality domain-specific datasets, which are generally scarce and costly to obtain. Moreover, these models typically contain billions of parameters, making continuous retraining with updated datasets highly GPU-intensive. These constraints highlight the need for more efficient fine-tuning strategies that can effectively adapt LLMs to domain-specific applications without excessive reliance on human annotation or computational resources. \\subsection{Small Language Models (SLMs).} Although Large Language Models (LLMs) achieve state-of-the-art performance across various NLP tasks, Small Language Models (SLMs) have gained traction as a resource-efficient alternative. Several techniques have been explored to reduce model size and computational cost while maintaining performance. \\textit{Lightweight architectures} use parameter-efficient designs, such as MobileBERT~\\cite{mobilebert} and DistilBERT~\\cite{distilbert} for encoder-only models and GPT-based models~\\cite{gpt1, gpt2, llama, llama2, llama3} for decoder-only structures. \\textit{Quantization}~\\cite{quantization1st, gptq, llmqat, lora} reduces precision to lower-bit representations, improving efficiency with minimal accuracy degradation. \\textit{Weight tying}~\\cite{weighttying_ffn} further reduces parameter count by sharing weights across model components. \\textit{Pruning} has emerged as a widely used approach for reducing model complexity while preserving essential functionality."}
{"input": "subnetworks (``winning tickets'') that can achieve performance comparable to the original model. Various structured and unstructured pruning methods~\\cite{structured, taskstructured, other1, other2, other3, shearedllama} have been proposed to remove non-critical parameters while retaining the network's representational capacity. Unlike other optimization techniques, which require either learning models from scratch or reducing precision uniformly, pruning allows models to selectively retain essential components, making it particularly suited for domain adaptation. Thus, in this work, we leverage pruning as a core component of FineScope to construct computation-efficient, domain-specialized models. \\ifx{ \\textbf{Domain-specific models : } When developing domain-specific models, the process starts with deciding the proper domain, collecting the dataset and training the model for the selected domain (adaptive pre-training). To mitigate the issues mentioned in Section~\\ref{intro}, there are some existing models like PharmaGpt-13B/70B~\\cite{pharmagpt}, trained with bio-pharmaceutical and chemical domains' datasets, SaulLM-54B/141B~\\cite{saullm}, tailored for legal sectors, Shai-10B~\\cite{shai} for asset management industry domain, BloombergGPT~\\cite{bloomberg} for finance, MedPalm~\\cite{medpalm}, made for answering medical questions accurately, ClimateBert~\\cite{climatebert} which was trained on climate related texts and more~\\cite{chatlaw,fingpt}. However, these models have a large number of parameters, making retraining with updated datasets highly GPU-intensive, while acquiring the necessary datasets remains a challenge. \\\\ \\textbf{Small language models (SLMs) : } % https://arxiv.org/pdf/2410.20011 : reference for slm study [not a good paper buyt can take the references hihi] Even though LLMs have been known for highly impressive performance, SLMs are efficient while having competetive performance record. There are different ways to develop SLMs."}
{"input": "and quantization. \\textbf{\\textit{Lightweight architectures}} may include encoder only structures like BERT's~\\cite{bert} variant models : MobileBert~\\cite{mobilebert}, DistilBert~\\cite{distilbert} or decoder only models like GPT~\\cite{gpt1,gpt2}, LLaMa~\\cite{llama,llama2,llama3} and their variants which usually uses knowledge transfer, memory overhead optimization or parameter sharing like strategies. \\textbf{\\textit{Pruning}}~\\cite{lottery} was inspired by lottery ticket hypothesis which states a smaller sub network (``winning ticket'') which can achievce performance comparable with the original model when trained in isolation. Various pruning methods like~\\cite{structured,taskstructured,other1,other2,other3,shearedllama} which were somewhat able to provide competitive performance compared to the original models. \\textbf{\\textit{Quantization}}~\\cite{quantization1st} allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. In short it reduces the precision of the model’s weights and activations from 32-bit floating-point numbers to lower-bit representations such as 8-bit integer~\\cite{gptq,llmqat,lora}. \\textbf{\\textit{Weight tying}} is a technique where certain weights in the model are shared between different components. This approach not only reduces the total number of parameters but also ensures that certain parts of the model are better aligned.~\\cite{weighttying_ffn} shares weights specifically between the feed-forward network (FFN) layers. By doing so, it achieves parameter efficiency without compromising on the model’s ability to learn and generalize. However, despite these optimizations, the model loses a significant portion of its knowledge, which becomes a drawback when fine-tuning smaller models, as they may retain only arbitrary information, leading to inconsistencies when trained on new datasets."}
{"input": "\\section{Methodology} \\label{Method} Figure~\\ref{fig:overall} provides an overview of \\textbf{FineScope}, our proposed framework for domain-specific LLM adaptation. FineScope consists of two key stages: (1) \\textit{Domain-Specific Data Cultivation}, where we train a Sparse Autoencoder (SAE) on the Top-$K$ activations of a pretrained LLM and use it to curate a domain-specific dataset based on cosine similarity with a small set of user-defined seed samples; and (2) \\textit{Pruning-Aware Fine-Tuning with Self-Data Distillation}, where structured pruning is applied based on the curated dataset, followed by fine-tuning the pruned model using self-data distillation to further refine task performance. The following sections provide a detailed explanation of each stage. \\subsection{Domain-Specific Data Cultivation} \\label{saetrain}\\label{datasetcuration} \\textbf{Training SAE: } A Sparse Autoencoder (SAE) is a neural network designed to learn compressed representations of input data while enforcing sparsity constraints on the hidden units~\\cite{EleutherAI_SAE}. In our framework, SAEs serve as a mechanism for extracting domain-relevant features from pretrained LLM activations to identify domain-relevant samples from a large corpus. Instead of operating directly on the raw model outputs, we train SAEs on activations from intermediate layers of the LLM, allowing us to capture a structured, low-dimensional representation of the underlying knowledge encoded in the model. Additionally, since processing all activations are computationally infeasible for the large corpus, our SAEs is adapted to learn only the representative activations in a way that highlights the most significant neurons, improving interpretability while discarding less relevant signals."}
{"input": "= f^{(l)}\\left(act^{(l-1)}(x)\\right), \\text{for } l = 1, \\dots, L-1 \\end{equation} where $act^{(l)}(x)$ denotes the activations at layer $l$ given input $x$, and $f^{(l)}$ represents the transformation function at layer $l$, which typically includes multi-head self-attention, feed-forward operations, and residual connections. The final model output is computed as: \\begin{equation}\\label{output} y = \\text{softmax}\\left(f^{(L)}\\left(act^{(L-1)}(x)\\right)\\right). \\end{equation} To train the SAE, we extract activations $act^{(l)}(x)$ from a selected layer of the pretrained LLM and feed them into the encoder network: \\begin{equation}\\label{enc} \\text{Enc}\\left(act^{(l)}(x)\\right) = \\text{ReLU}\\left(W_e act^{(l)}(x) + b_e\\right). \\end{equation} The corresponding reconstruction from the decoder is given by: \\begin{equation}\\label{sae} \\text{SAE}\\left(act^{(l)}(x)\\right) = D^\\top \\text{Enc}\\left(act^{(l)}(x)\\right) + b_d. \\end{equation} Here, $\\text{Enc}$ represents the SAE’s encoder, $W_e$ and $b_e$ are the encoder’s weights and biases, and $D^\\top$ and $b_d$ correspond to the decoder parameters. The SAE is trained to minimize the reconstruction loss: \\begin{equation} \\begin{split} \\mathcal{L}_{\\text{SAE}} &= \\| \\text{SAE}(act^{(l)}(x)) - act^{(l)}(x) \\|^2 \\\\ &\\quad + \\lambda \\| \\text{Enc}(act^{(l)}(x)) \\|_1, \\end{split} \\end{equation} where the second term enforces sparsity by penalizing the activation magnitudes of the encoder. \\textbf{Top-K Activation Selection for Efficient SAE Training: } Rather than training the SAE on full-layer activations, we apply a \\textit{Top-K filtering mechanism} to select the most important activations before feeding them into the encoder. Given dataset $D$ containing $m$ samples, $D = \\{x_1, x_2, \\dots, x_m\\}$, we compute the $K$ most significant activations based on the gradient magnitude: \\begin{multline}\\label{topk_selection} \\text{TopK}(act^{(l)}(x), K) = \\{a_i \\in act^{(l)}(x) \\mid \\\\ a_i \\text{ corresponds to one of the } K \\text{ largest } \\left|\\frac{\\partial act^{(l)}_i(x)}{\\partial x}\\right|\\}."}
{"input": "the $TopK$ activations reduces the input dimensionality of the SAE, significantly lowering computational overhead and enabling efficient training on large-scale corpora. Additionally, filtering out noisy activations improves generalization by focusing the SAE on the most informative neurons, enhancing interpretability of extracted features aligned with domain-relevant information. The encoder then operates on these filtered activations as: \\begin{multline}\\label{enc_topk} \\text{Enc}\\left(\\text{TopK}(act^{(l)}(x), K)\\right) = \\\\ \\text{ReLU}\\left(W_e \\cdot \\text{TopK}(act^{(l)}(x), K) + b_e\\right). \\end{multline} For each layer $l$ in the LLM, we train a separate SAE, resulting in a total of $L$ SAEs. Once trained, these SAEs serve as \\textit{feature extractors for data curation}, which we describe in the next section. \\textbf{Dataset Curation: } Figure~\\ref{fig:data_curation} provides an overview of our dataset curation process using trained SAEs. Given a \\textit{small number of seed samples for a target domain} (e.g., around ten samples), our goal is to extract a subset $D_s \\subseteq U$ from a larger mixed-domain dataset $U$ by selecting samples that are most similar to the seed samples $D_t$ in the learned embedding space. \\begin{figure}[!h] \\centering \\includegraphics[width=1\\linewidth]{Figures/init_rev_.jpg} \\caption{Dataset curation using SAE-based embeddings and cosine similarity.} \\label{fig:data_curation} \\end{figure} Using the trained SAE, we compute embeddings for all samples in both $D_t$ and $U$. The embeddings for the target domain are given by $ E_t = \\{\\text{SAE}(x) \\mid x \\in D_t\\}$. Similarly, we compute embeddings for all samples in the larger dataset $U$, $E_U = \\{\\text{SAE}(x) \\mid x \\in U\\}$."}
{"input": "pruning to remove redundant model components while maintaining computational efficiency. This approach ensures that pruned models retain domain-relevant representations while significantly reducing inference cost. Formally, let $\\mathcal{M}$ represent the full model and $\\mathcal{A}(D_s, m)$ denote an activity function that quantifies the contribution of each component $m \\in \\mathcal{M}$ to the dataset $D_s$. We define the pruned model $\\mathcal{M}_r$ as: \\begin{equation} \\mathcal{M}_r = \\{m \\in \\mathcal{M} \\mid \\mathcal{A}(D_s, m) > \\tau\\}, \\end{equation} where $\\tau$ is a predefined threshold determining which components are retained. The pruning ratio, denoted as $r$, is treated as a hyperparameter and optimized to balance model size and performance. \\textbf{Self-Data Distillation and Fine-Tuning: } Self-Data Distillation (SDFT) is a technique where a distilled dataset is generated to match the output distribution of the original model, effectively improving generalization. While SDFT is effective for knowledge transfer, its role in FineScope extends beyond dataset refinement. Since pruning removes a significant portion of the model’s parameters, it risks discarding essential representations that contribute to domain-specific knowledge. To mitigate this, we use self-data distillation to recover lost information and reinforce the pruned model by reintroducing high-confidence outputs from a stronger/unpruned teacher model. The pruned model may lose critical task-relevant features due to parameter reduction. Training on self-distilled outputs helps restore \\textit{soft knowledge} from the teacher model, ensuring that the pruned model retains domain-specific patterns without requiring additional labeled data."}
{"input": "32.77 & 29.87 & 20.35 \\\\ & Pruned & 11.41 & 7.99 & 5.01 \\\\ & Tuned $^\\ast$ & 9.23 & 5.56 & 9.10 \\\\ \\rowcolor{blue!13} & Tuned $^\\triangle$(Full FineScope) &$\\textbf{31.17}$ & $\\textbf{29.73}$ & $\\textbf{19.41}$ \\\\ \\midrule {GPT-3 (13B)~\\cite{gpt3}} &Pretrained& $6.80$ & $5.30$ & $4.50$ \\\\ \\midrule {GPT-3 (175B)~\\cite{gpt3}} &Pretrained(FS)& $7.70$ & $6.00$ & $4.70$ \\\\ \\bottomrule \\end{tabular} \\label{tab:math} \\end{table*} \\subsection{Experimental Setup} We evaluate FineScope under four experimental settings. \\textbf{\\textit{(1) Domain-Specific Tuning}} We prune models using our SAE-curated domain-specific dataset (Section~\\ref{datasetcuration}). SAEs are trained on the RedPajama dataset~\\cite{together2023redpajama}, which includes data from CommonCrawl, C4, GitHub, Wikipedia, Books3, ArXiv, and StackExchange, ensuring generalization across multiple domains. Using these SAEs, we curate a domain-specific dataset from OpenInstruct~\\cite{haku}, which comprises Alpaca~\\cite{albertying}, Self-Instruct~\\cite{selfinstruct}, GPT-4 Instruct, Roleplay~\\cite{GPTeacher}, Code Alpaca~\\cite{codealpaca}, Dolly~\\cite{DatabricksBlog2023DollyV2}, and other instruction datasets. We extract STEM, Social Sciences, and Humanities subsets, each curated from \\textit{15} GPT-4o-generated seed samples, where each curated dataset was augmented using Grok API~\\cite{grok}. The final curated dataset sizes are 2,703 (STEM), 3,327 (Social Sciences), and 3,522 (Humanities). \\textbf{\\textit{(2) Baseline Tuning}} We compare against two baselines: (a) Fine-tuning pretrained models with SAE-curated datasets. (b) Pruning the model and fine-tuning it with Alpaca data, using the same pruning strategy as in Section~\\ref{pruningtuning}. \\textbf{\\textit{(3) Baseline (Pretrained)}} Pretrained models are evaluated without fine-tuning, serving as a baseline. We compare against GPT-3 (6.7B, 175B)~\\cite{gpt3} and OLMO-7B~\\cite{olmo} (Table~\\ref{tab:model_results}), as well as GPT-3 (13B, 175B) (Table~\\ref{tab:math}). \\textbf{\\textit{(4) Sub-Domain Tuning (Math)}} To evaluate fine-tuning at a more granular level, we curate sub-domains within Math."}
{"input": "by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. % Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett. % Previous contributors include Dan Roy, Lise Getoor and Tobias % Scheffer, which was slightly modified from the 2010 version by % Thorsten Joachims & Johannes Fuernkranz, slightly modified from the % 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is % slightly modified from Prasad Tadepalli's 2007 version which is a % lightly changed version of the previous year's version by Andrew % Moore, which was in turn edited from those of Kristian Kersting and % Codrina Lauth. Alex Smola contributed to the algorithmic style files."}
{"input": "prediction. Finally, in the fourth step, SR module derives a series of expressions based on the recombined features. From the perspective of model architecture, the SA-GAT-SR framework consists primarily of two modules, i.e., the GNN module enhanced by self-adaptable encoding (SAE), and the SR solver module. The GNN module comprises an SAE layer, multiple message-passing and global feature updating layers, and a readout layer. The SAE layer is responsible for performing feature engineering, constructing the initial node, edge, and global feature vectors from atomic, bonding, and crystal cell features, respectively, based on ICs. The message-passing layers operate on the crystal graph, iteratively updating each feature vector. After processing through all updating layers, the readout layer produces the final output. The output from the GNN module serves as an initial approximation of the final prediction and is fed into the SR module, implemented by the sure independence screening and specifying operator (SISSO) method\\cite{bib37,bib38}. \\begin{figure}[H] \\centering \\includegraphics[width=1\\textwidth]{Figure1.pdf} \\caption{ \\textbf{The SA-GAT-SR model for materials prediction.} \\textbf{a} The flowchart of SA-GAT-SR model encompasses four sequential stages: data acquisition (blue), preliminary GNN prediction (yellow), feature screening (pink), and symbolic regression (brown). The combination of GNN prediction and reserved features in the feature screening step is used as the input features of SR module. \\textbf{b} The architecture of the GNN module. In the self-adaptable encoding (SAE) algorithm, the raw feature vector consists of scalar properties associated with atoms and unit cells from the crystal structure."}
{"input": "single-oxide and single-halogen perovskites with the general formulae $\\mathrm{ABO}_3$ and $\\mathrm{ABX}_3$ (X=F, Cl, Br, I) using the JARVIS-DFT\\cite{bib35} dataset (version 2021.8.18). This dataset comprises extensive materials and provides a comprehensive set of solid-state properties ideally suited for model training and evaluation. In total, we curated a subset of 689 perovskite materials for our studies. The hyperparameter configurations of the GNN and SR modules in the SA-GAT-SR model are summarized in Supplementary Table 1. The SR module takes a total of 15 input features, which include the prediction output from the GNN module. To simplify the final expressions and accelerate the SR derivation speed, we set the unified descriptors complexity and expression dimension to 1 and 3 for all prediction tasks, respectively. The train-validation-test split ratio was consistently maintained at 85\\%:10\\%:5\\% level. From the extensive set of properties available in this dataset, we select the following key attributes for evaluation: bandgaps corrected by the optimized Becke88 functional with van der Waals interaction ($\\text{OptB88vdW}$)\\cite{bib39}, formation energies, dielectric constants without ionic contributions ($\\epsilon_x$, $\\epsilon_y$, $\\epsilon_z$), Voigt bulk modulus (Kv) and shear modulus (Gv), total energies, and energies above the convex hull ($E_{hull}$). For the initial feature set, we select 7 physical quantities for each element and 9 physical quantities for crystal, resulting in a combined feature vector comprising 30 features for each crystal\\cite{bib4,bib19}."}
{"input": "SR module and the the number of initial features is 23. The symbols used in the SA-GAT-SR model are summarized in Table~\\ref{tabs} and their corresponding detailed information is provided in Supplementary Table 2. The octahedral factor $G_{\\mu}$ and tolerance factor $G_{t}$ are defined as $B_{ir}/r_{X}$ and $\\frac{{{A_{ir}} + {r_X}}}{{\\sqrt 2 ({B_{ir}} + {r_X})}}$, respectively, which are commonly used for describing perovskite structures, where the ${r_X}$ denotes the ionic radius of oxygen or halogen anion. Both features can, to a certain extent, reflect the stability of the entire crystal structure. Moreover, the ratio of these two features has been shown to exhibit a strong correlation with several functional properties of perovskite materials\\cite{bib3}. As a result, this ratio is an initial feature in our analysis. Multiple studies have demonstrated that lattice structural distortions in perovskite materials can significantly influence a range of their properties\\cite{bib36}. To characterize these distortion characteristics, we select a representative $BX_6$ octahedron and compute the mean B-X-B bond angle between its six neighboring octahedra, denoted as $\\varTheta_{BXB}$. To further enhance the stability and interpretability of the calculated values, we adopt the cosine of $\\varTheta_{BXB}$ as a distortion feature. This cosine-based measure is preferred as it maps the bond angle distortion to a range between -1 and 1, providing a naturally normalized representation of the structural distortion. Further analysis of these distortion features is provided in Supplementary Figs. 2 and 3."}
{"input": "superior performance compared to the control group models. The lowest MAE for bandgap ($E_{gap}$) with SA-GAT-SR is 0.147 eV, which outperforms GAT, CGCNN and ALIGNN by 19.7$\\%$, 18.3$\\%$ and 10.9$\\%$, respectively. Furthermore, the full SA-GAT-SR model builds upon this foundation, achieving additional improvements in accuracy. Without the predictions from the GNN module, the MAE for $E_{gap}$ of individual SR module struggles to achieve 0.768 eV accuracy. However, in certain cases, the SR module outperforms the GNN module, which is shown in Supplementary Table 4. The MAE for $E_{hull}$ using only the SR module is 0.075 eV outperforming the 0.090 eV achieved by the GNN module alone. To assess space complexity, we analyze the number of trainable parameters in each model. The SA-GAT-SR model contains a total of 6.303M parameters, primarily due to the feature embedding matrices, which scale with the size of the input feature set and are not involved in the model's training speed. In contrast, ALIGNN comprises only 4.027M parameters. Notably, the trainable core of SA-GAT-SR includes just 3.351M parameters. Moreover, when the hidden feature dimension is set to 128, the parameter count of the core model can be reduced to as few as 1.011M without a significant loss in prediction accuracy. \\begin{table}[h] \\caption{\\textbf{Regression model performances on the $\\mathrm{AB(O|X)}_3$ dataset for 9 properties using GAT, CGCNN, ALIGNN and SA-GAT-SR models.}}\\label{tabresOX} \\begin{tabular*}{\\textwidth}{@{\\extracolsep\\fill}lcccccccc} \\toprule% & \\multicolumn{5}{@{}c@{}}{} &\\multicolumn{3}{@{}c@{}}{Ours}\\\\\\cmidrule{7-9}% Property & \\#Mater."}
{"input": "689 & eV/at. & 0.515 & 0.240 & 0.105 & 0.109 & 0.318 & \\textbf{0.101}\\\\ Bandgap (OPT) & 689 & eV & 0.183 & 0.180 & 0.157 & 0.194 & 0.768 & \\textbf{0.147}\\\\ Total Energy & 689 & eV/at. & 0.280 & 0.145 & 0.123 & 0.126 & 0.644 & \\textbf{0.115}\\\\ $E_{hull}$ & 689 & eV & 0.129 & 0.078 & 0.071 & 0.064 & 0.161 & \\textbf{0.044}\\\\ Bulk Modulus Kv & 362 & GPa & 20.18 & 22.63 & 10.37 & 10.20 & 17.99 & \\textbf{9.28}\\\\ Shear Modulus Gv & 362 & GPa & 20.11 & 17.74 & 9.85 & 10.65 & 17.53 & \\textbf{6.00}\\\\ $\\epsilon_x$ & 576 & No unit & 14.61 & 12.34 & 12.68 & 10.10 & 21.77 & \\textbf{9.47}\\\\ $\\epsilon_y$ & 576 & No unit & 14.49 & 11.31 & 12.09 & 11.49 & 21.39 & \\textbf{9.39}\\\\ $\\epsilon_z$ & 576 & No unit & 14.56 & 12.70 & 12.40 & 10.65 & 22.69 & \\textbf{9.27}\\\\ \\midrule \\#Param. & - & - & 74.43K & 0.103M & 4.027M & 6.303M &-&-\\\\ \\botrule \\end{tabular*} \\end{table} To evaluate the impact of the GNN module within the SA-GAT-SR framework on the final results, we conducted another ablation study, with the findings summarized in Figure \\ref{Fig2}. This experiment primarily investigates the influence of the number of selected features on the SR module's derivation time and the prediction MAE for formation energy."}
{"input": "baseline, reflecting standard performance. The model with the CGCNN embedding algorithm utilizes one-hot encoding followed by the FCN to generate feature vectors. }\\label{Fig3} \\end{figure} Compared to DL based methods, the interpretability of our SA-GAT-SR model primarily lies in the final derived mathematical expressions. The expression result of the SA-GAT-SR model is composed of material-specific combination feature descriptors (e.g., $\\frac{B_{en}}{A_{en}}$) and a neural network-derived descriptor ($E_{P}$). Notably, the relationship between $E_{P}$ and the initial material features is nonlinear. Changes in the initial features not only affect the final output of the expression, but also alter the value of the coupled neural network descriptor $E_{P}$. Let the feature set comprising the initially selected physical quantities excluding $E_{P}$ be denoted as $M_0$. If we define the mapping relationship between a material's structure and its properties as $\\mathcal{F}(\\cdot)$, the SA-GAT-SR model result can be represented as follows: \\begin{equation} {\\hat Y} = \\mathcal{F}(E_{P}(M_0),M_0) \\label{eq11} \\end{equation} where $E_{P}(\\cdot)$ represents the function describing the relationship between the GNN module's predictions and the initial features. The above expression can be directly applied for material's feature screening tasks. However, when using the model for guiding new material development, it is crucial to understand how the predicted results are influenced by each feature. The predicted value from the GNN model cannot be directly used as a guiding descriptor because it lacks an explicit mathematical relationship with the feature set $M_0$. In most experimental outcomes, the coefficient of $E_{P}$ is typically close to 1."}
{"input": "complexity of 1 and 2 are provided in Supplementary Table 6 and 7. The work by Wang et al. also derived an expression predicting the bandgap property of photovoltaic perovskites based on SISSO.\\cite{bib1} In contrast to equation. \\ref{eq12}, they use the DFT calculation results based on Perdew-Burke-Ernzerhof (PBE) functional as a feature similar to the $E_p$ in equation. \\ref{eq12}, which can be expressed as \\begin{equation} {\\hat Y'_{bandgap}} = 1.28{E_{PBE}} + 9.05\\frac{{{l_X}}}{{{Q_B}}} - (0.77{Q_X} + 1.28){l_B} + 2.50 \\label{eq13} \\end{equation} where $E_{PBE}$ denotes the PBE bandgap energy. And $l_X$ and $l_B$ represent the energy levels of lowest-unoccupied molecular orbitals (LUMO) of the atoms in B site and X site, respectively. Given that the PBE functional typically underestimates bandgaps by around 50$\\%$, the coefficient of $E_{PBE}$ in equation~\\ref{eq13} is accordingly much greater than 1. This equation leverages a less accurate DFT result to learn a correction term that more closely approximates the true value. In comparison, our formulation uses a GNN-predicted value—offering higher accuracy—as the initial approximation and entirely removes the dependency on per-material first-principles calculations, making it highly suitable for fast, scalable property predictions over large materials databases. Many prior DL based approaches have attempted to develop fully universal models capable of accurately predicting any property using ultra-large-scale datasets. GNN models are often highly sensitive to the training dataset, making it challenging to improve robustness without careful tuning and data selection."}
{"input": "\\section*{Methods} \\subsection*{Dataset} In this study, we employ the dataset of $\\mathrm{ABO}_3$ and $\\mathrm{ABX}_3$ (X=F, Cl, Br, I) type perovskite materials from the JARVIS-DFT (Joint Automated Repository for Various Integrated Simulations)\\cite{bib35} database for training and evaluating our model. The JARVIS-DFT database is a widely recognized resource that provides high-quality, density functional theory (DFT) calculated properties for a diverse range of materials. For our analysis, we selected materials that conform to the perovskite crystal structure as the dataset. Due to the presence of lattice distortions, the crystal symmetry of these materials includes not only cubic but also tetragonal, hexagonal, monoclinic, and orthorhombic phases. In total, we curated a subset of 689 perovskite materials for further study. It is worth noting that some materials in the dataset may be missing certain properties due to incomplete data. Given that the JARVIS-DFT database is regularly updated, we chose the specific version of the dataset from 2021.8.18, as it is the most commonly used version in recent studies and ensures consistency with previous research. \\subsection*{Self-adaptable Graph attention networks (SA-GAT)} Our SA-GAT-SR model is a variant architecture based on GAT\\cite{bib28} for the specificity of material data, especially the crystal materials. In the following, we define the crystal graph as an undirected graph with $N$ nodes, denoted by $\\mathcal{G} = (V, E, g)$, where $V$ represents the node set, $E$ represents the edge set, and $g$ denotes the global node vector containing initial global features, which provides the unique representation of the graph."}
{"input": "is represented as $v_i$, and the feature vector of $v_i$ at layer $l$ is denoted by $h_i^l$. The corresponding edge feature in $E$, which connects nodes $v_i$ and $v_j$, is represented by $e_{ij}^l$. We consider the 12 nodes within a distance of 12 $\\text{Å}$ from $v_i$ as the neighboring nodes of $v_i$ and utilize a radial basis function (RBF) to compute the edge feature vectors. Additionally, we define a ranking index matrix $\\mathcal{R}_g \\in \\mathbb{Z}^{N \\times 12}$, which stores the distance ranking of each neighboring node relative to $v_i$. Each row vector of $\\mathcal{R}_g$ represents the ranking index of the neighboring nodes of $v_i$. To start, we convert the material structure into the graph representation using the self-adaptable encoding (SAE) method. Meanwhile, SAE produces the ICs of each feature for the following screening step. Then, the node features of graph are fed into the following message passing layers and iteratively updated. The normal nodes and global node representing atoms and crystal respectively are updated by different algorithms. After passing through all updating layers, we obtain the final global node feature $g^L$, where $L$ represents the total number of layers in the GNN module. The global node has learned the characteristics of the entire material and becomes a unique representation of this material."}
{"input": "depends on the nature of the prediction task, such as classification or regression. The $\\text{FFN}(\\cdot)$ function includes activation functions and regularization operations, tailored to optimize the model performance according to the task requirements. \\subsection*{Atomic nodes and global node updating method} In the GNN module, we incorporate the distance factors of different node pairs by leveraging the ranking matrix. We adopt the encoder architecture of the Transformer\\cite{bib30,bib12,bib13,bib22} to update graph node features and employ a novel set2set\\cite{bib17} based algorithm to update the global node feature. As shown in Figure \\ref{Fig1}b, the orange global node acts as $q^*$ vector in the set2set model to represent the state of the node set. The network is composed of multiple serially stacked blocks represented by green rectangle, each comprising a message passing layer and a set2set layer, which update the atomic and global nodes, respectively. At the start of each layer, the ranking matrix $\\mathcal{R}_g$ is embedded into a distance coefficient matrix $\\mathcal{C}_g \\in \\mathbb{R}^{N \\times 12}$, represented by $\\mathcal{C}_g = \\text{Emb}_r(\\mathcal{R}_g)$. The message passing update algorithm is formulated as follows: \\begin{equation} h_i^{l + 1} = h_i^l + \\sum\\limits_{k \\in {N_i}} {{a_{ik}}W_{val}^lz_{ik}^l} \\label{eq2} \\end{equation} \\begin{equation} e_{ij}^{l + 1} = e_{ij}^l + {a_{ij}}W_{edge}^l(h_i^l \\oplus h_j^l \\oplus e_{ij}^l) \\label{eq3} \\end{equation} \\begin{equation} {a_{ik}} = Softmax (W_{qry}^lh_i^l\\cdot W_{key}^lz_{ik}^l + {c_{ik}}) \\label{eq4} \\end{equation} \\begin{equation} z_{ik}^l = h_k^l \\oplus e_{ik}^l \\label{eq5} \\end{equation} where $c_{ij} \\in \\mathcal{C}_g$ is a scaling factor representing the distance coefficient between $v_i$ and $v_j$, and $\\oplus$ denotes the vector concatenation operation."}
{"input": "in graph $\\mathcal{G}$, while $W_{qry}, W_{key}, W_{val}, W_{edge}$ are learnable embedding matrices. Equations \\ref{eq2} and \\ref{eq3} represent the updates of the node feature $h_i$ and edge feature $e_{ij}$ connecting $v_i$ and $v_j$ in layer $l+1$. The core of the GNN module comprises multiple stacked message-passing layers that iteratively update the node and edge features. The readout operation is performed using the global node feature, which is updated at each layer. Inspired by previous work\\cite{bib31,bib27}, the global node, sometimes referred to as [VNode] in other models, is an artificially introduced node. Unlike other approaches, our model's global node, which represents the entire graph is initialized from actual crystal properties via the SAE module. Thus, it acts as a representation of the graph for the readout operation but does not participate in node updates within the graph. To enhance information retention across layers, we employ an inter-layer set2set method with a GRU\\cite{bib41} updating unit."}
{"input": "these queries accurately. We begin by independently embedding the raw series of each variable in IMTS using a Temporal Encoder, which generates a representation vector to capture temporal dependencies. To achieve this, we introduce an {\\ul A}daptive {\\ul Linear} (ALinear) network that dynamically adjusts weights based on observed time points, enabling the Temporal Encoder to handle intra-series inconsistencies in historical data effectively. Subsequently, a Spatial Encoder, composed of stacked Transformer \\cite{Transformer} blocks, is applied to these variable embeddings to capture variable correlations. Modeling correlations at the latent representation space efficiently mitigates the challenge of inter-series asynchrony. Finally, the comprehensive latent representations for each variable are independently mapped to the forecasting series via the Predictor, which also employs ALinear to manage intra-series inconsistencies in the forecasting queries. In contrast to iTransformer \\cite{iTransformer}, the SOTA method in RMTS, our approach just replaces the standard linear networks in both the Temporal Encoder and Predictor with ALinear. Accordingly, this new method is named AiT ({\\ul A}daptive {\\ul iT}ransformer). The contributions of our paper are summarised as follows: \\begin{itemize} \\item We present ALinear, which adaptively adjusts weights based on observation time points and effectively addresses intra-series inconsistency in IMTS. Experimental results demonstrate that it successfully bridges the gap between regular and irregular time series. \\item We introduce a novel IMTS forecasting method AiT, which addresses inherent challenges of inconsistency and asynchrony while exhibiting superiority in modeling temporal dependencies and variable correlations. \\item We evaluate AiT through extensive experiments on four publicly available IMTS datasets."}
{"input": "\\section{Related Work} \\label{section:Related_Work} \\subsection{Regular Multivariate Time Series Forecasting} \\label{subsection:Regular_Multivariate_Time_Series_Forecasting} As a significant real-world challenge, time series forecasting has garnered considerable attention. Initially, ARIMA \\cite{ARIMA} establishes an autoregressive model and performs forecasts in a moving average manner. However, the inherent complexity of the real world often renders it challenging to adapt. With the development of deep learning techniques, neural network-based methods have become increasingly important. RNNs \\cite{DeepAR} dynamically capture temporal dependencies by modeling semantic information within a sequential structure. Unfortunately, this architecture suffers from gradient vanishing/exploding and information forgetting when dealing with long sequences. To further improve prediction performance, CNNs \\cite{TimesNet} and self-attention mechanisms \\cite{PatchTST, Crossformer} have been introduced to capture long-range dependencies. Moreover, GNNs \\cite{StemGNN, CrossGNN, GraphWaveNet, MTGNN, FourierGNN} have also been further integrated into multivariate time series analysis due to their outstanding capability to model complex variable correlations. Linear networks \\cite{DLinear} have recently demonstrated remarkable capacity in capturing temporal dependencies. In contrast to more complex models, linear networks exhibit lower computational overhead while achieving comparable and occasionally superior prediction accuracy. Building upon the use of linear networks for capturing temporal dependencies, iTransformer \\cite{iTransformer} supplements the Transformer architecture to model variable correlations. By embedding series from distinct variables into individual tokens for the subsequent attention mechanism, iTransformer strikes an optimal balance between efficiency and accuracy, establishing itself as the SOTA approach for RMTS forecasting."}
{"input": "forecasting. GRU-D \\cite{GRU_D}, a model based on gated recurrent units, employs time decay and missing data imputation strategies to address irregularly sampled time series. mTAND \\cite{mTAND} is an IMTS imputation model that can be easily applied to forecasting tasks by only replacing the queries for imputation with forecasting. It learns embeddings for numerical values corresponding to continuous time steps and generates fixed-length representations for variable-length sequential data using an attention mechanism. Recently, several prospective studies have focused on forecasting tasks in IMTS. Specifically, these studies \\cite{NeuralFlows, GRU_ODE_Bayes, LatentODE, CRU} mainly utilize neural ODEs and concentrate on addressing continuous dynamics and inconsistency. However, the computation of ODE solvers is widely acknowledged as inefficient due to the substantial cost associated with numerical integration. Furthermore, although these studies have made significant progress in addressing inconsistencies within irregular time series, the effective modeling of variable correlations within asynchronous IMTS \\cite{GraFITi} remains an underexplored domain. Recently, T-PatchGNN \\cite{tPatchGNN} proposed a transformable patch method for converting univariate irregular time series into temporally aligned patches. These patches are then fed into the Transformer module and the temporal adaptive GNN to capture temporal dependencies and dynamic variable correlations. The time-aligned patches position T-PatchGNN as a SOTA approach for IMTS forecasting tasks, optimizing both efficiency and accuracy."}
{"input": "\\section{Methodology} \\label{section:Methodology} \\subsection{Problem Definition} \\label{subsection:Problem_Definition} An IMTS with $N$ variables can be represented as $\\mathcal{O}=\\left\\{\\mathbf{o}_{1:L_n}^n\\right\\}_{n=1}^N=\\left\\{\\left[\\left(t_i^n, x_i^n\\right)\\right]_{i=1}^{L_n}\\right\\}_{n=1}^N$, where the $n$-th variable contains $L_n$ observations. The $i$-th observation of the $n$-th variable is composed of the recorded time point $t_i^n$ and the corresponding value $x_i^n$. The time point $t_i^n$ are arranged chronologically, with the intervals between neighboring time points being variable. Due to the irregular sampling intervals and the occurrence of missing values, different samples or variables may exhibit varying numbers of observations $L_n$ within the same time span. Furthermore, even the same observation position $i$ may correspond to different actual time point $t_i^n$. Given historical IMTS observations $\\mathcal{O}=\\left\\{\\left[\\left(t_i^n, x_i^n\\right)\\right]_{i=1}^{L_n}\\right\\}_{n=1}^N$ and a set of forecasting queries $\\mathcal{Q}=\\left\\{\\left[q_j^n\\right]_{j=1}^{Q_n}\\right\\}_{n=1}^N$, where the $n$-th variable includes $Q_n$ queries, each query $q_j^n$ represents the $j$-th request to predict the value of the $n$-th variable at a future time step. The objective of IMTS forecasting is to predict the target values $\\hat{\\mathcal{X}} = \\left\\{\\left[\\hat{x}_j^n\\right]_{j=1}^{Q_n}\\right\\}_{n=1}^N$ corresponding to these queries. This forecasting task can be formulated as: \\begin{equation} \\begin{gathered} \\begin{aligned} \\mathcal{F} \\left(\\mathcal{O}, \\mathcal{Q}\\right) \\longrightarrow \\hat{\\mathcal{X}} \\end{aligned} \\end{gathered} \\end{equation} where $\\mathcal{F} \\left( \\cdot \\right)$ denotes the forecasting model to be learned. \\subsection{Framework Overview} \\label{subsection:Framework_Overview} \\begin{figure} \\centering \\resizebox{\\linewidth}{!} { \\includegraphics{./img/architecture.pdf} } \\caption{The overall framework of AiT. Raw observation series from different variables are independently embedded into dynamic variable embeddings via the Temporal Encoder. The Projection then integrates dynamic and static variable embeddings. The aggregated variable embedding is subsequently fed into the Spatial Encoder to capture variable correlations."}
{"input": "observations and their corresponding time points within the same period vary, resulting in shape mismatches and numerical misalignments for the static weight matrix. To address this issue, we propose an adaptive linear network to bridge the gap between regular and irregular time series. With an integrated attention mechanism \\cite{Transformer}, ALinear dynamically adjusts the weight assignments based on actual time points in historical observations and forecasting queries, ensuring consistent representations for observation series obtained from different locations within the same series. Given the input data $\\mathbf{x} \\in \\mathbb{R}^{L_{in}}$ and its associated time point $\\mathbf{s} \\in \\mathbb{R}^{L_{in}}$, the objective of ALinear is to produce the value $\\mathbf{y} \\in \\mathbb{R}^{L_{out}}$ corresponding to the output time point $\\mathbf{t} \\in \\mathbb{R}^{L_{out}}$. In IMTS, the input length $L_{in}$ and the output length $L_{out}$ typically vary across samples or variables. ALinear first extracts high dimensional features from the input time point $\\mathbf{s}$ and output time point $\\mathbf{t}$ using the Key Embedder and Query Embedder, respectively, converting them into keys $\\mathbf{K} \\in \\mathbb{R}^{L_{in} \\times D}$ and queries $\\mathbf{Q} \\in \\mathbb{R}^{L_{out} \\times D}$: \\begin{equation} \\begin{gathered} \\begin{aligned} \\mathbf{K} &= \\operatorname{KeyEmbedder} \\left( \\mathbf{s} \\right) \\\\ \\mathbf{Q} &= \\operatorname{QueryEmbedder} \\left( \\mathbf{t} \\right) \\\\ \\end{aligned} \\end{gathered} \\end{equation} To maintain architectural simplicity, both the Key Embedder and Query Embedder are constructed using an MLP comprising two layers of linear transformations and an activation function of ReLU \\cite{ReLU}. Subsequently, ALinear derives the dynamic weight matrix $\\mathbf{W} \\in \\mathbb{R}^{L_{out} \\times L_{in}}$ by calculating the dot product attention between $\\mathbf{Q}$ and $\\mathbf{K}$."}
{"input": "output expectation across varying input lengths $L_{in}$, we implement the function Softmax \\cite{Softmax} to normalize the weights. The final weight $\\mathbf{W}$ which can adaptively adjust its shape and value in accordance with time points, is employed to weight the input $\\mathbf{x}$ for summation. The entire process can be expressed as: \\begin{equation} \\begin{gathered} \\begin{aligned} \\mathbf{W} &= \\mathbf{Q}\\mathbf{K}^\\text{T} \\\\ \\mathbf{W} &= \\operatorname{Softmax} \\left( \\mathbf{W} \\right) \\\\ \\mathbf{y} &= \\mathbf{W}\\mathbf{x} \\\\ \\end{aligned} \\end{gathered} \\end{equation} Additionally, when the input time point $\\mathbf{s}$ (output time point $\\mathbf{t}$) is absent or remains constant, ALinear will utilize the built-in learnable matrix $\\mathbf{K}_\\text{default} \\in \\mathbb{R}^{L_{in} \\times D}$ ($\\mathbf{Q}_\\text{default} \\in \\mathbb{R}^{L_{out} \\times D}$) to substitute for the output of the Key Embedder (Query Embedder), thereby ensuring compatibility with the functionality of the standard linear networks. The comprehensive flow of ALinear is summarized in Algorithm \\ref{alg1}. A detailed comparative analysis of Adaptive Linear and standard Linear is provided in Appendix \\ref{section:Detailed_Analysis_of_ALinear}. \\subsection{Temporal Encoder} \\label{subsection:Temporal_Encoder} In this section, we employ a consistent processing procedure for all univariate time series and describe it in detail using the $n$-th variable as a case study. As illustrated in Figure \\ref{fig2}(a), the Temporal Encoder is tasked with modeling the temporal dependencies within the $n$-th variable observation series $\\mathbf{x}^n$ and converting it into a fixed-length dynamic variable representation $\\mathbf{h}^n_\\text{dyna}$. Given the challenges posed by variable sampling intervals and missing values in IMTS, we select ALinear to implement the Temporal Encoder."}
{"input": "built-in learnable matrix $\\mathbf{Q}_\\text{default}$ and Key Embedder to facilitate this process, mathematically represented as: \\begin{equation} \\begin{gathered} \\begin{aligned} \\mathbf{h}^n_\\text{dyna} = \\operatorname{ALinear}\\left(\\mathbf{x}=\\mathbf{x}^n, \\mathbf{s}=\\mathbf{t}^n, \\mathbf{t}=\\varnothing \\right) \\end{aligned} \\end{gathered} \\end{equation} Furthermore, in instances where no observations are available, we introduce an additional learnable static variable representation for the $n$-th variable, denoted as $\\mathbf{h}^n_\\text{stat}$, which is integrated with the dynamic features $\\mathbf{h}^n_\\text{dyna}$ generated from ALinear previously to mitigate the potential information loss. Subsequently, this concatenated variable representation is processed through an MLP comprising two layers of linear transformations and an activation function of ReLU, which ultimately maps back to the original dimension $\\mathbf{h}^n$. The following equation can summarize this process: \\begin{equation} \\begin{gathered} \\begin{aligned} \\mathbf{h}^n = \\operatorname{MLP}\\left(\\mathbf{h}^n_\\text{dyna} || \\mathbf{h}^n_\\text{stat}\\right) \\end{aligned} \\end{gathered} \\label{equ5} \\end{equation} where the notation $||$ represents the concatenation operation between vectors. Through the aforementioned temporal embedding and feature fusion processes, raw observation series of varying lengths can be transformed into feature representations of uniform dimension. Consequently, each IMTS instance can be comprehensively represented by a shape-fixed embedding matrix $\\mathbf{H} \\in \\mathbb{R}^{N \\times D} = \\left\\{\\mathbf{h}^n\\right\\}_{n=1}^N$. \\subsection{Spatial Encoder} \\label{subsection:Spatial_Encoder} Although significant correlations typically exist between the series of different variables, observations within IMTS may be considerably misaligned due to irregular sampling or missing values \\cite{tPatchGNN}. This asynchrony can obscure or distort the actual relationships among variables, posing a substantial challenge in accurately capturing variable correlations."}
{"input": "layers. The entire process can be succinctly expressed by the following equation: \\begin{equation} \\begin{gathered} \\begin{aligned} \\mathbf{H}^{l+1} =\\operatorname{TransformerBlock}\\left(\\mathbf{H}^l\\right),\\ l=0, \\cdots, L-1 \\end{aligned} \\end{gathered} \\end{equation} where $\\mathbf{H}^l \\in \\mathbb{R}^{N \\times D}$ represents the intermediate representation vector output from the $l$-th layer of Transformer block. Each Transformer block comprises two principal components: a multi-head self-attention mechanism and a feed-forward network. The self-attention mechanism generates query, key, and value vectors by applying a linear transformation to the representation embeddings of each variable. The attention scores computed from the dot product between the query and the key signify the variable correlations \\cite{iTransformer}. Consequently, variables that exhibit strong correlations will be assigned more weighted for the subsequent representation interaction with values. The feed-forward networks employ dense nonlinear connections to extract deeper features from time series. Recent revisiting on linear predictors \\cite{DLinear} highlights that temporal features extracted by MLPs are supposed to be shared within distinct time series. The neurons of MLP are trained to portray the intrinsic properties of any time series. \\subsection{Predictor} \\label{subsection:Predictor} In this section, we employ a consistent processing procedure for all univariate time series and describe it in detail using the $n$-th variable as a case study. The generation of forecasting series is performed by the linear network within the Predictor, whose superiority has been validated by numerous prior studies \\cite{iTransformer}. As illustrated in Figure \\ref{fig2}(c), the Predictor independently projects the comprehensive latent embedding $\\mathbf{h}^n$ of the $n$-th variable back into the data space to generate the forecasting series $\\hat{\\mathbf{x}}^n$."}
{"input": "2.89 $\\pm$ 0.03 & 3.40 $\\pm$ 0.05 & 5.29 $\\pm$ 0.04 & 3.16 $\\pm$ 0.09 \\\\ MTGNN & 6.26 $\\pm$ 0.18 & 4.46 $\\pm$ 0.07 & 2.71 $\\pm$ 0.23 & 9.55 $\\pm$ 0.65 & 3.03 $\\pm$ 0.03 & 3.53 $\\pm$ 0.03 & 5.39 $\\pm$ 0.05 & 3.34 $\\pm$ 0.02 \\\\ StemGNN & 6.86 $\\pm$ 0.28 & 4.76 $\\pm$ 0.19 & 1.73 $\\pm$ 0.02 & 7.71 $\\pm$ 0.11 & 8.81 $\\pm$ 0.37 & 6.90 $\\pm$ 0.02 & 5.75 $\\pm$ 0.09 & 3.40 $\\pm$ 0.09 \\\\ CrossGNN & 7.22 $\\pm$ 0.36 & 4.96 $\\pm$ 0.12 & 2.95 $\\pm$ 0.16 & 10.8 $\\pm$ 0.21 & 3.03 $\\pm$ 0.10 & 3.48 $\\pm$ 0.08 & 5.66 $\\pm$ 0.04 & 3.53 $\\pm$ 0.05 \\\\ FourierGNN & 6.84 $\\pm$ 0.35 & 4.65 $\\pm$ 0.12 & 2.55 $\\pm$ 0.03 & 10.2 $\\pm$ 0.08 & 2.99 $\\pm$ 0.02 & 3.42 $\\pm$ 0.02 & 5.82 $\\pm$ 0.06 & 3.62 $\\pm$ 0.07 \\\\ \\midrule GRU-D & 5.59 $\\pm$ 0.09 & 4.08 $\\pm$ 0.05 & 1.76 $\\pm$ 0.03 & 7.53 $\\pm$ 0.09 & 2.94 $\\pm$ 0.05 & 3.53 $\\pm$ 0.06 & 5.54 $\\pm$ 0.38 & 3.40 $\\pm$ 0.28 \\\\ SeFT & 9.22 $\\pm$ 0.18 & 5.40 $\\pm$ 0.08 & 1.87 $\\pm$ 0.01 & 7.84 $\\pm$ 0.08 & 12.2 $\\pm$ 0.17 & 8.43 $\\pm$ 0.07 & 5.80 $\\pm$ 0.19 & 3.70 $\\pm$ 0.11 \\\\ RainDrop & 9.82 $\\pm$ 0.08 & 5.57 $\\pm$ 0.06 & 1.99 $\\pm$ 0.03 & 8.27 $\\pm$ 0.07 & 14.9 $\\pm$ 0.14 & 9.45 $\\pm$ 0.05 & 5.78 $\\pm$ 0."}
{"input": "0.04} \\\\ \\midrule Ours & \\textbf{4.58 $\\pm$ 0.06} & \\textbf{3.39 $\\pm$ 0.02} & \\textbf{1.27 $\\pm$ 0.02} & \\textbf{6.16 $\\pm$ 0.08} & \\textbf{2.37 $\\pm$ 0.02} & \\textbf{2.93 $\\pm$ 0.02} & \\textbf{4.43 $\\pm$ 0.03} & \\textbf{3.03 $\\pm$ 0.03} \\\\ \\midrule Impr. & \\multicolumn{2}{c|}{8.45 \\%} & \\multicolumn{2}{c|}{19.77 \\%} & \\multicolumn{2}{c|}{8.94 \\%} & \\multicolumn{2}{c}{6.64 \\%} \\\\ \\bottomrule \\end{tabular} } \\label{tab1} \\end{table} \\begin{table} \\centering \\caption{The average training time per epoch and total inference time of AiT and baselines for irregular multivariate time series forecasting. A lower value denotes greater efficiency. The optimal and suboptimal results are highlighted in bold and underlined, respectively.} \\resizebox{0.85\\linewidth}{!} { \\begin{tabular}{c|cc|cc|cc|cc} \\toprule & \\multicolumn{2}{c|}{PhysioNet} & \\multicolumn{2}{c|}{MIMIC} & \\multicolumn{2}{c|}{Activity} & \\multicolumn{2}{c}{USHCN} \\\\ \\cmidrule(l){2-9} Method & \\multicolumn{1}{l}{Training} & \\multicolumn{1}{l|}{Inference} & \\multicolumn{1}{l}{Training} & \\multicolumn{1}{l|}{Inference} & \\multicolumn{1}{l}{Training} & \\multicolumn{1}{l|}{Inference} & \\multicolumn{1}{l}{Training} & \\multicolumn{1}{l}{Inference} \\\\ & Time (s) & Time (s) & Time (s) & Time (s) & Time (s) & Time (s) & Time (s) & Time (s) \\\\ \\midrule GRU-D & 76.62 & 10.06 & 511.8 & 24.99 & 39.52 & 0.769 & 131.9 & 8.762 \\\\ SeFT & 106.7 & 7.406 & 378.6 & 25.85 & 22.70 & 1.510 & 84.15 & 9.770 \\\\ RainDrop & 86.71 & 6.641 & 286.3 & 27.41 & 19.56 & 1.396 & 51.99 & 8.706 \\\\ Warpformer & 12.88 & 8.997 & 80.93 & 27.68 & 5.117 & 1.323 & 19.65 & 5.826 \\\\ mTAND & 9.754 & 2.623 & 31.13 & 5.829 & 1.964 & 0.415 & 6.926 & 2.830 \\\\ Latent-ODE & 2796. & 55."}
{"input": "algorithms incorporating variable correlations (e.g., Crossformer and FourierGNN) generally outperform models that rely solely on single-variable information (e.g., TimesNet and PatchTST) in IMTS contexts. This difference may arise because, in RMTS, the excess information leads models to overfit and misgeneralize variable correlations from the training set to the test set \\cite{CI}. In contrast, in the IMTS setting, sparse observations alone are insufficient to provide a solid foundation for future predictions, making incorporating information from other relevant variables a valuable complementary approach. Among baselines for IMTS analysis, those focused on imputation and forecasting outperform algorithms centered on classification. This disparity arises primarily because different tasks necessitate distinct representations: the former demands fine-grained representations, while the latter emphasizes coarse-grained modeling \\cite{TimesNet}. Furthermore, baselines based on RNNs for capturing temporal dependencies (e.g., CRU and Latent-ODE) are less effective than T-PatchGNN, which leverages attention mechanisms for modeling temporal dependencies. This is due to the inherent limitations of RNNs, including vanishing/exploding gradients and information loss in long sequences \\cite{RMTS}. However, previous studies \\cite{iTransformer, DLinear} have shown that this temporal dependencies modeling approach, based on the patch+attention architecture, is inferior to linear networks in both prediction accuracy and computational efficiency. AiT effectively addresses the challenge of intra-series inconsistency in IMTS scenarios by introducing an adaptive linear network. Extensive empirical analysis confirms its superiority on modeling temporal dependencies and variable correlations. To further investigate the impact of ALinear on model efficiency, we compare the average training time per epoch and overall inference time."}
{"input": "0.03 & 5.25 $\\pm$ 0.03 & 3.24 $\\pm$ 0.02 & -34.5\\% \\\\ rp TsMLP & 4.63 $\\pm$ 0.09 & 3.43 $\\pm$ 0.06 & 1.29 $\\pm$ 0.04 & 6.27 $\\pm$ 0.12 & 2.47 $\\pm$ 0.03 & 3.02 $\\pm$ 0.04 & 4.56 $\\pm$ 0.06 & 3.12 $\\pm$ 0.03 & -2.29\\% \\\\ \\bottomrule \\end{tabular} } \\label{tab3} \\end{table} \\begin{figure} \\centering \\resizebox{0.99\\linewidth}{!} { \\includegraphics{./img/ablation.pdf} } \\caption{The prediction error, average training time per epoch, and total inference time of AiT and its variants for irregular multivariate time series forecasting. A lower value denotes a better performance.} \\label{fig3} \\end{figure} To assess the advantages of ALinear in modeling temporal dependencies, we replace the ALinear in the Temporal Encoder with the TTCN and TempTF, commonly utilized in previous IMTS forecasting frontier algorithms \\cite{ISTSPLM, tPatchGNN}. The results indicate that the performance of TTCN is comparable to that of ALinear. However, TTCN employs a masking mechanism within a high-dimensional hidden space to address the challenge of missing values in IMTS. This operation, which is more computationally intensive, led to an increase of 195.28\\% in training time and a 175.40\\% rise in inference time. TempTF achieves competitive prediction accuracy across most datasets, with the exception of USHCN, which features short horizon lengths and a limited number of variables. The sparse observations and diminished correlations among variables hinder the complex Transformer architecture from extracting sufficient information efficiently. Additionally, the square computational complexity resulting from the self-attention mechanism between time points substantially escalates the computational burden, increasing 366.09\\% and 259."}
{"input": "times, respectively. Sparse observations in IMTS are typically insufficient for accurate predictions, making integrating information from other relevant variables an effective complementary approach. Removing the Spatial Encoder results in a performance decline of 8.47\\%. Additionally, we observe that the Spatial Encoder does not impose a significant computational cost, as the attention mechanism operates across a limited number of variables \\cite{iTransformer}. The static variable representation mitigates the issue of insufficient information caused by high missing rates or even complete absence. Once this representation is removed, substantial performance degradation is observed across all datasets, with a particularly pronounced impact on datasets with high missing rates, such as PhysioNet and MIMIC. Finally, using TsMLP as an alternative led to the most minor performance reduction. However, MLP operations based on high-dimensional hidden vector concatenation introduce more computational overhead compared to ALinear, increasing training and inference times by 58.54\\% and 55.27\\%, respectively."}
{"input": "\\section{Detailed Analysis of ALinear} \\label{section:Detailed_Analysis_of_ALinear} \\begin{algorithm} \\caption{ALinear} \\begin{algorithmic}[1] \\STATE \\textbf{Parameter:} hidden dimension $D$, input length $L_\\text{in}$ (optional), output length $L_\\text{out}$ (optional) \\STATE \\textbf{Input:} input data $\\mathbf{x}$, input time point $\\mathbf{s}$ (optional), output time point $\\mathbf{t}$ (optional) \\STATE \\textbf{Output:} output data $\\mathbf{y}$ \\STATE \\STATE \\textbf{Initialization:} \\IF{input length $L_\\text{in}$ is provided} \\STATE Initialize $\\mathbf{K}_\\text{default} \\in \\mathbb{R}^{L_\\text{in} \\times D}$ \\hfill \\footnotesize \\textit{\\# fixed input time point} \\ENDIF \\IF{output length $L_\\text{out}$ is provided} \\STATE Initialize $\\mathbf{Q}_\\text{default} \\in \\mathbb{R}^{L_\\text{out} \\times D}$ \\hfill \\footnotesize \\textit{\\# fixed output time point} \\ENDIF \\STATE \\STATE \\textbf{Forward Pass:} \\IF{input time point $\\mathbf{s}$ is provided} \\STATE $\\mathbf{K} \\gets \\operatorname{KeyEmbedder}\\left( \\mathbf{s} \\right)$ \\hfill \\footnotesize \\textit{\\# variable input time point} \\ELSE \\STATE $\\mathbf{K} \\gets \\mathbf{K}_\\text{default}$ \\hfill \\footnotesize \\textit{\\# fixed input time point} \\ENDIF \\IF{output time point $\\mathbf{t}$ is provided} \\STATE $\\mathbf{Q} \\gets \\operatorname{QueryEmbedder}\\left( \\mathbf{t} \\right)$ \\hfill \\footnotesize \\textit{\\# variable output time point} \\ELSE \\STATE $\\mathbf{Q} \\gets \\mathbf{Q}_\\text{default}$ \\hfill \\footnotesize \\textit{\\# fixed output time point} \\ENDIF \\STATE \\STATE $\\mathbf{W} \\gets \\operatorname{Matmul}\\left( \\mathbf{Q}, \\mathbf{K}^\\text{T} \\right)$ \\STATE $\\mathbf{W} \\gets \\operatorname{Softmax}\\left( \\mathbf{W} \\right)$ \\STATE $\\mathbf{y} \\gets \\operatorname{Matmul}\\left( \\mathbf{W}, \\mathbf{x} \\right)$ \\STATE \\STATE \\textbf{Return} $\\mathbf{y}$ \\end{algorithmic} \\label{alg1} \\end{algorithm} The attention mechanism in ALinear can be considered as a specialized form of a fully connected linear network \\cite{DLinear}. While the attention mechanism \\cite{Transformer} weights and sums the Value through the product of Query and Key, a traditional fully connected linear network directly applies a weight matrix for weighting and summation."}
{"input": "are dynamically generated based on input. In the case of a standard Linear that weights and sums the inputs $\\mathbf{x}$ using fixed weights $\\mathbf{W}_\\text{static} \\in \\mathbb{R}^{L_{out} \\times L_{in}}$, the standard Linear and ALinear can be expressed as follows: \\begin{equation} \\begin{gathered} \\begin{aligned} \\mathbf{y}^{L_{out} \\times 1} &= \\left( \\mathbf{W}_\\text{static}^{L_{out} \\times L_{in}} \\right) \\times \\mathbf{x}^{L_{in} \\times 1} \\\\ &\\approx \\left( \\mathbf{Q}_\\text{default}^{L_{out} \\times D} \\mathbf{K}_\\text{default}^{D \\times L_{in}} \\right) \\times \\mathbf{x}^{L_{in} \\times 1} \\\\ &\\approx \\left( \\mathbf{t}^{L_{out} \\times 1} \\mathbf{E}_\\text{Q}^{1 \\times D} \\mathbf{E}_\\text{K}^{D \\times 1} \\mathbf{s}^{1 \\times L_{in}} \\right) \\times \\mathbf{x}^{L_{in} \\times 1} \\end{aligned} \\end{gathered} \\end{equation} where the Query Embedder and Key Embedder in ALinear are represented by two transformation matrices $\\mathbf{E}_\\text{Q} \\in \\mathbb{R}^{1 \\times D}$ and $\\mathbf{E}_\\text{K} \\in \\mathbb{R}^{1 \\times D}$, respectively, serving the same function. Additionally, the transpose operation of the matrices is omitted to maintain conciseness in the formulas. For the RMTS forecasting task, the lengths and values of the input and output time points are fixed, allowing the standard linear networks to perform a weighted sum over the input $\\mathbf{x}$ using the static weight matrix $\\mathbf{W}_\\text{static}$. In this context, ALinear approximates $\\mathbf{W}_\\text{static}$ by employing the product of two fixed-shape learnable static low-rank matrices, $\\mathbf{Q}_\\text{default}$ and $\\mathbf{K}_\\text{default}$. The experiments detailed in Appendix \\ref{subsection:Module_Generality} indicate that the performance of both approaches is nearly identical. However, when addressing IMTS, the static weight matrix becomes inapplicable due to the variability in the number and actual value of input and output time points."}
{"input": "shapes and values to approximate $\\mathbf{K}_\\text{default}$ ($\\mathbf{Q}_\\text{default}$) through the product of the input time point $\\mathbf{s}$ (output time point $\\mathbf{t}$) and the Key Embedder $\\mathbf{E}_\\text{K}$ (Query Embedder $\\mathbf{E}_\\text{Q}$). This methodology is tantamount to approximating a high-rank matrix with the product of two low-rank matrices of rank 1, which may compromise the fitting capability of the model. Consequently, ALinear favors the utilization of the built-in learnable matrix $\\mathbf{K}_\\text{default}$ ($\\mathbf{Q}_\\text{default}$) instead of depending on a learnable time point vector when the input time point $\\mathbf{s}$ (output time point $\\mathbf{t}$) is absent or remains constant. Furthermore, the input time point $\\mathbf{s}$, Key Embedder $\\mathbf{E}_\\text{K}$, and the learnable matrix $\\mathbf{Q}_\\text{default}$ (as well as the output time point $\\mathbf{t}$, Query Embedder $\\mathbf{E}_\\text{Q}$, and the learnable matrix $\\mathbf{K}_\\text{default}$) can be mixed according to the specific scenario."}
{"input": "cross-variable GNN. We use the official implementation at \\href{https://github.com/hqh0728/CrossGNN}{https://github.com/hqh0728/CrossGNN}. \\item \\textbf{FourierGNN} \\cite{FourierGNN} initially establishes a hypervariate graph and transforms features into the Fourier space. It then stacks Fourier graph operators in the Fourier domain to capture temporal-spatial dependencies. We use the official implementation at \\href{https://github.com/aikunyi/FourierGNN}{https://github.com/aikunyi/FourierGNN}. \\item \\textbf{GRU-D} \\cite{GRU_D} is a model based on gated recurrent units, employs time decay and missing data imputation strategies to address irregularly sampled time series. We use the official implementation at \\href{https://github.com/zhiyongc/GRU-D}{https://github.com/zhiyongc/GRU-D}. \\item \\textbf{SeFT} \\cite{SeFT} transforms time series into a set of embeddings and then uses a set of functions to model them, offering excellent parallelism and efficient memory usage. We use the open source implementation at \\href{https://github.com/mims-harvard/Raindrop}{https://github.com/mims-harvard/Raindrop}. \\item \\textbf{RainDrop} \\cite{RainDrop} represents each sample as an independent sensor graph, capturing temporal dependencies between sensors through innovative message-passing operators. It infers the underlying sensor graph structure and uses it alongside proximate observations to forecast unaligned readings. We use the official implementation at \\href{https://github.com/mims-harvard/Raindrop}{https://github.com/mims-harvard/Raindrop}. \\item \\textbf{Warpformer} \\cite{Warpformer} is a Transformer-based model, adopts a tailored input representation that explicitly encapsulates both intra-series inconsistency and inter-series asynchrony. It produces multi-scale representations that balance coarse-grained and fine-grained signals for subsequent tasks. We use the official implementation at \\href{https://github.com/imJiawen/Warpformer}{https://github.com/imJiawen/Warpformer}. \\item \\textbf{mTAND} \\cite{mTAND} is an IMTS imputation model that can be easily applied to forecasting tasks by only replacing the queries for imputation with forecasting. It learns embeddings for numerical values corresponding to continuous time steps and generates fixed-length representations for variable-length sequential data using an attention mechanism."}
{"input": "adjust the specific learning rate for each epoch, with a fixed adjustment period of 40 epochs. Optimal hyperparameters for AiT are determined via grid search. The hidden dimension of each module is fixed at 64 (selected from 16, 32, 64, and 128). The numbers of attention heads and Transformer blocks in the Spatial Encoder are set to 4 (selected from 1, 2, 4, and 8) and 3 (selected from 1, 2, 3, and 4), respectively. Appendix \\ref{subsection:Hyperparameter_Analysis} provides a detailed hyperparameter sensitivity analysis. It is important to note that, in contrast to T-PatchGNN \\cite{tPatchGNN}, which optimizes parameters individually for each dataset, we employ a consistent hyperparameter configuration across all datasets to evaluate generalizability and mitigate deployment complexity. With dataset-specific tuning, the performance of AiT can be further enhanced. To facilitate open science and community collaboration, we will make all data, source code, and pre-trained weight checkpoints publicly accessible following the release. These resources will offer substantial support for subsequent research. All experiments are implemented using Python 3.10.13 and PyTorch 2.1.2, and executed on an Ubuntu server equipped with an AMD Ryzen 9 7950X 16-core processor and a single NVIDIA GeForce RTX 4090 GPU. To mitigate randomness, we perform each experiment using five different random seeds and present the mean and standard deviation of results. \\subsection{Ablation Study} \\label{subsection:Ablation_Study_Detailed} To validate the effectiveness and reasonableness of each component, we evaluate the performance of AiT and its variants across all datasets."}
{"input": "4.64 $\\pm$ 0.12 \\\\ Crossformer & 9.48 $\\pm$ 0.11 & 5.86 $\\pm$ 0.16 & 8.57 $\\pm$ 0.16 & 5.70 $\\pm$ 0.13 & 5.70 $\\pm$ 0.14 & 4.47 $\\pm$ 0.12 & 5.33 $\\pm$ 0.06 & 4.44 $\\pm$ 0.09 \\\\ Graph Wavenet & 9.43 $\\pm$ 0.29 & 5.86 $\\pm$ 0.13 & 7.23 $\\pm$ 0.25 & 4.82 $\\pm$ 0.12 & 4.71 $\\pm$ 0.22 & 3.90 $\\pm$ 0.10 & 4.10 $\\pm$ 0.23 & 3.73 $\\pm$ 0.10 \\\\ MTGNN & 9.83 $\\pm$ 0.27 & 5.95 $\\pm$ 0.11 & 7.48 $\\pm$ 0.21 & 5.01 $\\pm$ 0.08 & 5.08 $\\pm$ 0.17 & 3.99 $\\pm$ 0.08 & 5.22 $\\pm$ 0.13 & 4.19 $\\pm$ 0.06 \\\\ StemGNN & 8.70 $\\pm$ 0.27 & 5.37 $\\pm$ 0.21 & 7.46 $\\pm$ 0.25 & 4.84 $\\pm$ 0.21 & 6.65 $\\pm$ 0.25 & 4.69 $\\pm$ 0.19 & 5.47 $\\pm$ 0.21 & 4.56 $\\pm$ 0.16 \\\\ CrossGNN & 10.4 $\\pm$ 0.34 & 6.56 $\\pm$ 0.17 & 7.97 $\\pm$ 0.35 & 5.37 $\\pm$ 0.14 & 6.87 $\\pm$ 0.29 & 4.73 $\\pm$ 0.12 & 4.80 $\\pm$ 0.29 & 4.24 $\\pm$ 0.10 \\\\ FourierGNN & 9.59 $\\pm$ 0.36 & 5.61 $\\pm$ 0.14 & 7.95 $\\pm$ 0.39 & 4.99 $\\pm$ 0.14 & 6.35 $\\pm$ 0.33 & 4.61 $\\pm$ 0.11 & 5.37 $\\pm$ 0.29 & 4.34 $\\pm$ 0.10 \\\\ \\midrule GRU-D & 8.18 $\\pm$ 0.13 & 4.99 $\\pm$ 0.09 & 6.89 $\\pm$ 0.08 & 4.55 $\\pm$ 0.06 & 4.42 $\\pm$ 0.08 & 3.66 $\\pm$ 0.05 & 4.44 $\\pm$ 0.07 & 3.79 $\\pm$ 0.05 \\\\ SeFT & 9.78 $\\pm$ 0."}
{"input": "5.55 $\\pm$ 0.12 & 9.30 $\\pm$ 0.19 & 5.41 $\\pm$ 0.11 & 9.15 $\\pm$ 0.15 & 5.15 $\\pm$ 0.08 & 8.76 $\\pm$ 0.16 & 5.57 $\\pm$ 0.07 \\\\ RainDrop & 10.5 $\\pm$ 0.04 & 5.72 $\\pm$ 0.11 & 9.89 $\\pm$ 0.11 & 5.62 $\\pm$ 0.07 & 9.70 $\\pm$ 0.07 & 5.40 $\\pm$ 0.06 & 9.28 $\\pm$ 0.08 & 5.62 $\\pm$ 0.06 \\\\ Warpformer & 8.48 $\\pm$ 0.43 & 5.13 $\\pm$ 0.16 & 7.57 $\\pm$ 0.38 & 4.83 $\\pm$ 0.14 & 5.60 $\\pm$ 0.33 & 4.09 $\\pm$ 0.12 & 6.44 $\\pm$ 0.29 & 4.67 $\\pm$ 0.10 \\\\ mTAND & 8.45 $\\pm$ 0.31 & 5.23 $\\pm$ 0.21 & 7.11 $\\pm$ 0.29 & 4.67 $\\pm$ 0.21 & 5.71 $\\pm$ 0.20 & 4.17 $\\pm$ 0.16 & 5.44 $\\pm$ 0.18 & 4.33 $\\pm$ 0.14 \\\\ Latent-ODE & 8.25 $\\pm$ 0.69 & 5.04 $\\pm$ 0.33 & 7.20 $\\pm$ 0.63 & 4.69 $\\pm$ 0.28 & 6.70 $\\pm$ 0.53 & 4.36 $\\pm$ 0.25 & 7.10 $\\pm$ 0.46 & 5.33 $\\pm$ 0.21 \\\\ CRU & 9.20 $\\pm$ 0.34 & 5.38 $\\pm$ 0.11 & 9.20 $\\pm$ 0.33 & 5.31 $\\pm$ 0.10 & 9.50 $\\pm$ 0.23 & 5.41 $\\pm$ 0.10 & 11.6 $\\pm$ 0.20 & 6.98 $\\pm$ 0.08 \\\\ Neural Flow & 8.30 $\\pm$ 0.10 & 4.99 $\\pm$ 0.06 & 8.50 $\\pm$ 0.07 & 5.27 $\\pm$ 0.05 & 7.70 $\\pm$ 0.06 & 4.68 $\\pm$ 0.05 & 7.40 $\\pm$ 0.04 & 5.10 $\\pm$ 0.03 \\\\ GraFITi & 8.21 $\\pm$ 0.32 & 5.14 $\\pm$ 0.22 & 6.95 $\\pm$ 0.26 & 4.45 $\\pm$ 0."}
{"input": "3.30 $\\pm$ 0.07 & 2.78 $\\pm$ 0.07 & \\\\ \\multicolumn{1}{c|}{} & 720 & 4.86 $\\pm$ 0.09 & 3.70 $\\pm$ 0.10 & \\textbf{4.86 $\\pm$ 0.10} & \\textbf{3.65 $\\pm$ 0.10} & 3.50 $\\pm$ 0.06 & 2.93 $\\pm$ 0.06 & \\textbf{3.39 $\\pm$ 0.10} & \\textbf{2.79 $\\pm$ 0.08} & \\\\ \\midrule \\multicolumn{2}{c|}{Impr.} & \\multicolumn{4}{c|}{-0.18 \\%} & \\multicolumn{4}{c|}{0.05 \\%} & -0.08 \\% \\\\ \\bottomrule \\end{tabular} } \\label{tab6} \\end{table} To validate the applicability of ALinear for RMTS forecasting, we specifically design a series of supplementary experiments. These experiments utilize Electricity and Traffic datasets \\cite{GLAFF}, which are extensively used for benchmarking in the RMTS domain and are publicly accessible. We follow the standard segmentation protocol \\cite{iTransformer}, strictly dividing each dataset into training, validation, and testing sets chronologically to ensure no information leakage issues. The segmentation ratio for each dataset is set to 6:2:2. Regarding forecasting settings, we also adhere to established mainstream protocols \\cite{DLinear}. Specifically, we set the length of the historical horizon to 96, while the forecasting length varies within \\{96, 192, 336, 720\\}. Unless otherwise indicated, the remaining experimental setups align with Section \\ref{subsection:Experimental_Setup}. \\begin{figure} \\centering \\resizebox{\\linewidth}{!} { \\includegraphics{./img/generalization.pdf} } \\caption{The average training time per epoch and total inference time of ALinear and backbones for regular multivariate time series forecasting. A lower value denotes greater efficiency.} \\label{fig5} \\end{figure} We select two mainstream methods as baselines for comparison: NLinear \\cite{DLinear} and iTransformer \\cite{iTransformer}."}
{"input": "\\section{Introduction} % Deep reinforcement learning (RL) has achieved significant results in many fields, such as robotics tasks in simulation~\\cite{mnih2015human,peng2017deeploco}, game playing~\\cite{silver2017mastering}, and large language models~\\cite{achiam2023gpt,touvron2023llama}, and so on. However, its broader application is limited by the challenges of interacting with real-world environments, which can be costly or risky~\\cite{garcia2015comprehensive}. Offline reinforcement learning addresses this by allowing agents to learn from fixed datasets collected by behavior policies~\\cite{zhangzhe}, thereby avoiding the high risk interactions~\\cite{lange2012batch}. % However, deploying online RL framework under offline setting would significantly hinder the performance of the learned policy. This is caused by the well-known {\\it distributional shift} problem~\\cite{BCQ,cql}, where the TD target would be overestimated on those out-of-distribution (OOD) actions during training, resulting in extrapolation error~\\cite{pessimism}, hence degrading the agent's performance. % Previous works, like Conservative Q-Learning (CQL)~\\cite{cql}, Bootstrapping Error Accumulation Reduction (BEAR)~\\cite{bear} and Supported Policy Optimization (SPOT)~\\cite{SPOT}, address this issue by suppressing those OOD actions with specific regularization. However, these methods only focus on the current OOD decisions, while ignoring the issue of {\\it state distributional shift}~\\cite{osr,sdc} - the agent may enter those unseen states during test, accumulating errors and failing the task. % Currently, to deal with the {\\it state distributional shift} problem, methods, such as State Deviation Correction (SDC)~\\cite{sdc} and Out-of-sample State Recovery (OSR)~\\cite{osr}, enable the agent to move back to its familiar once it entering those OOD states, by aligning the transition distribution between the new policy and behavior policy."}
{"input": "or low-density states during test, leading to cumulative errors and task failure, i.e., the phenomenon of State deviation. %To tackle this problem, methods like State Deviation Correction (SDC)\\cite{sdc} and Out-of-sample State Recovery (OSR)\\cite{osr} enable the agent to return to familiar states after entering OOD regions by aligning the transition distribution between the new policy and the behavior policy. In this way, the new policy would prefer to actions that lead to those in-distribution outcomes once entering OOD states, avoiding the accumulation of state deviation, hence enhancing the safety and robustness of the offline agent. Besides, State Correction and OOD Action Suppression (SCAS)~\\cite{scas} has been proposed to construct a value-sensitive transition distribution to guide the new policy, which improves the agent's performance when learning the behaviors of OOD state correction from non-expert datasets. \\begin{figure}[t] \\centering \\setlength{\\abovecaptionskip}{0.cm} % \\setlength{\\belowcaptionskip}{-0.cm} \\includegraphics[width=0.85\\linewidth]{framework.pdf} \\caption{The basic idea behind the proposed DASP-based OOD state correction - guiding the agent from OOD states (low-density) to the high density regions according to the dataset.} \\label{fig:framework} \\end{figure} % Nevertheless, it is important to note that all these OOD state correction methods require the accurate estimation for specific transition distributions as the demonstration for new policy, which could increase the risk of introducing additional model bias, particularly in high-dimensional and complicated environments. % Recall the definition of OOD states - the states with low visitation frequency by the behavior policy, where, in another word, OOD states have lower density than those in-distribution states according to the offline dataset."}
{"input": "OOD state correction could also be seen as a procedure that guiding the agent to move from low density states to high density states, guaranteeing the decision-making supported by sufficient data, hence keeping safe. Such density-based safety requirement is common in online control~\\cite{kang2022lyapunov}, but to the best of our knowledge, it has not been utilized for OOD state correction in offline RL yet. %Nevertheless, it is crucial to recognize that all these OOD state correction methods rely on accurately estimating specific transition distributions as demonstrations for the new policy. This reliance can heighten the risk of introducing additional model bias, particularly in high-dimensional and complex environments. By OOD states, we mean these states that experience low visitation frequency by the behavior policy. In other words, OOD states exhibit lower density compared to in-distribution states based on the offline dataset. From this perspective, as is shown in Figure \\ref{fig:framework}, OOD state correction can be viewed as a process that guides the agent to transition from low-density states to high-density states, ensuring that decision-making is supported by sufficient data and thereby maintaining safety. Such density-based safety requirement is common in online control~\\cite{kang2022lyapunov}, but to the best of our knowledge, it has yet to be applied to OOD state correction in offline RL. % Therefore, we aim to design a novel method, called Density-Aware Safety Perception (DASP), to realize OOD state correction via a single signal, where the difference between previous and our methods is illustrated in Figure \\ref{fig:framework} (middle)."}
{"input": "algorithmic design, requiring only minor modifications to standard off-policy algorithms to be effective. We experimentally show that the proposed method outperforms several closely related state-of-the-art (SOTA) methods in offline MuJoCo control suites across various settings. % Specifically, we develop a module that predicts the density of a compact feature extracted from the joint input of state and action, while aligning this feature with the consequences of the given state-action pair to embed sequential information. This approach allows the indicator to highlight regions with a high density of decision-making consequences, thereby guiding the agent to move toward these safer regions through policy constraints. % Considering all the issues above, in this paper, we propose a novel method to deal with the {\\it state distributional shift} problem. The basic idea is to design an density-based indicator to guide the agent recover from OOD regions to regions with high density of data, which is a common safety requirement in Cybernetics~\\cite{kang2022lyapunov}. To be specific, we design a module that predicts the density of a compact feature extracted from the joint input of state and action, while the feature is also aligned with the consequence of the given state-action pair to embedding the sequential information. In this way, it could indicate the regions with high density of the consequences of decision-making, hence guiding the agent move to such safe regions by policy constraint."}
{"input": "training. The global convergence properties of the proposed method are also theoretically demonstrated. In practical implementation, our method leverages a modular algorithmic design, requiring only minor modifications to standard off-policy algorithms to be effective. We experimentally demonstrate that the proposed method outperforms several closely related state-of-the-art methods in offline MuJoCo control suites under various settings. % Although offline reinforcement learning can learn policy from an offline dataset, as~\\cite{BCQ,cql} have shown, it tends to overestimate the value of out-of-distribution (OOD) actions during training, which would result in the problem of extrapolation error~\\cite{pessimism}, significantly reducing the agents' performance. % To address this, previous works propose either value conservative methods~\\cite{cql,an2021uncertainty} or policy constraint methods~\\cite{td3bc,bear,SPOT} to restrict the learned policy within the support set of behavior policy during training. % On the other hand, the problem of OOD states also affects the reliability of offline RL during testing phase. Consequence-driven offline RL methods, such as State Deviation Correction (SDC)~\\cite{sdc} and Out-of-sample Situation Recovery (OSR)~\\cite{osr}, have shown the effectiveness of considering the potential consequences of the new policy in the field of reliable offline RL~\\cite{safety}. State Correction and OOD Action Suppression (SCAS)~\\cite{scas} improved upon these approaches by proposing to constrain the consequences of new policy within the support set of the dataset, but their method relies on accurate estimation of the dynamic model, particularly susceptible to stochastic dynamics. Therefore, there is still significant room for improvement in existing methods. % In this paper, we aim to address OOD states problem in a better way."}
{"input": "propose a novel and pluggable method in this paper, called Density-based State-Supported Bootstrapping (DSSB). Our method corrects the agent from OOD states to high-value in-distribution (ID) states through a probability density indicator, without relying on accurate estimation of probability density function values, combines the advantages of both action-supported and consequence-driven methods. The benefit of this approach is that large rewards are given when the subsequent states by following the current policy are beneficial to improve the performance (e.g., falling within the state support area and have high value), thereby achieving trajectory stitching, as the concept shown in (b) of Figure \\ref{fig:SSB_trajectorystitching}. In what follows, after an introduction and a review of related works, Section \\ref{sec:preliminary} provides a brief overview of the preliminary knowledge on action constraint methods and consequence-driven methods in offline RL. Section \\ref{sec:method} details the DASP method with variational inference and implementation details. Experimental results are presented in Section \\ref{sec:exper} to evaluate the effectiveness of the proposed methods under various settings. Finally, the paper concludes with a summary."}
{"input": "\\section{Related Works} \\paragraph{Offline reinforcement learning.} The most significant issue in offline RL is balancing conservatism with performance of the learned policy. The Conservative Q-Learning (CQL)~\\cite{cql} and Bootstrapping Error Accumulation Reduction (BEAR)~\\cite{bear} methods regulate the divergence within a relaxation factor of the new policy. Supported Policy Optimization (SPOT)~\\cite{SPOT} takes a different approach by explicitly estimating the behavior policy’s density using a high-capacity Conditional VAE (CVAE)~\\cite{cvae} architecture. The most recent advancement in this field is Constrained Policy optimization with Explicit Behavior density (CPED)~\\cite{cped}, which utilizes a flow-GAN model to estimate the density of behavior policy more accurately. However, all these methods are to be overly restrictive and lacks robustness and generalization ability, especially at those OOD or unseen states. \\paragraph{OOD state correction.} OOD state correction methods, also known as state recovery methods, like State Deviation Correction (SDC)~\\cite{sdc} align the transitioned distributions of the new policy and the behavior policy, forming a robust transition to avoid the OOD consequences. To further avoid the explicit estimation of consequences in high-dimensional state space, Out-of-sample Situation Recovery (OSR)~\\cite{osr} introduces an inverse dynamics model (IDM)~\\cite{markovrep} to consider the consequential knowledge in an implicit way when decision making. However, such methods may limit their ability to generalize effectively. State Correction and OOD Action Suppression (SCAS)~\\cite{scas} achieves value-aware OOD state correction by state value function and consequence prediction, i.e., aligning high-value transitions of the new policy."}
